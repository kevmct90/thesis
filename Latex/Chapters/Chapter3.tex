% Chapter Template

\chapter{Data} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Data}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}
This chapter presents the data that will be used for the experiments to be carried out in this research and will be split into two major sections. 

The first section will outline where the customers for the experiment have been gathered and under what criteria they have been selected. As part of the experiment a baseline predictive model will be built, this will be done using features that were used in historic industry credit scorecard models in \subjectname. Performance measure measure will selected evaluated also using this base line analysis.

The second section will outline what macro-economic features will be used as part of the research in the experiment. The aim of this research is to investigate the predictive power of macro-economic features by geographic regions such as electoral divisions and local authority in Ireland. To do this addresses are stored in \subjectname's databases are queried against an search engine built for this research and mapped to \textit{Global Positioning System} (GPS) coordinates string metric algorithms. The macro-economic features will be sourced internally in \subjectname\ and externally from open data sources. It will also explain how these features have been created, what transformations or data wrangling had to be done so the features fit/map into an analytical base table (ABT) discussed in Section \ref{sec:datasetConstruction} that is a requirement for predictive modelling.

\section{Customers for Credit Scoring and Existing Features}

The customer data used for prediction in these experiments was sourced from a financial institution \subjectname which is one of the two main pillar banks in Ireland. It contains details of 27,082 SME customers who were active between June 2014 and June 2015. These 27,082 customers are a subset of SME customers on the \subjectname\ book as the experiment will only be completed on one of the loan systems in the financial institution. None of the 27,082 are in area in arrears at the point of selection June 2014. The baseline model for this experiment will built from features of a historic scorecard in \subjectname.

As mentioned in Section \ref{sec:segment} it is very common in credit scoring to model the population into multiple groups. This is done so homogeneous customers are grouped and modelled together based on for example pattern, characteristic, demographic etc. This common practice in industry also, in \subjectname\ is one method of modelling the credit risk of customers by splitting the customers into two segments. The criteria for selecting which segment each customer is modelled in is if that customer has been in arrears previously or not. If they have been for this research that customer will be modelled in the \textit{Previous Delinquency} segment, if the customer has been performing well and not in arrears previously they will be modelled in the \textit{No Previous Delinquency} segment. The historic scorecards that was used in \subjectname\ used different features for prediction in each segment population. This was decided doing empirical analysis of the data where it was observed that different features contributed to the prediction of each subset of the population with a few overlapping features in each subset. Due to sensitivity of information the feature set for these models could not be documented in this thesis paper.

As mentioned in Section \ref{sec:imBalance} class imbalance in datasets is a major real world problem when building a predictive model. As mentioned previous this happens the target class is is not distributed evenly in your dataset. The dataset in this example suffers from this imbalance also, however because the data is partitioned into two segments the imbalance in \textit{previous delinquency} dataset improves significantly but gets worse in the \textit{no previous delinquency} dataset.

The characteristics of the two datasets that will be used to build the baseline benchmark can be illustrated in Table \ref{characteristicsDatasets} below.

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l r r r r r r}
			\hline
			\textbf{Model} &  \textbf{\# Numeric} & \textbf{\# Nominal} & \textbf{\# Observations} & \textbf{\# Good} & \textbf{\# Bad} & \textbf{Good:Bad}\\
			\hline
			Previous Delinquency & 11 & 0 & 2,926 & 2,198  & 738 & 75:25 \\ 
			No Previous Delinquency & 9 & 0 & 24,156 & 23,505 & 651  & 97:03 \\ \hline
			\textbf{Total} &  &  & \textbf{27,082} & \textbf{25,703} & \textbf{1,389} & \textbf{95:05} \\ \hline
		\end{tabular}
	}
	\caption{Characteristics of datasets to be used in the exploratory evaluation for training a baseline model and assessing the evaluation metrics to be used in the research \\
		 \# Numeric refers to the number of continuous features \\
		 \# Nominal refers to the number of categorical features
		}
	\label{characteristicsDatasets}
\end{table}

It can be seen above in Table \ref{characteristicsDatasets} that two datasets are not very similar. There are only 2,936 customers in the \textit{previous delinquency} dataset and 24,156 in the \textit{no previous delinquency} dataset. Perhaps the the biggest difference is class imbalance difference between the datasets. 25\% of customers in the \textit{previous delinquency} dataset are bad by the end of the outcome window but there is only 3\% of the \textit{no previous delinquency} dataset that are bad at the end of the outcome window. As discussed in Section \ref{sec:imBalance} this presents significant challenges when try to build a predictive model but also a very common challenge in real world applications. 



\section{Macro-Economic Areas for Experiment}
The experiment in this research is to investigate if macro-economic features by locations are useful for predicting if SME customers are likely to go into default. The two macro-economic regions that features will be based on are \textit{Electoral Division} (ED) and \textit{Local Authority} (LD). Below Fig. \ref{fig:Ireland_ED_LA_Example} maps out the electoral divisions and local authorities in the Republic of Ireland.

\begin{figure}[H]
	\begin{subfigure}[b]{0.5\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height = 10cm]{IrelandElectoralDivisions}
		\caption{Map of the Current 3,440 Electoral Divisions}\label{fig:IrelandElectoralDivisions}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.5\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height = 10cm]{IrelandLocalAuthorities}
		\caption{Map of the Current 34 Local Authorities}\label{fig:IrelandLocalAuthorities}
	\end{subfigure}
	\caption{Republic of Ireland's Electoral Divisions Local Authorities}
	\label{fig:Ireland_ED_LA_Example}
\end{figure}

There are 34 primary local authorities in the Republic of Ireland, including 29 county councils and 5 city councils. Organisations within each area are responsible for managing issues such as housing, planning, roads, water supply and sewerage, Development Incentives and Controls, Environmental Protection, Recreation Facilities and Amenities, Agriculture, Education, Health and Welfare \footnote{\url{http://www.iro.ie/local_authorities.html}}. There are 3,440 electoral divisions in the Republic of Ireland. Electoral divisions are formed by grouping town-lands together and are the smallest legally defined administrative areas in the state which small population statistics are published from in the Census \footnote{\url{http://census.cso.ie/censusasp/saps/boundaries/eds_bound.htm}}.


\section{Converting Addresses to GPS Coordinates}
To link macro-economic features by electoral division and local authorities to a SME customer there has to be a mechanism to link to that customers address. In an ideal world this would be done through a data model where a customers address would be linked to the address reference database. This can be illustrated through the Entity-Relationship Model (ERD) in Fig. \ref{fig:MasterReferenceAddressDataArchitecture}.

\begin{figure}[H]
	\includegraphics[width=1\textwidth,center]{MasterReferenceAddressDataArchitecture}
	\caption{Optimal Entity Relationship Data Model for Mapping Customers, Addresses, Electoral Division and Local Authorities}
	\label{fig:MasterReferenceAddressDataArchitecture}
\end{figure}

Unfortunately this is not the case currently in \subjectname. Currently addresses are stored in free text fields across multiple systems. This can be illustrated in Fig. \ref{fig:AIB_Situtation} where you can see unstandardised and free text addresses are stored in the Customer table instead of an address reference table like in Fig. \ref{fig:MasterReferenceAddressDataArchitecture}.

\begin{figure}[H]
	\includegraphics[width=.7\textwidth,center]{AIB_Situtation}
	\caption{Current Entity Relationship Data Model for Mapping Customers, Addresses, Electoral Division and Local Authorities}
	\label{fig:AIB_Situtation}
\end{figure}

With the release of Ireland's new postcode system in July 2015 Eircode\footnote{\url{http://www.eircode.ie/}} \subjectname\ was looked to position itself strongly for its deployment and how it would integrate into \subjectname's current systems. For this there and for other private reason there was an investment in the GeoDirectory database \footnote{\url{https://www.geodirectory.ie/}}. It is a product established by An Post\footnote{\url{http://www.anpost.ie/AnPost/}} and the Ordnance Survey Ireland\footnote{\url{http://www.osi.ie/}}. It provides a complete database of all the addresses in the Republic of Ireland and geolocation details including 1.8 million buildings. The Eircode database is heavily connected with GeoDirectry database as it is essentially the same database with a new address identifier called \textit{Eircode}. The database diagram for GeoDirectory is show below in Fig. \ref{fig:GeoDirectoryDatabase} which includes an Electoral Division and Local Authorities table.

\begin{figure}[H]
	\includegraphics[width=1.2\textwidth,center]{GeoDirectoryDatabase}
	\caption{GeoDirectory database diagram}
	\label{fig:GeoDirectoryDatabase}
\end{figure}

So to complete this research and experiment a mechanism or application for matching customer addresses to the the GeoDirectory database needed to be built. There are many vendors in Ireland and International that provide serves correct and validate services such as Address Doctor\footnote{\url{https://www.informatica.com/addressdoctor.html}}, Gamma\footnote{\url{http://www.gamma.ie/about-gamma}} and Data Ireland\footnote{\url{http://www.dataireland.ie/}} to name a few. Committing to one of these products would require a project to evaluate each service \subjectname\ to analyse the pros and cons and understanding the full requirements of the bank.

In the interim as part of this research it was decided to look at in house solution that could be developed using existing resources. After some investigation and experimentation it was identified that it was possible to build a solution levering an address database GeoDirectory database, a search platform/engine Solr\footnote{\url{http://lucene.apache.org/solr/}} and a high level programming language \footnote{\url{https://www.python.org/}}.


The Solr search works by creating an index of the data chosen for application. an example of this can be illustrated in Fig. \ref{fig:solrIndexing}\footnote{\url{http://blog.e-zest.net/about-apache-solr/}}. 

\begin{figure}[H]
	\includegraphics[width=1\textwidth,center]{solrIndexing}
	\caption[Illustration of Inverted Indexing]
	{How Solr Indexes and Stores Data}
	\label{fig:solrIndexing}
\end{figure}

For this experiment the GeoDirectory database was indexed using Solr allowing it to be queried through the web interface or multiple Application Program Interface (API) such as Python, JavaScript, Ruby, Java, HTTP to name a few. Solr returns a number results from queries posted against it based on these indexes which can be seen in Fig. \ref{fig:Solr_Example_Search}.

\begin{figure}[H]
	\includegraphics[width=1.1\textwidth,,height = 10cm,center]{Solr_Example_Search}
	\caption{Solr Query Example and Syntax}
	\label{fig:Solr_Example_Search from Web Interface}
\end{figure}

Although this solution worked quite well in the majority of cases observed it did have some issue because way Irish addresses are structured. For example because there are number of addresses that contain "Some County Road" there were cases when the first result return by Solr returned a false positive. To cater for this number of the top results returned from Solr were compared using sting similarity metrics. String comparison algorithms/metrics are used to determine the distance or number of changes between two strings \citep{wagner_string--string_1974}. The two string comparison methods that were used as part of this experiment are the \textit{Levenshtein Distance} \citep{levenshtein_binary_1966} and \textit{Jaro–Winkler Distance} \citep{winkler_string_1990}. The Levenshtein is computed by calculating the smallest number of single character changes between to strings. The score can be normalised for it produces a so it produces a value between 0 and 1 by $1 -\frac{\text{number of edits}}{\text{length of the larger string}}$. It is very useful for compensating for typos in string matching. The Jaro-Winkler algorithm is used to measure number of characters in common but also works on the basis that differences at the start of the string are more important than those at end. In research completed by \cite{christen_comparison_2006} the Jaro-Winkler method techniques performed quite well across all experiments and was included in this experiment as a result.

\newpage

\section{Data for Experiment}


\subsubsection{Transaction Dataset}

\subsubsection{Binned Features}

Retails Customers split by Loans, Mortgages, Both. Then SME arrears features/
Grouped / WOE 

\subsubsection{SME Trends}

\subsubsection{CSO Features}



\section{Old Sections}
This chapter will detail the design of the experiments that are to be implemented as part of the research project. It will detail how customers addresses are mapped to locations and how features are generated for these locations. It will also discuss any of the pre-processing, and feature selection and statistical techniques used to create select the data for the ABT.

The SME default problem would be considered imbalanced dataset for classification problem where algorithms make the assumption there is a equal number of good and bad classes/group \citep{japkowicz_class_2000}. The dataset used in this experiment has a small number of SME customer in default, because of this issue a number of approaches will taken to considered to address.

Statistical analysis and evaluation methods used to assess the strength of models will be detailed and discussed, also it will outline any further transformations necessary for specific algorithms. Finally criteria for deciding which model will be chosen as the best and most fit for purpose will be detailed.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Data for Research and Data Preparation}
This research project will combine data from an existing scorecard in \subjectname\ with location based data. There will be three new sources of location based data for this experiment. They are as follows......

\subsection{Existing Data in \subjectname's Model}

\subsection{Electoral Division and Local Authority Classification}
\begin{itemize}
	\item \subjectname's Customers Transactional behaviour by location.
	\item \subjectname\ default ratio by location.
	\item CSO metrics captured using the 2011 census. 
\end{itemize}

Locations for this research will be found using \subjectname's customer addresses. These will be cleansed to be standardised them and will put through a search engine built using Solr. The top 10 results are returned using Solr, a matching algorithms are then ran to compare the searched address and returned results. The two distance formulas used to compare the are the following \textit{Levenshtein distance} and the \textit{Jaro-Winkler distance}.

\subsection{Mapping Addresses using Apache Solr}


\begin{figure}[h!]
	\includegraphics[width=0.8\textwidth,center]{Solr_Example_Search}
	\caption[Query Example and Syntax]
	{Query Example and Syntax}
	\label{fig:Solr_Example_Search}
\end{figure}



\subsubsection{Levenshtein Distance}
Mathematically, the Levenshtein distance between two strings \textit{a}, \textit{b} (of length \text{\textbar}a\text{\textbar} and \text{\textbar}b\text{\textbar} respectively) is given by lev$_{a,b}$ (\text{\textbar}a\text{\textbar},\text{\textbar}b\text{\textbar}) where
\begin{align}
	lev_{a,b}(i,j) = 
	\begin{cases}
		\max(i,j) & \text{ if} \min(i,j)=0, \\
		\min \begin{cases}
			\operatorname{lev}_{a,b}(i-1,j) + 1 \\
			\operatorname{lev}_{a,b}(i,j-1) + 1 \\
			\operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
		\end{cases} & \text{ otherwise.}
	\end{cases}
\end{align}
where $1_{(a_i \neq b_j)}$ is the indicator function equal to $0$ when $a_i = b_j$ and equal to 1 otherwise, and $\operatorname{lev}_{a,b}(i,j)$ is the distance between the first $i$ characters of $a$ and the first $j$ characters of $b$.

\subsubsection{Jaro-Winkler Distance}
The Jaro distance $d_j$ of two given strings $s_1$ and $s_2$ is
 
\begin{align}
 d_j =\begin{cases}
 \begin{array}{l l}
 	0 & \text{if }m = 0\\
 	\frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right) & \text{otherwise} \end{array} \end{cases}
\end{align}

Where: 
\vspace{-7mm} 
\begin{itemize}
	\item $m$ is the number of matching characters 
 	\item $t$ is half the number of transpositions
\end{itemize}

\subsubsection{Examples}
Give example of how addresses are cleansed, standardised, searched and results are calculated.

\subsection{Visa Debit Transactional Data}
\subsubsection{Discretionary Non-Discretionary Categorisation}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
	\label{my-label}
	\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{MCC Category} & \textbf{Parent}          & \textbf{Child}                 & \textbf{Spend / Live} \\ \hline
		2.2                   & Bills \& Utilities       & Cable/Satellite TV \& Internet & Spend                 \\ \hline
		2.7                   & Bills \& Utilities       & Gas/Electricity/Energy         & Live                  \\ \hline
		3.1                   & Leisure \& Entertainment & Cinema \& Theatre              & Spend                 \\ \hline
		4.1                   & Shopping                 & Groceries                     & Live                  \\ \hline
		4.4                   & Shopping                 & Clothing \& Accessories        & Spend                 \\ \hline
		5.2                   & Health \& Personal Care  & Doctor                         & Live                  \\ \hline
		5.6                   & Health \& Personal Care  & Hair \& Beauty                 & Spend                 \\ \hline
		6.2                   & Household/Home           & Household Maintenance          & Live                  \\ \hline
		6.5                   & Household/Home           & Computers \& Technology        & Spend                 \\ \hline
	\end{tabular}
	}
	\caption{Spend Live Categorisation }
\end{table}



\subsubsection{Percentage Change}

\begin{align}
\text{Percentage Change} = \frac{X^{2} - X^{1}}{X^{1}}*100  
\end{align}
Where: 
\vspace{-7mm} 
\begin{itemize}
	\item $X^{1} = $ the original variable
	\item $X^{2} = $ the new variable
\end{itemize}


\subsection{Electoral Division and Local Authority}
\textbf{Local Authorities}
In census reports the country is divided into 29 counties/administrative counties and the five Cities which represent the
local authority areas. Outside Dublin there are 26 administrative counties (North Tipperary and South Tipperary each
ranks as a separate county for administrative purposes) and four Cities, i.e. Cork, Limerick, Waterford and Galway. In
Dublin the four local authority areas are identified separately, i.e. Dublin City and the three administrative counties of
Dún Laoghaire-Rathdown, Fingal and South Dublin.

\textbf{Electoral Divisions}
There are 3,440 Electoral Divisions (EDs) which are the smallest legally defined administrative areas in the State. One ED, St. Mary's, straddles the Louth-Meath county border, and is presented in two parts in the SAPS1 tables, with one part in Louth and the other in Meath. There are 32 EDs with low population, which for reasons of confidentiality have been amalgamated into neighbouring EDs giving a total of 3,409 EDs which appear in the SAPS tables.

\begin{table}[H]
	\centering
	\label{my-label}
	\begin{tabular}{|l|l|p{10cm}|}
		\hline
		\textbf{Feature} & \textbf{Data Type} & \textbf{Description}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\ \hline
		ED\_ID           & Categorical        & There are 3,440 Electoral Divisions (EDs) which are the smallest legally defined administrative areas in Ireland.                                                                                                                                                                                                                                                                                                                                                                             \\ \hline
		LA\_ID           & Categorical        & In census reports the country is divided into 29 counties/administrative counties and the five Cities which represent the local authority areas. Outside Dublin there are 26 administrative counties (North Tipperary and South Tipperary each ranks as a separate county for administrative purposes) and four Cities, i.e. Cork, Limerick, Waterford and Galway. In Dublin the four local authority areas are identified separately, i.e. Dublin City and the three administrative counties of Dun Laoghaire-Rathdown, Fingal and South Dublin. \\ \hline
	\end{tabular}
	\caption{Electoral Division and Local Authority Details }
\end{table}


\subsection{Census 2011 Ireland}
The Census gives a comprehensive picture of the social and living conditions of the population in 2011. It provides detail to the smallest area and the results are an essential tool for effective policy, planning and decision making purposes. 

The data is organised around a number of themes (see below) and broken down by a range of geographic areas as
described below.

Themes of data available in the census data.
\begin{table}[H]
	\centering
	\label{my-label}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Theme Number} & \textbf{Theme Description} \\ \hline
		Theme 1    & Sex, age and marital status \\ \hline
		Theme 2    & Migration, ethnicity and religion \\ \hline
		Theme 3    & Irish Language \\ \hline
		Theme 4    & Families \\ \hline
		Theme 5    & Private Household \\ \hline
		Theme 6    & Housing \\ \hline
		Theme 7	   & Communal establishments \\ \hline
		Theme 8    & Principal status \\ \hline
		Theme 9    & Social class and socio-economic group \\ \hline
		Theme 10   & Education \\ \hline
		Theme 11   & Commuting \\ \hline	
		Theme 12   & Disability, careers and general health \\ \hline		
		Theme 13   & Occupation \\ \hline
		Theme 14   & Industries \\ \hline
		Theme 15   & PC and internet Access \\ \hline			
	\end{tabular}
	\caption{Lending Product Classification }
\end{table}


\url{http://www.cbs.gov.il/census/census/pnimi_sub_page_e.html?id_topic=1&id_subtopic=5}

\begin{table}[H]
	\centering
	\label{my-label}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Product Summary Description} & \textbf{Product Detailed Description} \\ \hline
		Branch Advances     & Fixed Loans \\ \hline
		Branch Advances     & Matrix Loans \\ \hline
		Branch Advances     & Other Loans \\ \hline
		Branch Advances     & Premium Business Rate \\ \hline
		Branch Advances     & Prime Loans \\ \hline
		Branch Advances     & Staff Credit Flex \\ \hline
		Branch Advances		& Suspense Interest \\ \hline
		Home Loan           & Buy to Let \\ \hline
		Home Loan           & Commercial Mortgages \\ \hline
		Home Loan           & Home Loan \\ \hline
		Home Loan           & Staff Homeflex \\ \hline
		Home Loan           & Standard Mortgages \\ \hline	
		Home Loan           & Surplus Builder \\ \hline		
		Home Loan           & Tracker Mortgages \\ \hline		
	\end{tabular}
	\caption{CSO Theme Breakdown}
\end{table}

\subsubsection{Principal Status/Employment}
\subsubsection{Occupation}
\subsubsection{Education}
Lower than Upper Secondary Education
A question I wanted to address was there was areas with higher concentrations of lower education by ED and LA, and if this had any correlations with SME arrears in \subjectname.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
	\label{my-label}
	\begin{tabular}{|l |l|}
		\hline
		\textbf{Education Levels}                                   & \textbf{Lower than Upper Secondary Education} \\ \hline
		No Formal Education                                         & Yes                                           \\ \hline
		Primary Education                                           & Yes                                           \\ \hline
		Lower Secondary                                             & Yes                                           \\ \hline
		Upper Secondary                                             & No                                            \\ \hline
		Technical or Vocational qualification                       & No                                            \\ \hline
		Advanced Certificate/Completed Apprenticeship               & No                                            \\ \hline
		Higher Certificate                                          & No                                            \\ \hline
		Ordinary Bachelor Degree or National Diploma                & No                                            \\ \hline
		Honours Bachelor Degree, Professional Qualification or both & No                                            \\ \hline
		Postgraduate Diploma or Degree                              & No                                            \\ \hline
		Doctorate(Ph.D) or higher                                   & No                                            \\ \hline
		Not Stated                                                  & NA                                            \\ \hline
	\end{tabular}
	}
	\caption{My caption}
\end{table}

\subsection{Arrears Ratio for \subjectname\ Personal Customers}
\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
	\label{my-label}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Feature} & \textbf{Data Type} & \textbf{Description}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\ \hline
		ED\_Arrears\_Ratio          & Interval        & Percentage of personal customers in a ED to be in arrears.                                                                                                                                                                                                                                                                                                                                                                           \\ \hlineß
		LA\_Arrears\_Ratio           & Interval        & Percentage of personal customers in a LA to be in arrears \\ \hline
	\end{tabular}
	}
	\caption{Arrears Ratio for \subjectname\ Personal Customers }
\end{table}

\begin{table}[H]
	\centering
	\label{my-label}
	\resizebox{\textwidth}{!}
	{
	\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{Product Summary Description} & \textbf{Product Detailed Description} \\ \hline
		Branch Advances     & Fixed Loans \\ \hline
		Branch Advances     & Matrix Loans \\ \hline
		Branch Advances     & Other Loans \\ \hline
		Branch Advances     & Premium Business Rate \\ \hline
		Branch Advances     & Prime Loans \\ \hline
		Branch Advances     & Staff Credit Flex \\ \hline
		Branch Advances		& Suspense Interest \\ \hline
		Home Loan           & Buy to Let \\ \hline
		Home Loan           & Commercial Mortgages \\ \hline
		Home Loan           & Home Loan \\ \hline
		Home Loan           & Staff Homeflex \\ \hline
		Home Loan           & Standard Mortgages \\ \hline	
		Home Loan           & Surplus Builder \\ \hline		
		Home Loan           & Tracker Mortgages \\ \hline		
	\end{tabular}
	}
	\caption{Lending 
		Product Classification }
\end{table}

\subsection{Generating the ABTs}
We now have a reference list that allows us to apply a rank to all achievable loan application events prior to completion. A method is required to join this reference list with our csv (comma seperated value) log files resulting from the MapReduce job. 

%Fine from here
\subsection{Software Used}\label{softwareUsed}

\subsubsection{Address Matching}
A combination of Apache Solr\footnote{\url{http://lucene.apache.org/solr/}} and Python\footnote{\url{https://www.python.org/}} were used to match \subjectname's addresses to an ED/LA in GDD. Solr ia an open source web application enterprise search engine. It is an open source application written in Java, which is a wrapper around the Apache Lucence \footnote{{\url{https://lucene.apache.org/core/}}}. Combined they provide a reliable, fast, scalable platform capable of providing distributed indexing which can then be used for searching or navigation. Solr was used to index GD which then allowed it to be searched. Python is a high-level, general purpose programming language which can be used to build both large and small scale programs. Python like Solr is open source and freely available. One its most attractive and best characteristics is it is easy to read and use. A program was created to standardise and cleanse the addresses in \subjectname's database. The program would then take the cleansed addresses and query them against GDD indexed through the Solr web API, using string comparison algorithms discussed in Chapter 2 to return the most likely GDD address.

\subsubsection{Data Wrangling}
Anecdotally speaking, data scientists and analysts spend majority of their time data wrangling. Data wrangling is time consuming mundane process used to collect and prepare data prior to being explored for useful information. In the experiment of this thesis a variety of data types and data sources were used. Data from the \subjectname\ EDW, GDD in Solr served in JSON, CSO data in flat file to name a few. R\footnote{{\url{https://www.r-project.org/}}}, another open source programming language but has much more emphasis on statistical computing. It also has many libraries available for processing and data manipulation which are available in the CRAN\footnote{{\url{https://cran.r-project.org/}}} repository. The most useful package used during this process was \textit{reshape}\footnote{{\url{https://cran.r-project.org/web/packages/reshape/index.html}}}. Reshape allows to easily restructure, transpose and aggregate your data. 

\subsubsection{Visualisations}
R is also a very strong programming language at creating beautiful visualisations so it will be used throughout this paper. One package used in this paper was \textit{ggplot}
\footnote{{\url{https://cran.r-project.org/web/packages/ggplot2/index.html}}}. 

Some other custom geospatial visualisations may also be required. This could be done using Arcgis, Qgis, D3 or R still to be decided.

\subsubsection{Modelling}
SQL was used to identify customers to be used for predictions and generate the target class.
The models and experiments performed in this used in this are built in R and SAS. SAS is a proprietary software. SAS is the tool of choice by the modelling teams in \subjectname. Anecdotally SAS is a legacy in financial institutions, it is what people are used to using but also there is a for-profit corporation vetting the code for its customers and customer service and support corporations are used to. SAS offers a graphical interfaces which means users do not have to enter code, but this can be complemented using the SAS programming language. Anecdotally speaking SAS is excellent for building predictive modes resulting in good time to value. SAS Enterprise Miner includes the following components Time Series, Variable Clustering, Cluster, Interactive Binning, Principal Components, AutoNeural, DMNeural, DMine Regression, Gradient Boosting, Ensemble, and Text Mining.
\\
R is a very strong competitor to SAS in this space. Because of its open source nature there are many libraries available for building predictive models. For example one popular package \textit{caret}\footnote{{\url{https://cran.r-project.org/web/packages/caret/index.html}}} contains many models and continues to grow

\section{Building Models}
In order to avoid over-fitting models, considerations need to be made as to what data to use in training the model. Each of the models built will be trained using a training set which is a subset of the over all available samples. We discussed possible options for training set selection in subsection


\section{Evaluation Methods}
Several evaluation techniques will be applied to quantify the usefulness of all models produced. As discussed in section \ref{modelEval}, there are various ways to evaluate the performance of a classifier. A common metric used is overall accuracy (\% of records correctly classified). 


\section{Conclusions}\label{desConc}
\begin{comment}
This chapter has discussed the processes required to carry out data mining techniques on non-relational web log data. The required data transformations have been discussed as well as the implemented methods for feature generation.
\end{comment}