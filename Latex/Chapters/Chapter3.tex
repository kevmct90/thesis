% Chapter Template

\chapter{Design / Methodology} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Design / methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------

This chapter will detail the design of the experiments that are to be implemented as part of the research project. It will detail how customers addresses are mapped to locations and how features are generated for these locations. It will also discuss any of the pre-processing, and feature selection and statistical techniques used to create select the data for the ABT.

The SME default problem would be considered imbalanced dataset for classification problem where algorithms make the assumption there is a equal number of good and bad classes/group \citep{japkowicz_class_2000}. The dataset used in this experiment has a small number of SME customer in default, because of this issue a number of approaches will taken to considered to address.

Statistical analysis and evaluation methods used to assess the strength of models will be detailed and discussed, also it will outline any further transformations necessary for specific algorithms. Finally criteria for deciding which model will be chosen as the best and most fit for purpose will be detailed.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Data for Research}
This research project will combine data from an existing scorecard in \subjectname\ with location based data. There will be three new sources of location based data for this experiment. They are as follows......

\begin{itemize}
	\item \subjectname's Customers Transactional behaviour by location.
	\item \subjectname\ default ratio by location.
	\item CSO metrics captured using the 2011 census. 
\end{itemize}

Locations for this research will be found using \subjectname's customer addresses. These will be cleansed to be standardised them and will put through a search engine built using Solr. The top 10 results are returned using Solr, a matching algorithms are then ran to compare the searched address and returned results. The two distance formulas used to compare the are the following \textit{Levenshtein distance} and the \textit{Jaro-Winkler distance}.

\subsection{Mapping Addresses using Apache Solr}


\begin{figure}[h!]
	\includegraphics[width=0.8\textwidth,center]{Solr_Example_Search}
	\caption[Query Example and Syntax]
	{Query Example and Syntax}
	\label{fig:Solr_Example_Search}
\end{figure}

\begin{figure}[h!]
	\includegraphics[width=0.8\textwidth,center]{Solr_Inverted_Index}
	\caption[Illustration of Inverted Indexing]
	{Illustration of Inverted Indexing}
	\label{fig:Solr_Inverted_Index}
\end{figure}

\subsubsection{Levenshtein Distance}
Mathematically, the Levenshtein distance between two strings a, b (of length |a| and |b| respectively) is given by \\

$lev_{a,b}(i - 1,j) + 1 $\\
$lev_{a,b}(i,j - 1) + 1 $\\
not equal 
$lev_{a,b}(i -1 ,j - 1) + 1_{(a_{i}=!b_{j})} $

\subsubsection{Jaro-Winkler Distance}

Ultimately much of the initial data pre-processing task involves simple checks for logs of relevant activity relating to the personal loan application process and discarding the others. This step is performed using 


\subsection{Generating Ordinal Reference Data}\label{ordRef}
In the literature we reviewed ideas about distance and ordinal distance measures and postulated the potential in analysing pages and clicks as ordinal variables by considering their position in the context of a particular process. 


\subsection{Generating the ABTs}
We now have a reference list that allows us to apply a rank to all achievable loan application events prior to completion. A method is required to join this reference list with our csv (comma seperated value) log files resulting from the MapReduce job. 


%Fine from here
\subsection{Software Used}\label{soft}
Java MapReduce was used to extract the appropriate event data and create the target variable. Further processing could be implemented at this stage using MapReduce but at an early stage of data exploration, it was felt that it was better to not reduce the data any more than was needed for it to be processed in memory.


\section{Building Models}
In order to avoid over-fitting models, considerations need to be made as to what data to use in training the model. Each of the models built will be trained using a training set which is a subset of the over all available samples. We discussed possible options for training set selection in subsection


\section{Evaluation Methods}
Several evaluation techniques will be applied to quantify the usefulness of all models produced. As discussed in section \ref{modelEval}, there are various ways to evaluate the performance of a classifier. A common metric used is overall accuracy (\% of records correctly classified). 


\section{Conclusions}\label{desConc}
This chapter has discussed the processes required to carry out data mining techniques on non-relational web log data. The required data transformations have been discussed as well as the implemented methods for feature generation.



