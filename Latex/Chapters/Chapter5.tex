% Chapter Template

\chapter{Implementation and Evaluation} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Experiment Implementation \& Evaluation}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}
This chapter will present the implementation of the experiments to evaluate the use of macro-economic features for predicting SME defaults. The predictive capability of customers spending behaviour, personal customer default rates, SME default rates and Census data  will be analysed and discussed.

The experiments in this research are sequential meaning results from one experiment are used in the following experiment. Therefore results will also be discussed and evaluated in this section.

A benchmark predictive model will be built in SAS and R based on historical scorecard features. This will be used to make fair comparisons of the results from the experiments.


\section{Data Exploration}\label{sec:Dataexplor}

This section will explore some of the trends and patters of SME default rates of customers in the dataset. We will visualise SME defaults trends from June 2012 - June 2015 by looking at the number of SME customers in default each month and the percentage of customers in default each month to see if there as anything obvious that stands out in the data. 

Finally we will visualise the percentage of SME customers in default by electoral division and local authority at the observation point to see if there is reason to believe that location has any obvious correlations with default rates which will be done using a geographic information system (GIS) application.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,height=6cm, center]{SME_Status}
	\caption{SME Customers Default/Performing June 2012 - June 2015 \\ x-axis = Year-Month  \\
		y-axis = \# SME Customers
		\\Red Trend Line = \# SME Customers in Default each month
		\\Blue Trend Line = \# SME Customers in Performing each month}
	\label{fig:SME_Status}
\end{figure}

Fig. \ref{fig:SME_Status} shows the number of SME customers that are performing or are defaulting each month from June 2012-June 2015. The data looks to appears to be relatively well behaved. You can see to the right of the chart that the number of SME customers performing is increasing while at the same time the number of customers in default is dropping. 

\begin{figure}[H]
	\includegraphics[width=.9\textwidth,height=6cm, center]{percentageArrears}
	\caption{Percentage of SME Customers in Default Each Month 
		\\ June 2012 - June 2015
		\\ x-axis = Year-Month \\
		y-axis = \% of SME Customers in Default
		\\Black Trend Line = \% SME Customers in Default Each Month}
	\label{fig:percentageArrears}
\end{figure}

Fig. \ref{fig:percentageArrears} shows the percentage of SME customers that are in default each month from June 2012 - June 2015. The trend of default is more obvious in this visualisation compared to Fig. \ref{fig:SME_Status}. You can see at the lowest percentage of defaulting customers occurs in June 2012 (left of the chart) where default percentage is just over 17.8\%. This percentage increases over time reaching a maximum at July 2014 (Observation Point) with a percentage of 22.5\%. By June 2015 the SME default has fallen significantly to just over 18\%. This could make predictions using macro-economic features challenging as in the trained dataset the percentage of SME default is rising while a prediction is being made when the percentage of SME default is falling.


Fig. \ref{fig:SMEArrearsLAED} below is a GIS application of the \% SME Customers in Default by County and Electoral Division at the observation point (June 2014). Regions that have a small percentage of SME customers in default are coloured in white while regions that have a high percentage of default are coloured a dark red.
\begin{figure}[H]
	\begin{adjustwidth}{-2cm}{-2cm}
	\begin{subfigure}[b]{0.55\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height = 11cm]{June2014ArrearsByLA}
		\caption{\% SME Customers in Default by Local Authority June 2014}\label{fig:June2014ArrearsByLA}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.55\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth, height = 11cm]{June2014ArrearsByED}
		\caption{\% SME Customers in Default by Electoral Division June 2014}
		\label{fig:June2014ArrearsByED}
	\end{subfigure}
	\caption{\% SME Customers in Default by 34 Local Authority and 3,440 Electoral Division}
	\label{fig:SMEArrearsLAED}
	\end{adjustwidth}
\end{figure}

The results of this visualisation are quite interesting and backup the hypothesis that location maybe an important in factor in SMEs defaulting in Ireland. Firstly looking at Fig. \ref{fig:June2014ArrearsByLA} we can see that the local authorities with a high percentage of default cluster to the east coast of Ireland with Cavan, Meath, Louth, Dublin performing the worst. 

It can also be seen when you focus in on the cities that they are performing worse than counties they are in. Galway City, Limerick City, Cork City and Waterford City all have higher percentage default than the default percentage of the counties. Waterford City is an extreme example where there huge is difference between the default rates in the city and county.

Fig. \ref{fig:June2014ArrearsByED} is a little bit more difficult to draw conclusions from due to the number of electoral division in Ireland. It does demonstrate that within each local authority there is a large amount of variation between the percentage default of a electoral divisions. Focusing on the Dublin Area in particular in Fig. \ref{fig:June2014ArrearsByED} it can be seen that large differences between electoral divisions where you can see some performing very well and other performing very badly. 
 
 
The results from this section have been very useful. They have proved that the data is well behaved in Fig. \ref{fig:SME_Status}. It has identified that falling SME default percentages from the observation point could provide a challenge Fig. \ref{fig:percentageArrears} in making predictions. Results were able to backup the hypothesis that location is important factor when analysing SME default by mapping the percentage default by electoral division and local authority in Fig. \ref{fig:SMEArrearsLAED}. 


\section{Benchmark Models}\label{sec:benchModels}
As mentioned in previous sections when building predictive models it is essential to have a baseline or benchmark model to compare the experiments to. Therefore benchmark regression models have been built in R and SAS that will be used to make comparisons to experiments in this chapter. Logistic regression was chosen because of its success in experiments in Section \ref{sec:benchFeature} and its wide use in industry. The models have been trained both using R and SAS using a 70\% stratified sample dataset with 30\% being kept for holdout which will be used for testing. Results may vary from both R and SAS due to varying samples in each application and algorithms will be slightly different but they should relatively close. This will not impact analysis of comparing experiments as experiments in R will be compared to the benchmark from R and likewise for comparisons in SAS. The AUC will be the performance measure of choice for evaluating how well the model performs over all possible thresholds. As the AUC does not give the accuracy of the model at a specific threshold recall, specificity and balanced accuracy(BA) will also be assessed. The threshold used is based on fine tuning of the parameters testing using a validation dataset in Section \ref{sec:benchFeature}. The breakdown of the training and test data for the benchmark model and their target class distribution can be shown in Table \ref{table:benchmark_holdout_train_test}.

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			Previous Delinquency & Training       & 483 & 1,565 & 2,048 & 76:24\\
			          & Test & 245 & 633 & 878 & 72:28\\\hline
			\textbf{Previous Previous Delinquency}     & \textbf{Total} & \textbf{728} & \textbf{2,198} & \textbf{2,926} & \textbf{75:25} \\
			\hline
			No Previous Delinquency & Training & 474 & 16,435 & 16,909 & 97:03 \\ 
			          & Test & 177 & 7,070 & 7,247 & 97:03 	\\\hline
			\textbf{No Previous Delinquency}     & \textbf{Total} & \textbf{651} & \textbf{23,505} & \textbf{24,156} & \textbf{97:03} \\
			\hline
			\textbf{Total } 	&     	     & \textbf{1,379} & \textbf{25,703} & \textbf{27,082} & \textbf{95:05}\\ \hline
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Test Dataset \\for Benchmark Models}
	\label{table:benchmark_holdout_train_test}
\end{table}

The threshold used for performance measuring is based on fine tuning of the parameters testing using a validation dataset in Section \ref{sec:benchFeature}. The results from the benchmark models can be found in Table \ref{table:benchmodel} below.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{\footnotesize
		\begin{tabular}{l l l r r r r}
			\hline
			\textbf{Model} & \textbf{Dataset} & \textbf{Software} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA} & \textbf{AUC}  \\ \hline
			\textit{PD\_Bench\_R} & Previous Delinquency & R & 0.542 & 0.696 & 0.619 & 0.654   \\ 
			\textit{PD\_Bench\_SAS} & Previous Delinquency & SAS & 0.529 & 0.616 & 0.573 & 0.615   \\ \hline
			\textit{NPD\_Bench\_R} & No Previous Delinquency & R & 0.525 & 0.739 & 0.632 & 0.671   \\ 
			\textit{NPD\_Bench\_SAS} & No Previous Delinquency & SAS & 0.492 & 0.748 & 0.620 & 0.654   \\ \hline
		\end{tabular}
	}
	\caption{Benchmark Model Results for Experiment Comparison}
	\label{table:benchmodel}
\end{table}

Models in this this section will be aliased for ease of reading, for example the benchmark models will be referred to as \textit{PD\_Bench\_R}, \textit{PD\_Bench\_SAS}, \textit{NPD\_Bench\_R} and \textit{NPD\_Bench\_SAS} which can be seen in Table \ref{table:benchmodel} above. The results found here are consistent with the evaluations completed in Chapter \ref{Chapter4} which guarantees we have good benchmarks to compare the results in the experiment in this chapter to.


For each of the model results the AUC will be used to measure the the accuracy of the model over all possible thresholds. Performance measures over one threshold will also be evaluated. Recall, Specificity, Balanced Accuracy will be recorded to evaluate how the model performs at specified threshold.


\section{Correlation Analysis}
Correlation matrix is a very simple and powerful way of analysing the relationship of predictive features with each other and the target features. Including highly correlated predictive features has the potential to throw off or fool your predictive model potentially causing misleading results. The Pearson correlation coefficient is a common measure used to test the data for this relationship and will be deployed in this experiment. The correlation tests are ran separately on each category of features created as part of this experiment e.g. \textit{CSO Features by ED \& LA}, \textit{SME Default Trends by ED \& LA}, \textit{Homeloan and Personal Loan Default Trends by ED \& LA}, \textit{Transactions Behaviour by ED \& LA}, and \textit{Binned SME Defaults Rates by ED and LA}. Fig. \ref{fig:unbal_corr_analysis} shows a subset of the results demonstrating correlation scores between the experiment features categories with a full breakdown of the results available in Appendix \ref{AppendixB} where a correlation score coloured red represents a very strong positive correlation between features and blue a very low negative correlation.

\begin{figure}[H]
	\begin{adjustwidth}{-2cm}{-2cm}
	\centering
 	\begin{subfigure}[b]{0.6\textwidth}
 		\captionsetup{font=scriptsize}
 		\includegraphics[width=\textwidth,height=4.5cm]{CSO_Correlation_Analysis}
 		\caption{CSO Features by ED \& LA\\}
 		\label{fig:CSOCorrelation}
 	\end{subfigure}
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{SME_Arrears_Trends_Correlation_Analysis_Subset}
		\caption{SME Default Trends by ED \& LA}\label{fig:smeArrearsCorrelation}
	\end{subfigure} 
		\medskip
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{Personal_Arrears_Ratio}
		\caption{Homeloan and Personal Loan \\Default Trends by ED \& LA}
		\label{fig:personalArrearsCorrelation}
	\end{subfigure} 
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{Visa_Debit_Correlation_Analysis_Subset}
		\caption{Transactions Behaviour \\by ED \& LA }\label{fig:transVisaCorrelation}
	\end{subfigure}
		\medskip	
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{Grouped_Features_Correlation_Analysis_Subset}
		\caption{Binned SME Defaults Rates \\by ED and LA\\}
		\label{fig:groupedFeaturesCorrelation}
	\end{subfigure}
	\caption{Correlation Analysis of Macro-Economic Features}
	\label{fig:unbal_corr_analysis}
	\end{adjustwidth}
\end{figure}

There is a high level of correlation between many of the macro-economic features generated as part of the experiment. Predictive features with pairwise correlation above 0.80 are considered candidates for removal. For features that were this highly correlated the feature scoring the lowest bivariate correlation to the target feature was removed, ensuring the feature which had the strongest relationship with the target class was prioritised and kept for further predictions. A subset of the results after this feature reduction process can be found in Fig. \ref{fig:unbal_corr_analysis_filtered} with a full set of results available in Appendix \ref{AppendixC}. 

\begin{figure}[H]
	\begin{adjustwidth}{-2cm}{-2cm}
	\centering
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{CSO_Correlation_Analysis}
		\caption{CSO Features by ED \& LA\\}
		\label{fig:CSOCorrelation}
	\end{subfigure}
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{SMETrendsCorrSub}
		\caption{SME Default Trends \\by ED \& LA}\label{fig:SMETrendsCorrSub}
	\end{subfigure} 
			\medskip
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{PersonalTrendFull}
		\caption{Homeloan and Personal Loan Default Trends by ED \& LA}
		\label{fig:PersonalTrendFull}
	\end{subfigure} 
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{TransactionCorrSub}
		\caption{Transactions Behaviour \\by ED \& LA }\label{fig:TransactionCorrSub}
	\end{subfigure}
		\medskip
	\begin{subfigure}[b]{0.6\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4.5cm]{GroupedCorrSub}
		\caption{Binned SME Defaults Rates \\by ED and LA\\}
		\label{fig:GroupedCorrSub}
	\end{subfigure}
	\caption{Correlation Analysis after Highly Correlated Features removal}
	\label{fig:unbal_corr_analysis_filtered}
	\end{adjustwidth}
\end{figure}

The most correlated macro-economic features with the target feature are then included in the prediction model and will be evaluated against the benchmark model for performance comparison. 
\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{CorrelationChart}
	\caption{Most correlated features with target feature}
	\label{fig:Correlation Analysis}
\end{figure}


Table \ref{table:PDCorrModelResults} details the results for the
re-trained model on the previous delinquency training data using the macro-economic features selected as part of correlation analysis.

\begin{table}[H]
\centering
\small
		\begin{tabular}{l r r r r}
			\hline
			\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
			\textit{PD\_Bench\_R} & 0.542 & 0.696 & 0.619 & 0.654 \\ \hline
			\textit{PD\_Cor5\_R}  & 0.534 & 0.669 & 0.602 & 0.654   \\ 
			\textit{PD\_Cor10\_R} & 0.556 & 0.679 & 0.618 & 0.658  \\ 
			\textit{PD\_Cor15\_R} & \cellcolor{green!25}0.594 & 0.690 & \cellcolor{green!25}0.642 & 0.665  \\
			\textit{PD\_Cor20\_R} & 0.561 & \cellcolor{green!25}0.698 & 0.630 & \cellcolor{green!25}0.665  \\\hline
		\end{tabular}
	\caption{Previous Delinquency Model results when most important \\Macro-Economic features calculated using Correlation were included in training.\\
		*BA = Balanced Accuracy}
	\label{table:PDCorrModelResults}
\end{table}

The results from including macro-economic features based on the correlation analysis are compared against the benchmark model (\textit{PD\_Bench\_R}) built in Section \ref{sec:benchModels}. \textit{PD\_Cor5\_R} will relate to the model that was trained using the top 5 features from the correlation analysis, \textit{PD\_Cor10\_R} is the top 10 features, \textit{PD\_Cor15\_R} is the top 15 features, \textit{PD\_Cor20\_R} is the top 20 features. The highest results for each performance metric are highlighted in green.

The results from Table \ref{table:PDCorrModelResults} are quite promising three of the models AUC is better than the benchmark model. \textit{PD\_Cor15\_R} and \textit{PD\_Cor20\_R} in particular perform very strongly across all performance measures. Based on the results \textit{PD\_Cor15\_R} would be selected as the best model as it scored strongly in specificity, balanced accuracy and AUC but also performed exceptionally with a recall of 59.4\%, much larger than any of the experimental models and 5.2\% higher than the benchmark. The AUC is also 1.1\% higher in \textit{PD\_Cor15\_R} compared with the benchmark model.


Table \ref{table:NPDCorrModelResults} details the results for the
re-trained model on the no previous delinquency training data using the macro-economic features selected as part of correlation analysis.
\begin{table}[H]
	\centering
	\small
	\begin{tabular}{l r r r r}
		\hline
		\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA} & \textbf{AUC}  \\ \hline
		\textit{NPD\_Bench\_R} & 0.525 & 0.739 & 0.632 & 0.671   \\ \hline
		\textit{NPD\_Cor5\_R}  & 0.525 & \cellcolor{green!25}0.743 & 0.634 & 0.671   \\ 
		\textit{NPD\_Cor10\_R} & \cellcolor{green!25}0.555 & 0.725 & \cellcolor{green!25}0.640 & 0.677  \\ 
		\textit{NPD\_Cor15\_R} & 0.538 & 0.723 & 0.630 & \cellcolor{green!25}0.680  \\
		\textit{NPD\_Cor20\_R} & 0.549 & 0.731 & \cellcolor{green!25}0.640 & 0.676  \\\hline 
	\end{tabular}
	\caption{No Previous Delinquency Model results when most important \\Macro-Economic features calculated using Correlation were included in training.\\
		*BA = Balanced Accuracy}
	\label{table:NPDCorrModelResults}
\end{table}

The results from including macro-economic features based on the correlation analysis are compared against the benchmark model (\textit{NPD\_Bench\_R}) built in Section \ref{sec:benchModels}. \textit{NPD\_Cor5\_R} will relate to the model that was trained using the top 5 features from the correlation analysis, \textit{NPD\_Cor10\_R} is the top 10 features, \textit{NPD\_Cor15\_R} is the top 15 features, \textit{NPD\_Cor20\_R} is the top 20 features. The highest results for each performance metric are highlighted in green.

The results from Table \ref{table:NPDCorrModelResults} are also
promising like the previous experiment three of the models AUC outperform the benchmark model. \textit{NPD\_Cor10\_R} looks the strongest of them all for predicting defaults as it has the largest recall score.  \textit{NPD\_Cor15\_R},  \textit{NPD\_Cor20\_R} recall and AUC are strong but because  \textit{NPD\_Cor10\_R} has less features it is chosen as the best.

\section{Feature Selection}
As mentioned in the research literature in Section \ref{sec2:featureSection} feature selection is important when building a predictive model for reasons such as reducing the complexity of the model, mitigating the risk of over-fitting, overhead involved with having to understand and maintain a larger number of features, model training and computation time and when you need evaluate/explain your results. In summary simpler in the majority of cases is better.

Therefore a number of feature selection process will be experimented with in this section to try and identify the features that are of key importance, reducing the complexity of the model and identifying the important features. The two process that will be used are \textit{Information Gain Feature Importance} and \textit{Random Forest Feature Importance}

The feature selection process of this experiment was only carried out on the training partitions of the dataset.

\subsection{Information Gain Feature Importance}
Information gain is an approach that utilises measures commonly seen when a decision tree model is being trained (See Section \ref{decTrees}). It calculates and ranks features using entropy and information gain. For each experiment the existing scorecard features and macro-economic features as part of this experiment were analysed. Since we are only interested in the identifying the importance of macro-economic features as part of this research the existing features were stripped out of the result as we cannot tamper with the benchmark model dataset as this could lead to misleading results. Due to the risks previously discussed with having too many features in your training dataset only the top 20 features will be included in models to be trained along with the existing scorecard features as part of this experiment.

\subsubsection{Information Gain for Previous Delinquency Data} \label{IGPDExper}
Addressing the \textit{Previous Delinquency} dataset the information gain was calculated for each of the existing scorecard features and macro-economic features. Details of the feature importance can be seen in Fig. \ref{fig:DelinqInformationGainAnalysis}

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{DelinqInformationGainAnalysis}
	\caption{Top 20 Macro-Economic Feature Calculated by \\ Information Gain on Previous Delinquency Dataset}
	\label{fig:DelinqInformationGainAnalysis}
\end{figure}

As can be seen in Fig. \ref{fig:DelinqInformationGainAnalysis} the results from this test are not very promising. The information gain is very small for features which suggests the features were not any better at explaining the target feature than the existing scorecard features. 

The features that show strongest performance \textit{ED\_LOAN}, \textit{ED\_HOME} as based on default rates of personal loan and homeloan customers at electoral division. This intuitively could makes sense i.e. if people in an electoral division are struggling with their loan and mortgage repayments they are less likely to spend money in businesses in that area. \textit{ED\_LOWER\_THAN\_UPPER\_SECONDARY}, \textit{ED\_UNEMPLOYMENT} and \textit{ED\_NON\_MANUALOCCUPATION} relates to features created in the census data with low levels of education and high unemployment rate which again intuitively at least make sense. If there are people in an area with low levels of education and cant get work they're less likely to spend money on businesses in that area.

Despite the low information gain of all these macro economic feature in the results above the model was re-trained and tested. Separate models will be trained using the top 5, 10, 15 and 20 features from the information gain calculation. Table \ref{table:InfoGainPDModelResults} details the results for the re-trained model on the test data using the features selected as part of the information gain calculation.

\begin{table}[H]
\centering
\small
		\begin{tabular}{l r r r r}
			\hline
			\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
			\textit{PD\_Bench\_R} & 0.542 & 0.696 & 0.619 & \cellcolor{green!25}0.654 \\ \hline
			\textit{PD\_IG5\_R} & 0.540 & \cellcolor{green!25}0.715 & 0.627 & 0.652   \\ 
			\textit{PD\_IG10\_R} & \cellcolor{green!25}0.548 & 0.693 & 0.621 & 0.649  \\ 
			\textit{PD\_IG15\_R} & 0.536 & 0.690 & 0.613 & 0.650  \\
			\textit{PD\_IG20\_R} & 0.544 & 0.711 & \cellcolor{green!25}0.628 & 0.651 \\\hline 
		\end{tabular}
	\caption{Previous Delinquency Model results when most important \\Macro-Economic features calculated using Information Gain were included in training.\\
		*BA = Balanced Accuracy}
	\label{table:InfoGainPDModelResults}
\end{table}

The results from including macro-economic features based on the information gain feature importance are compared against the benchmark model (\textit{PD\_Bench\_R}) built in Section \ref{sec:benchModels}. \textit{PD\_IG5\_R} will relate to the model that was trained using the top 5 features from the information gain calculation, \textit{PD\_IG10\_R} is the top 10 features, \textit{PD\_IG15\_R} is the top 15 features, \textit{PD\_IG20\_R} is the top 20 features. The highest results for each performance metric are highlighted in green.

None of the models trained in this experiment performed better under the AUC which identifies the best model accuracy of the model over all possible thresholds. This is not really surprising none of the features exhibited useful information in the information gain analysis. The \textit{PD\_IG5\_R}, \textit{PD\_IG20\_R} models have a larger specificity than the benchmark meaning the model was able to identify a larger proportion of the negative cases correctly. \textit{PD\_IG10\_R} and \textit{PD\_IG20\_R} both returned higher balanced accuracy and recall. Higher recall means both models predicted higher percentage of the positive class correctly. Although this is an improvement on the benchmark models the results are not markedly better and would need further investigation to test for statistical significance.

\subsubsection{Information Gain for No Previous Delinquency Data}\label{IGNPDExper}
Addressing the \textit{No Previous Delinquency} dataset the information gain was calculated for each of the existing scorecard features and macro-economic features. Details of the feature importance can be seen in Fig. \ref{fig:NoDelinqInformationGainAnalysis}

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{NoDelinqInformationGainAnalysis}
	\caption{Top 20 Macro-Economic Feature Calculated by \\ Information Gain on No Previous Delinquency Dataset}
	\label{fig:NoDelinqInformationGainAnalysis}
\end{figure}

It can be seen in Fig. \ref{fig:NoDelinqInformationGainAnalysis} the results from this test are not very promising as with the previous experiment. The information gain is very small for features which suggests the features were not any better at explaining the target feature than the existing scorecard features. 

The features that show strongest performance are based on SME default rates in an electoral division (See Table \ref{SMEDefaultBehaviourDataset}). The features that show strongest performance \textit{ED PERCENT 30 06 2014}, \textit{ED CNT 1 30 06 2014} as based on the default rate of SME customers and the number of SME customers in default at an electoral division. \textit{DIFF 12 2013} to \textit{ED DIFF 06 2012 ED} are based on the percentage change of default rates.  If defaulting is trending up in an area this could be indicative of further defaulters, likewise if default rate is going down then consumer spend must be good in these areas and there is smaller risk of default is the future

Again despite the low information gain of all macro economic feature in the results above the model was re-trained and tested including the most important features from the information gain calculation. Separate models were trained using the top 5, 10, 15 and 20 features from the information gain calculation. Table \ref{table:InfoGainNPDModelResults} details the results for the re-trained model on the test data using the features selected as part of the information gain calculation.


\begin{table}[H]
\centering
\small
		\begin{tabular}{l r r r r}
			\hline
			\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
			\textit{NPD\_Bench\_R} & 0.525 & \cellcolor{green!25}0.739 & 0.632 & \cellcolor{green!25}0.671   \\ \hline
			\textit{NPD\_IG5\_R} & \cellcolor{green!25}0.542 & 0.736 & \cellcolor{green!25}0.639 & 0.667   \\ 
			\textit{NPD\_IG10\_R} & 0.536 & 0.736 & 0.636 & 0.667  \\ 
			\textit{NPD\_IG15\_R} & 0.520 & 0.732 & 0.626 & 0.663 \\
			\textit{NPD\_IG20\_R} &  0.508 & 0.730 & 0.619 & 0.665  \\\hline 
		\end{tabular}
	\caption{No Previous Delinquency Model results when most important \\Macro-Economic features calculated using Information Gain were included in training.\\
		*BA = Balanced Accuracy}
	\label{table:InfoGainNPDModelResults}
\end{table}

The results from including macro-economic features based on the information gain feature importance are compared against the benchmark model (\textit{PD\_Bench\_R}) built in Section \ref{sec:benchModels}. \textit{NPD\_IG5\_R} will relate to the model that was trained using the top 5 features from the information gain calculation, \textit{NPD\_IG10\_R} is the top 10 features, \textit{NPD\_IG15\_R} is the top 15 features, \textit{NPD\_IG20\_R} is the top 20 features. The highest results for each performance metric are highlighted in green.

None of four models (\textit{NPD\_IG5\_R}, \textit{NPD\_IG10\_R}, \textit{NPD\_IG15\_R}, \textit{NPD\_IG20\_R}) outperformed the benchmark model in terms of the AUC which identifies the best model based over all possible thresholds. \textit{NPD\_IG5\_R} performed the strongest by scoring a highest recall and balanced accuracy of models compared. This result is promising as it means this model is capturing almost 2\% more of the defaulters. This result could be significant for in identifying higher percentage of SME customers that will default in the future. Tests would need to be carried out to identify statistical significance.


\subsection{Random Forest Feature Importance}
A random forest is a very common and popular method of choosing features for predictive modelling. It is an extension of the previously discussed decision tree (See Section \ref{decTrees} and Section \ref{boosting}). The approach is to create large number of decision trees and then combine them together to make a classification, hence why it's called a forest. For each decision tree in the forest, random subsets of the full training set are used, hence random. Hence this is why it is called a random forest. Random forests are also widely used in the process of feature selection as the forest is able to identify what the important features are in building the model taking information from each tree and aggregating this together to do so.

For both the \textit{Previous Delinquency} and \textit{No Previous Delinquency} experiments in this section 1000 trees will be created to evaluate the importance of all the features. Due to the risks previously discussed
with having too many features in the training dataset only the top 20 features will be
included in models to be trained along with the existing scorecard features as part of this experiment. 


\subsubsection{Feature Importance for Previous Delinquency Data using Random Forests}\label{RFPDExper}
Random forest feature importance was calculated for
each of the existing scorecard features and macro-economic features in the previous delinquency dataset. Details of the feature importance can be seen in Fig. \ref{fig:DelinqRandomForestsAnalysis} below. Unfortunately due to complex nature of random forest (1000 trees) what a good and bad score for variable importance was not clear.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{DelinqRandomForestsAnalysis}
	\caption{Top 20 Macro-Economic Feature Calculated by \\
		Random Forest Feature Importance on Previous Delinquency Dataset}
	\label{fig:DelinqRandomForestsAnalysis}
\end{figure}

It can be seen in Fig. \ref{fig:DelinqRandomForestsAnalysis} that features associated with discretionary and non discretionary spend (See Table \ref{discretionarySpend}) have strong variable importance after running random forests importance variable. It is worth noting that all of the features selected to have higher variable importance are at an electoral division level opposed to a local authority level. This could signify there has been some shift in customer discretionary and non discretionary spend at the geographic level that is useful for predicting SME defaults or non-defaults. As with the previous experiment calculating information gain  (See Fig. \ref{fig:NoDelinqInformationGainAnalysis}) the SME default rates at an electoral division was also in the top 20 most important features.

Using these results separate models will be trained using the top 5, 10, 15 and 20 features from
the random forest feature importance algorithm. Table \ref{table:RFPDModelResults} details the results for the re-trained model
on the test data using the features selected as part of the random forest feature ranking.

\begin{table}[H]
\centering
\small
		\begin{tabular}{l r r r r}
			\hline
			\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
			\textit{PD\_Bench\_R} & 0.542 & 0.696 & 0.619 & 0.654 \\ \hline
			\textit{PD\_RF5\_R} & 0.548 & 0.692 & 0.620 & \cellcolor{green!25}0.662   \\ 
			\textit{PD\_RF10\_R} & \cellcolor{green!25}0.556 & \cellcolor{green!25}0.703 & \cellcolor{green!25}0.630 & 0.655  \\ 
			\textit{PD\_RF15\_R} & 0.548 & 0.698 & 0.623 & 0.654  \\
			\textit{PD\_RF20\_R} & 0.535 & 0.698 & 0.617 & 0.651  \\\hline 
		\end{tabular}

	\caption{Previous Delinquency Model results when most important\\
Macro-Economic features calculated using Random Forest feature \\selection were included in training.
\\ *BA = Balanced Accuracy}
	\label{table:RFPDModelResults}
\end{table}

The results from including macro-economic features based on the random forest feature selection are compared against the benchmark model (\textit{PD\_Bench\_R}) built in Section \ref{sec:benchModels}. \textit{PD\_RF5\_R} will relate to the model that was trained using the top 5 features from the random forest variable importance algorithm, \textit{PD\_RF10\_R} is the top 10 features, \textit{PD\_RF15\_R} is the top 15 features, \textit{PD\_RF20\_R} is the top 20 features. The highest results for each performance metric are highlighted in green.

Results are promising from this experiment, the benchmark model did not outperform the models ran as part of this experiment for any performance measure. \textit{PD\_RF10\_R} demonstrated  very promising results returning a better result for every performance measure compared to the benchmark model, including a 1.4\% increase in classifying customers in default (recall) . The only result it was not higher in was the AUC which scored highest in the \textit{PD\_RF5\_R} model. Overall \textit{PD\_RF10\_R} looks like a promising model and further tests could prove if macro-economic features are significant to this.

\subsubsection{Feature Importance for No Previous Delinquency Data using Random Forests}\label{RFNPDExper}

Random forest feature importance was calculated for
each of the existing scorecard features and macro-economic features in the no previous delinquency dataset. Details of the feature importance can be seen in Fig. \ref{fig:NoDelinqRandomForestsAnalysis} below

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{NoDelinqRandomForestsAnalysis}
	\caption{Top 20 Macro-Economic Feature Calculated by \\
		Random Forest Feature Importance on No Previous Delinquency Dataset}
	\label{fig:NoDelinqRandomForestsAnalysis}
\end{figure}


It can be seen in Fig. \ref{fig:NoDelinqRandomForestsAnalysis} that features associated with discretionary and non discretionary (See Table \ref{discretionarySpend}) have strong variable importance after running random forests importance variable. This was also the case in the previous experiment. This could be caused due to spending shift by customers in those electoral divisions going up or down. The random forest variable importance top 20 features are based on electoral division. Lower education rates and occupation rate appear in the top 20 features as they did in information gain experiment in (See Fig. \ref{fig:DelinqInformationGainAnalysis} and Fig. \ref{fig:NoDelinqInformationGainAnalysis}).


Using these results separate models will be trained using the top 5, 10, 15 and 20 features from
the random forest feature importance algorithm. Table \ref{table:RFNPDModelResults} details the results for the re-trained model
on the test data using the features selected as part of the random forest feature ranking.

\begin{table}[H]
\centering
\small
		\begin{tabular}{l  r r r r}
\hline
\textbf{Model}  & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
\textit{NPD\_Bench\_R} & 0.525 & \cellcolor{green!25}0.739 & \cellcolor{green!25}0.632 & \cellcolor{green!25}0.671   \\ \hline
\textit{NPD\_RF5\_R}  & \cellcolor{green!25}0.526 & 0.733 & 0.630 & 0.670   \\ 
\textit{NPD\_RF10\_R} & 0.497 & 0.734 & 0.615 & 0.670 \\ 
\textit{NPD\_RF15\_R} & 0.473 & 0.732 & 0.602 & 0.664  \\
\textit{NPD\_RF20\_R} & 0.485 & 0.730 & 0.608 & 0.664  \\\hline 
		\end{tabular}

	\caption{No Previous Delinquency Model results when most important\\
		Macro-Economic features calculated using Random Forest feature \\selection were included in training.
		\\ *BA = Balanced Accuracy}
	\label{table:RFNPDModelResults}
\end{table}

The results from including macro-economic features based on the random forest feature selection are compared against the benchmark model (\textit{PD\_Bench\_R}) built in Section \ref{sec:benchModels}. \textit{NPD\_RF5\_R} will relate to the model that was trained using the top 5 features from the random forest variable importance algorithm, \textit{NPD\_RF10\_R} is the top 10 features, \textit{NPD\_RF15\_R} is the top 15 features, \textit{NPD\_RF20\_R} is the top 20 features. The highest results for each performance metric are highlighted in green.

Results from the macro economic based predictive models are not very promising from this experiment as the benchmark model outperforms the others across all performance measures, apart from \textit{NPD\_RF5\_R} which performs slightly better on recall than the benchmark model \textit{NPD\_Bench\_R}.



\section{Coarse Classification}
As mentioned in Section \ref{sec:binning}, coarse classification or binning is a process where you transform features, continuous or nominal into a simplified structure of just a number of categories. There are many benefits which have been noted in the literature for using coarse classification such as handling missing data and outliers and increasing robustness by reducing risks or over-fitting. A standard approach is to split each feature into approximately three to six groups or bins. This is done by finding cut-points in the data and evaluating the relationship with target feature using Weight of Evidence (WoE) and the Information Value (IV) to compare the predictive capability of grouped features.

For both the \textit{Previous Delinquency} and \textit{No Previous Delinquency} dataset experiments in this section coarse classification will applied to all the macro economic features where the aim will be to transform each feature into a much more simplified feature which will have up to 6 groups or bins. Coarse classification will not be applied to existing model features as this would not allow for a fair comparison with the benchmark model. It also worth noting that coarse classification must only be run on the training dataset.


\subsubsection{Coarse classification for Previous Delinquency Data}
Coarse classification will be run on the macro-economic previous delinquency features. As mentioned previously a maximum of 6 bins/groups per feature will be created as per the literature  \citep{hand_optimal_2005}. As with previous experiments in this chapter the top 5, 10, 15, 20 grouped features will be added to the existing previous delinquency model feature set and models will be generated. The results from these models will then be compared and evaluated against the results from the benchmark model.

Coarse classification is applied to the previous delinquency macro-economic features, results are shown in Fig. \ref{fig:PDCoarseSelectionVariableImportanceInformationGain}

\begin{figure}[H]
	\includegraphics[width=.6\textwidth,center]{PDCoarseSelectionVariableImportanceInformationGain}
	\caption{Feature Ranking by Information Value after Coarse Classification \\is Applied to the Macro-Economic Features of the Previous Delinquency Dataset}
	\label{fig:PDCoarseSelectionVariableImportanceInformationGain}
\end{figure}

The results from the coarse classification are interesting, there are seven binned features based on local authority and thirteen based on electoral division. In other experiments so far the feature importance has been dominated completely by electoral division features. Most (14) of the binned features are sourced from the discretionary/non-discretionary spend dataset  (See Table \ref{discretionarySpend}). The top rated feature \textit{DIFF PERCENT 06 2012 ED} is the difference in SME default rates at June 2012 and at the observation point June 2014 by electoral division. It is worth noting also that the top five or six binned features appear to be much more predictive based on the information value than the remaining features. This illustrated in in Fig. \ref{fig:Information Value using SAS Previous Delinquency Features}.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{PreviousDelinq_InformationGain_InteractiveGrouping}
	\caption{Barplot of Feature Ranking by Information Value after Coarse \\ Classification is Applied to the Macro-Economic Features of the Previous Delinquency Dataset}
	\label{fig:Information Value using SAS Previous Delinquency Features}
\end{figure}

If you look to the left of the barplot above you can see that there are five or six features which have a much higher information value than the rest.


Fig. \ref{fig:Interactive Grouping Diff Percent 06 2012 ED} demonstrates the results of the top 5 macro economic binned features.
\begin{figure}[H]
	\includegraphics[width=1\textwidth,center]{ExampleInteractiveGrouping}
	\caption{Top 5 Binned Features Results based on Information Value after Coarse \\Classification is Applied to the Macro-economic\\ Features of the Previous Delinquency Dataset}
	\label{fig:Interactive Grouping Diff Percent 06 2012 ED}
\end{figure}

Fig. \ref{fig:Interactive Grouping Diff Percent 06 2012 ED} shows demonstrates how the features  \textit{DIFF PERCENT 06 2012 ED, ED LIVE LST MTH VS 3MTH AVG LIVE MEAN, LA LIVE LST MTH VS 12MTH AVG LIVE MEAN, ED DISC LST 3MTH MED VS PRV 3MTH MED SPEND MEDIAN, ED LIVE LST MTH VS 6MTH AVG LIVE MEAN} appear in their binned state after coarse classification. 

The optimal cut-off for each bin in a feature is calculated using the WoE. Information value is used for ranking binned features importance for prediction. 

The group highlighted in the chart above shows the rules for building that bin which is highlighted the equation below
\[
\text{Group 4(Event Rate 37.75) Values} = 0.05 \leq \text{\textit{DIFF PERCENT 06 2012 ED}} < 0.16
\]
This equation is saying for all \textit{DIFF PERCENT 06 2012 ED} values greater than or equal to $0.05$ and less than $0.16$ assign then to bin 4, the event rate details what percentage of the times it happens in this example it 37.75\% of the time. It can be observed that all the features in Fig. \ref{fig:Interactive Grouping Diff Percent 06 2012 ED} got transformed into features of 5 bins which would have been the optimal result based on WoE and information value.

Using these results separate models will be trained using the top 5, 10, 15 and 20 features from
the coarse classification ranked on information value. Table \ref{table:CoarsePDModelResults} details the results for the re-trained model
on the test data using the features selected as part of the coarse classification process

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{l r r r r}
		\hline
		\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
		\textit{PD\_Bench\_SAS} & 0.529 & 0.616 & 0.573 & 0.615 \\ \hline
		\textit{PD\_Coarse5\_SAS}  & 0.543 & \cellcolor{green!25}0.645 & \cellcolor{green!25}0.594 & \cellcolor{green!25}0.627   \\ 
		\textit{PD\_Coarse10\_SAS} & \cellcolor{green!25}0.552 & 0.618 & 0.585 & 0.619  \\ 
		\textit{PD\_Coarse15\_SAS} & 0.511 & 0.618 & 0.564 & 0.59  \\
		\textit{PD\_Coarse20\_SAS} & 0.511 & 0.625 & 0.568 & 0.59  \\\hline 
	\end{tabular}
	\caption{Previous Delinquency Model results when most important\\
		Macro-Economic features calculated using coarse classification were included.
		\\ *BA = Balanced Accuracy}
	\label{table:CoarsePDModelResults}
\end{table}

The results from including macro-economic features based on the coarse classification process are compared against the benchmark model (\textit{PD\_Bench\_SAS}) built in Section \ref{sec:benchModels}. \textit{PD\_Coarse5\_SAS} will relate to the model that was trained using the top 5 features from the information value calculation, \textit{PD\_Coarse10\_SAS} is the top 10 features, \textit{PD\_Coarse15\_SAS} is the top 15 features, \textit{PD\_Coarse20\_SAS} is the top 20 features. The highest results for each performance metric are highlighted in green.

Results are very promising from this experiment, the \textit{PD\_Coarse5\_SAS} and \textit{PD\_Coarse10\_SAS} models outperformed the benchmark model (\textit{PD\_Bench\_SAS}) over every performance measure suggesting it would be valid to include these macro-economic features in the development of a model in the future. The best model appears to be \textit{PD\_Coarse5\_SAS}, it uses a less features than \textit{\textit{PD\_Coarse10\_SAS}} and has a much larger AUC. However \textit{PD\_Coarse10\_SAS} has a much larger recall value. 

These are strong results, the differences between the benchmark model \textit{PD\_Coarse5\_SAS} and coarse classification model \textit{PD\_Coarse5\_SAS} performance measures are so large that these macro economic features did improve the model.


\subsubsection{Coarse classification for No Previous Delinquency Data}

Coarse classification is applied to the no previous delinquency macro-economic features, results are shown in Fig. \ref{fig:NPDCoarseSelectionVariableImportanceInformationGain}

\begin{figure}[H]
	\includegraphics[width=0.6\textwidth,center]{NPDCoarseSelectionVariableImportanceInformationGain}
	\caption{Feature ranking by Information Value after Coarse Classification \\is Applied to the Macro-economic Features of the No Previous Delinquency Dataset}
	\label{fig:NPDCoarseSelectionVariableImportanceInformationGain}
\end{figure}


The features chosen by coarse classification are very similar to the previous experiment on the previous delinquency dataset. This time however there are more features based on local authority than on electoral division. This is first time during any of the experiments in this research where local authority features are more dominant than the electoral division features after a feature selection process. The top ranked feature \textit{DIFF PERCENT 06 2013 ED} is the difference in SME default rates at June 2013 and at the observation point June 2014 by electoral division. 

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{NoPreviousDelinq_InformationGain_InteractiveGrouping}
	\caption{Barplot of Feature Ranking by Information Value after Coarse \\ Classification is Applied to the Macro-economic Features of the No Previous\\ Delinquency Dataset}
	\label{fig:Information Value using SAS No Previous Delinquency Features}
\end{figure}

In Fig. \ref{fig:Information Value using SAS No Previous Delinquency Features} like in the previous experiment there appears to five strong features ranked by the information value. 

Using the results separate models will be trained using the top 5, 10, 15 and 20 features from
the coarse classification ranked on information value. Table \ref{table:CoarseNPDModelResults} details the results for the re-trained model
on the test data using the features selected as part of the coarse classification process

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{l r r r r}
		\hline
		\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
		\textit{NPD\_Bench\_SAS} & 0.492 & \cellcolor{green!25}0.748 & 0.620 & 0.654 \\ \hline
		\textit{NPD\_Coarse5\_SAS}  & 0.492 & 0.738 & 0.615 & 0.664   \\ 
		\textit{NPD\_Coarse10\_SAS} & 0.497 & 0.742 & 0.619 & 0.667  \\ 
		\textit{NPD\_Coarse15\_SAS} & \cellcolor{green!25}0.517 & 0.741 & \cellcolor{green!25}0.629 & \cellcolor{green!25}0.669  \\
		\textit{NPD\_Coarse20\_SAS} & 0.500 & 0.739 & 0.619 & 0.665  \\\hline 
	\end{tabular}
	\caption{{No Previous Delinquency Model results when most important\\
			Macro-Economic features calculated using coarse classification were included.
			\\ *BA = Balanced Accuracy}}
	\label{table:CoarseNPDModelResults}
\end{table}
The results from including macro-economic features based on the coarse classification process are compared against the benchmark model (\textit{PD\_Bench\_SAS}) built in Section \ref{sec:benchModels}. \textit{NPD\_Coarse5\_SAS} will relate to the model that was trained using the top 5 features from the information gain calculation, \textit{NPD\_Coarse10\_SAS} is the top 10 features, \textit{NPD\_Coarse15\_SAS} is the top 15 features, \textit{NPD\_Coarse20\_SAS} is the top 20 features. The highest results for each performance metric are highlighted in green.

Results are promising for this experiment but not as promising as the experiment for coarse classification on the previous delinquency dataset. This is because there are not promising results from the models using macro economic features until introducing 15 features, ideally the goal is to keep number of features in the model small for simplicity and robustness. However the \textit{NPD\_Coarse15\_SAS} model has outperformed the benchmark on three of the four performance measures. AUC, recall and balanced accuracy. This means across all possible thresholds this model is performing better than the benchmark (AUC), its identifying correctly more customers that will go into arrears (recall), and on average its identifying correctly more customers than will or will not go into arrears (balanced accuracy). This model also introduces a large number of features to training model which could be problematic. 

There are two hypothesis suggested for why this experiment has not been as successful for the no previous delinquency data compared to the previous delinquency data is that. One is that the class imbalance in the no previous delinquency model is so low (97:03) that the information is not getting picked up. The other is that the coarse selection results for previous delinquency data that was successful because it mainly made up of electoral division features where coarse classification results in this experiment was mainly made up of local authority features. It may be that local authority features are at not granular enough. 

\section{Addressing the Imbalance in the Dataset}
To address the class imbalance property that exists in the datasets in this experiment a number of methods that manipulate the data by re-sampling have been proposed such as \textit{random undersampling of the majority class}, \textit{random oversampling of the minority class} and \textit{synthetic sampling of the minority class} (See Section \ref{sec:imBalance}). Re-sampling of the dataset must and will only be carried out on the training set of the data.

It is decided that as part of this experiment random undersampling of the majority class would not carried out as part of the experiment. This is because there is a possibility of removing important information from the training dataset that would allow the model to identify non-defaulters.

As part of this experiment re-sampling data will be carried by means of oversampling the minority class. These experiments will evaluate whether adding macro-economic features to datasets improved the benchmark results. For practical reasons tests will not be carried out on every macro-economic feature created as part of the research. Instead features will be used from previous experiments where macro-economic demonstrated promising results. 

For the previous delinquency dataset macro economic features that were included in the \textit{PD\_RF10\_R} model in Section \ref{RFPDExper} were included in the experiment. This model performed the best of all previous delinquency models when macro economic features were included.

For the no previous delinquency dataset macro economic features that were included in the \textit{NPD\_IG5\_R} model in Section \ref{IGNPDExper} were included in this experiment. This model performed the best of all no previous delinquency models when macro economic features were included. 

It is hoped that introducing extra observations of the target class through random oversampling in the training set will improve the accuracy of the models across the performance measures for datasets including macro-economic features




\subsection{Random Oversampling of the Minority Class}
Random oversampling will be carried out using the \textit{ovun.sample} function as part of the \textit{rose} package in R\footnote{\url{https://cran.r-project.org/web/packages/ROSE/index.html}}. The ovun.sample function generates a balanced sample by over-sampling minority class examples. It can also be used to under-sample majority classes or use a combination of over and undersampling to balance the training set.

The results from the oversampling of the previous delinquency and no previous delinquency dataset can be found at Table \ref{table:oversample_train_testPD} and Table \ref{table:oversample_train_testNPD} respectively.

\subsubsection{Oversampling for Previous Delinquency Data}

It can be observed in Table \ref{table:oversample_train_testPD} that after oversampling of the previous delinquency dataset the target class distribution is now 50:50 meaning the dataset is now balanced.

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			Previous Delinquency & Training Oversample & 1,562 & 1,552 & 3,114 & 50:50\\
			& Test & 245 & 633 & 878 & 72:28\\\hline
			\textbf{Total}     & & \textbf{1,807} & \textbf{2,185} & \textbf{3,992} & \textbf{55:45} \\
			\hline 
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Test Dataset \\for Oversampled Previous Delinquency Models}
	\label{table:oversample_train_testPD}
\end{table}

Table \ref{table:overPD} below details the results from the macro-economic features included in the \textit{PD\_RF10\_R} model in Section \ref{RFPDExper} retrained on the oversampled previous delinquency balanced training dataset. The retrained model is aliased \textit{PD\_Over\_RF10\_R} below. The highest results for each performance metric are highlighted in green.

\begin{table}[H]
	\centering
	\small
		\begin{tabular}{l l l r r r r}
			\hline
			\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
			\textit{PD\_Bench\_R} & 0.542 & 0.696 & 0.619 & 0.654 \\ \hline
			\textit{PD\_RF10\_R} & 0.556 & \cellcolor{green!25}0.703 & \cellcolor{green!25}0.630 & 0.655  \\ 
			\textit{PD\_Over\_RF10\_R}  & \cellcolor{green!25}0.569 & 0.662 & 0.615 & \cellcolor{green!25}0.666   \\ \hline
		\end{tabular}
	\caption{{Oversampled Previous Delinquency Model results when most important\\
			Macro-Economic features from \textit{PD\_RF10\_R} model are included.
			\\ *BA = Balanced Accuracy}}
	\label{table:overPD}
\end{table}

The results from oversampling are promising. The retrained model \textit{PD\_Over\_RF10\_R} 1.1\%. This increase appears to be based on oversampling alone and not the introduction of the macro-economic features as there was only a marginal increase in the AUC when \textit{PD\_RF10\_R} was tested before. Specificity appears to have preformed much worse in the oversampled model but this is most likely due to threshold that was selected is not valid any more and may need to be retrained for the oversampled dataset. 

\subsubsection{Oversampling for No Previous Delinquency Data}

It can be observed in Table \ref{table:oversample_train_testNPD} that after oversampling of the no previous delinquency dataset the target class distribution is now 50:50 meaning the dataset is now balanced.

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			No Previous Delinquency & Training Oversample & 16,198 & 16,435 & 32,633 & 50:50 \\ 
			& Test & 177 & 7,070 & 7,247 & 97:03 	\\\hline
			\textbf{Total}     &  & \textbf{16,375} & \textbf{23,505} & \textbf{39,880} & \textbf{68:32} \\
			\hline
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Test Dataset \\for Oversampled No Previous Delinquency Models}
	\label{table:oversample_train_testNPD}
\end{table}

Table \ref{table:overNPD} below details the results from the macro-economic features included in the \textit{NPD\_IG5\_R} model in Section \ref{IGNPDExper} retrained on the oversampled previous delinquency balanced training dataset. The retrained model is aliased \textit{NPD\_Over\_IG5\_R} below. The highest results for each performance metric are highlighted in green.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{l l l r r r r}
		\hline
\textbf{Model} & \textbf{Recall} & \textbf{Specificity} & \textbf{BA*} & \textbf{AUC}  \\ \hline
\textit{NPD\_Bench\_R} & 0.525 & \cellcolor{green!25}0.739 & 0.632 & 0.671   \\ \hline
\textit{NPD\_IG5\_R} & 0.542 & 0.736 & \cellcolor{green!25}0.639 & 0.667   \\ 
\textit{NPD\_Over\_IG5\_R} &  \cellcolor{green!25}0.553 & 0.711 & 0.632 & \cellcolor{green!25}0.688   \\ \hline
	\end{tabular}
	\caption{{Oversampled No Previous Delinquency Model results when most \\important
			Macro-Economic features from \textit{NPD\_IG5\_R} model were included.
			\\ *BA = Balanced Accuracy}}
	\label{table:overNPD}
\end{table}

As with the previous example the oversampled model has performed very well on the AUC performance measure. This consistency of improvements across datasets is promising. The other performance measures recall, specificity and balanced accuracy did not perform that well but this is most likely caused by using the wrong threshold for the oversampling classification. 

It can be confirmed that oversampling should be considered when training a prediction model for the two datasets in this research project.




\section{Interpretation of Results and Experiment Overview}
This chapter presented the experiment implementation and evaluation of this project. 

An experimental address matching application was designed and built as part of this research project to map 1.4 million personal customers addresses and 28,000 SME addresses to two geographic regions in Ireland called Electoral Division and Local Authorities. The experiment was a huge success and 120 macro-economic features were able to be generated and tested as part of this experiment. Based on the success of limited tests work carried out as part of this research \subjectname\ are going carry out an enterprise application evaluation where they are going to assess the application built as part of this research against external vendors which provide solutions and software for address matching.

This chapter began with some data exploration where SME default trends were visualised over time. It went onto introduce a geographical informations systems (GIS) application visualised how default appears at local authority level. It was observed that local authorities of cities appear to have a higher percentage of default than rural county areas do. It was also observed that there was a much higher percentage of default on the east coast of Ireland. Also SME default rates by the electoral division in Dublin were analysed and it appears that they are not all part of the one homogeneous group. These trends and patterns back up the hypothesis that macro-economic factors have an influence on whether or not an SME defaults on their financial obligation.

Two benchmark models were trained as part of the experiment \textit{previous delinquency} and \textit{no previous delinquency}. These were used throughout the experiments to compare to predictive models that included macro-economic features. Feature selection processes such as random forest feature importance and information gain were employed along with correlation analysis to try and identify macro-economic features that would be useful in predicting whether or not a SME customer was likely to default on their financial obligations. These methods did not excel, and were not able to identify clear signs that these features were having any real impact on the model. 

Coarse classification was deployed to try and simplify the macro-economic by transforming the features from continuous into grouped features with a number of bins or categories. Feature selection was carried out on these binned features to identify useful features for the prediction model. When building a predictive model including these features on the previous delinquency dataset it was found that these macro-economic features were predictive returning better results than the benchmark model. The same result did not hold through for the no previous delinquency dataset and hypothesis have been proposed for why this is.


There was a large imbalance problem within the target class of the datasets in this experiment. An experiment was carried out that oversampled the minority class for the previous delinquency and no previous delinquency datasets. It was found in both cases that AUC which measures model performance over all possible thresholds of the model performed better after oversampling had been carried out. Based on this is is suggested that any future models using this data investigate oversampling of the minority class.




  