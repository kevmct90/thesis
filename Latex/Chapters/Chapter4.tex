% Chapter Template

\chapter{Design \& Methodology} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Design \& Methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Introduction}

This chapter will present the design of the experiment to be undertaken as part of this research. The aim of this research is to see if including macro-economic features while training a SME credit scoring model will it return better results than a model trained using the benchmark features from a historic scorecard only. 

There will be two main sections to this chapter. The first will be an exploratory evaluation building a predictive model and evaluating the benchmark features for \textit{previous delinquency} and \textit{no previous delinquency} datasets. The dataset will be split into training, validation and test as models and performance measures will be assessed as part of this experiment. This will involve training multiple models based on features selected for a historic credit scorecard. Each model will be assessed to accurately ascertain the strength of each model and make model selection for future experiments in this research. This section will also explore and evaluate the results from performance measures that will be used  to evaluate the accuracy of the model. Performance measures will be chosen to evaluate the the accuracy of the whole model and how well it generalises and also over a specific threshold. This will done using the validation dataset. 

The second part of the experiment will involve assessing the use if macro-economic features when building a predictive model. Another baseline model will be built using model and performance measure selection from part one of the this chapter using just a training and test dataset. Feature selection, coarse selection  modelling these features will be outlined. Following on from these experiments an approach to tackling the class imbalance issue will outlined using oversampling of the minority class.


\section{Benchmark Features Evaluation}\label{sec:benchFeature}
It is useful to have a baseline or benchmark model from which to form model comparisons when building a predictive model. When building a predictive model, the first thing you are trying to do is build a model which results/predictions are better than the \textit{no information rate}. This means the accuracy of the model must be better than the no information rate, which is taken to be the biggest class percentage in the data to be modelled. 

For this research it would be redundant work/research to try and build a model that is better than the information rate as there are already industry models that exist in \subjectname\ for predicting arrears which can be leveraged. For this research the Risk team in \subjectname\ have provided two feature sets that have used historically for predicting arrears. They have segmented the data into two groups, \textit{Previous Delinquency} \& \textit{No Previous Delinquency}. As discussed in Section \ref{sec:existFeatures}, this was done because features that detailed if a customer had been in default previously would be dominant in when training a model on the full population.

So two models will be built as experiments benchmarks which will be compared to the results ;later experiments. One model will be based on a feature set for customers who have been delinquent in the past and the other feature set for customers who have not been in delinquent in the past. The customers will be modelled with these features first and results will be recorded. As part of the experiment in later Sections location based features will be added to be modelled, with the aim that these features will statistically significant for predicting SME arrears. 

There were many model validation methods discussed in Section \ref{sec:modelValid} such as LOOCV, k-fold validation and hold-out validation method. Due to practicability and reproducibility it was decided that hold-out method with training/validation/test dataset was the best assess model results and performance measures in this experiment. As mentioned before at the observation point, June 2014 SME customers were selected that were not in default. The number of customers for these experiments is 27,082 which is broken down into a previous delinquency and no previous delinquency dataset. Stratified sampling on the target class to build a training/validation/test dataset. The split will be broken down as 60\% training, 20\% validation and 20\% testing. The breakdown for the these partitions can be observed in Table \ref{table:benchmark_holdout} below. 

Note: The data partitions will be consistent across all tests to mitigate the risk of misleading results.

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			          & Training       & 436 & 1,318 & 1,754 & 75:25\\
			Previous Delinquency          & Validation       & 146 & 440 & 586 & 75:25\\
			         & Test & 146 & 440 & 586 & 75:25 \\ \hline
    \textbf{Previous Previous Delinquency}     & \textbf{Total} & \textbf{728} & \textbf{2,198} & \textbf{2,926} & \textbf{75:25} \\
			         			     \hline
			          & Training & 391 & 14,103 & 14,494 & 97:03 \\ 
			No Previous Delinquency          & Validation & 130 & 4,701 & 4,831 & 97:03 	\\
			          & Test & 130 & 4,701 & 4,831 & 97:03 \\\hline
			     \textbf{No Previous Delinquency}     & \textbf{Total} & \textbf{651} & \textbf{23,703} & \textbf{24,156} & \textbf{97:03} \\
			          	\hline
		\textbf{Total } 	&     	     & \textbf{1,379} & \textbf{25,703} & \textbf{27,082} & \textbf{95:05}\\ \hline
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Validation/Test Dataset \\for Benchmark Evaluation}
	\label{table:benchmark_holdout}
\end{table}

It can be observed it Table \ref{table:benchmark_holdout} that the \textit{Good:Bad} ratio is consistent for each partition due because stratified sampling based on the target feature ensuring each partition shares the same homogeneous properties as the full dataset. 

Five different predictive models will be trained as part of this experiment which have been discussed previously in Section \ref{sec:predictModels}. Typically in \subjectname\ and industry the predictive model of choice is logistic regression. This experiment will evaluate if there is any significant difference between the results from a logistic regression compared mode compared with a decision tree, neural network, gradient boosting, and SVM. As discussed in the research these are some of the most common algorithms used in predictive modelling.

Once the each of the models have been trained they will need to assesses for performance and accuracy. Based on the literature and industry practice the AUC is the performance measure of choice for evaluating how well the model performed across all test observations. When combined with the ROC chart it allows you identify if the model generalised well or if the model has over-fitted the training data and how accurate the model is overall. However AUC does not allow you to identify what threshold should be chosen to break your predictions into goods and bads. Statistics such as the minimum misclassification rate, K-S statistic, equal precision equal recall (EPER), lift and the default threshold will be evaluated to identify the optimal performance measure to split the test results into goods and bads. 

The evaluation process used for model and performance measure is illustrated below in Fig. \ref{fig:exploratoryevaluationmodel}. 

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 12cm,center]{exploratoryevaluationmodel}
	\caption{Overview of Benchmark Evaluation Process\\for model and performance measure selection}
	\label{fig:exploratoryevaluationmodel}
\end{figure}

\subsubsection{Previous Delinquency Dataset Benchmark Evaluation}

There are 11 features in the dataset to model customers who \textbf{have been} in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total number of customers to be be modelled in the Previous Delinquency dataset approximately 25\% of customers are in default at the end of outcome window which can be seen in Table \ref{table:benchmark_holdout}. This default rate is just based on customers in this analysis and is not reflective of the enterprise default rate. The results for the benchmarks models can be seen in Table \ref{table:prevdelinqbase}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Gradient Boosting & \cellcolor{green!25}0.655 & \cellcolor{green!25}0.331 & \cellcolor{green!25}0.681 & \cellcolor{green!25}0.362 & \cellcolor{green!25}0.62 & \cellcolor{green!25}0.239 \\
			Regression & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			AutoNeural Network & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			Regression Backstep & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Forward Step & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Both & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			SVM Polynomial & 0.654 & 0.308 & 0.62 & 0.241 & 0.593 & 0.186 \\
			SVM Radial Basis Fn & 0.812 & 0.624 & 0.6 & 0.2 & 0.619 & 0.238 \\
			Decision Tree & 0.626 & 0.252 & 0.588 & 0.176 & 0.55 & 0.1 \\
			SVM Sigmoid & 0.492 & -0.016 & 0.511 & 0.241 & 0.023 & -0.018 \\
			\hline
		\end{tabular}
	}
	\caption{Previous Delinquency Benchmark Results}
	\label{table:prevdelinqbase}
\end{table}

The model that outputted the best result as seen in Table \ref{table:prevdelinqbase} highlighted in green above was the Gradient Boosting Model(Ada), this is based on the highest validation AUC = 0.681. We can see above Table \ref{table:prevdelinqbase} and Fig. \ref{fig:Delinq_Model_ROC} below that the Gradient Boosting Model generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_Delinq_Model_ROC}
	\caption{Previous Delinquency Benchmark Model ROC Charts}
	\label{fig:Delinq_Model_ROC}
\end{figure}

It can be observed that the SVM Radial Basis Fn model does not generalise well where it appears to have completely over-fitted the training partition with an AUC training = 0.812 which drops to 0.60 and 0.619 for the validation and testing partitions datasets respectively. Overall most of the models appears to be predictive and have generalised quite well, however there may be case for investigating and removing the SVM Radial Basis Fn, Decision Tree and SVM Sigmoid as these appear to not be predictive, not generalised well or to have over-fitted the training data partition. There is no one stand-out model from the results in Table \ref{table:prevdelinqbase} and there is an argument that logistic regression should be continued to be used. 


Next we will evaluate the threshold that will be selected to evaluate the model. Results can observed in Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} and Table \ref{table:DelinquencyModelCutoff}. 	

\begin{figure}[H]
	\begin{adjustwidth}{-2cm}{-2cm}
	\centering
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Default_50}\caption{Default Cut-Off = 0.50}\label{fig:Base_Delinq_Model_CutOff_Analysis_Default_50}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_48}
		\caption{Min Misclassification Rate Cut-Off = 0.48}\label{fig:Base_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_54}
	\end{subfigure} 
	\newline
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Event_KS_25}
		\caption{K-S Cut-Off = 0.25}\label{fig:Base_Delinq_Model_CutOff_Analysis_Event_KS_25}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_29}
		\caption{EPER Cut-Off = 0.29}\label{fig:Base_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_29}
	\end{subfigure}
	\caption{Delinquency Model Cut-off Analysis Confusion Matrix}
	\label{fig:Base_Delinq_Model_CutOff_Analysis}
\end{adjustwidth}
\end{figure}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l|l|r|r|r|r|r|}
			\hline
\textbf{Cut-off} & \textbf{Method}       & \textbf{Accuracy} & \textbf{Recall} & \textbf{Specificity} & \textbf{MR*} & \textbf{BA*} \\ \hline
0.5              & Default (Valid)       & 0.763             & 0.055           & 0.998                & 0.237   & 0.526    \\
0.5              & Default (Test)        & 0.765             & 0.062           & 0.998                & 0.235      & 0.53  \\ \hline
0.48             & Min MR (Valid)        & 0.758             & 0.062           & 0.989                & 0.242       & 0.526 \\
0.48             & Min MR (Test)         & \cellcolor{yellow!25}0.765         & 0.068 & \cellcolor{yellow!25}0.995                & \cellcolor{yellow!25}0.235    & 0.532   \\ \hline
0.25             & K-S* (Valid) 		     & 0.666             & 0.548           & 0.705                & 0.334     & 0.627  \\
0.25             & K-S* (Test)  		     & 0.630             & \cellcolor{yellow!25}0.507           & 0.670                & 0.370  & \cellcolor{yellow!25}0.586     \\ \hline
0.29             & EPER* (Valid)          & 0.705             & 0.370           & 0.816                & 0.295    & 0.593   \\
0.29             & EPER* (Test)           & 0.688             & 0.342           & 0.802                & 0.312  & 0.572    \\ \hline
\end{tabular}
	}
	\caption{Previous Delinquency Cut-off Results 
	\\ K-S = Kologorov-Smirnov Statistic
	\\ MR = Misclassification Rate
	\\ BA = Balanced Accuracy
	\\ EPER = Event Precision Equals Recall
		}
	\label{table:DelinquencyModelCutoff}
\end{table}

It can be observed in Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} and \ref{table:DelinquencyModelCutoff} that using the default, min misclassification and EPER methods for selecting the threshold are not very useful in this problem.

The default cut-off assumes that the dataset is balanced therefore it is not optimal for this dataset. The misclassification rate which inverse is accuracy, is made bias by the large number of true negative values it is getting correct. Basically overall it is making the making the most correct predictions but not on the true positive defaulting customers that we are really interested. 

The EPER cut-off derived where the threshold gives you precision rate and recall rate intersect. which are almost the same. With this method you are aiming to classify a large number of true positive will not classifying too many false positives. 

The KS statistic aim to set the threshold that maximises the separation between good and bads. As discussed in Section \ref{modelPerformMeasure} it widely used in credit scoring industry for selecting the threshold cut-off. Looking at the results from Table \ref{table:DelinquencyModelCutoff} it scores the best results in Recall which tells us how well its being able to classify all the customers in default. Balanced accuracy which is not biased to large imbalanced also performed best using the KS statistic. It is also worth noting that the KS statistic sets a threshold equal to the imbalance ratio in the dataset. For selecting the threshold of future experiments on the previous delinquency dataset the KS statistic will be used. 



\subsubsection{No Previous Delinquency}

There are 9 features in the dataset to model customers who \textbf{have not been} in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total customers to be be modelled in the No Previous Delinquency model approximately 2.7\% of customers are in default at the end of outcome window which can be seen in Table \ref{table:benchmark_holdout}. This default rate is just based on customers in this analysis and is not reflective of the enterprise default rate. The results for the baseline benchmarks models are as follows in Table \ref{table:NoPreviousDelinquencyBaseModelDetails}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Regression & \cellcolor{green!25}0.695 & \cellcolor{green!25}0.389 & \cellcolor{green!25}0.71 & \cellcolor{green!25}0.419 & \cellcolor{green!25}0.677 & \cellcolor{green!25}0.354 \\
			Regression Backstep & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Forward Step & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Both & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Gradient Boosting & 0.653 & 0.305 & 0.683 & 0.366 & 0.688 & 0.336 \\
			SVM Radial Fn & 0.591 & 0.182 & 0.558 & 0.116 & 0.512 & 0.025 \\
			SVM Sigmoid & 0.605 & 0.21 & 0.549 & 0.099 & 0.588 & 0.176 \\
			AutoNeural Network & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			Decision Tree & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			SVM Polynomial & 0.477 & -0.046 & 0.497 & -0.007 & 0.487 & -0.026 \\
			\hline
		\end{tabular}
	}
	\caption{No Previous Delinquency Bench Mark Results}
	\label{table:NoPreviousDelinquencyBaseModelDetails}
\end{table}

The model that outputted the best result as seen table \ref{table:NoPreviousDelinquencyBaseModelDetails} highlighted in green above  was Logistic Regression, this is based on the highest validation AUC = 0.71. We can see above table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} below that the Logistic Regression Model generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_NonDelinq_Model_ROC}
	\caption{No Previous Delinquency Model ROC Chart}
	\label{fig:NonDelinq_Model_ROC}
\end{figure}

It can be observed that most the majority of the models generalise quite well and are predictive. However it can observed in Table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} the AutoNeural Network, Decision Tree and SVM Polynomial Models are not predictive at all. These models as a result will be removed from any further analysis. Out of the remaining model logistic regression performed the best like in the previous dataset in this experiment. 



The class imbalance problem in the No Previous Delinquency dataset becomes very evident in Fig.  \ref{fig:NonDelinquencyModelCutOffAnalysis} when compared to Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis}. This issue of imbalance will be investigated further in this Chapter. The algorithm really struggles to distinguish the good from the bad customers in this dataset. Results can observed in Fig. \ref{fig:NonDelinquencyModelCutOffAnalysis} and Table \ref{table:NoDelinquencyModelCutoff}

\begin{figure}[H]
	\centering
	\begin{adjustwidth}{-2cm}{-2cm}
	\begin{subfigure}[b]{ 0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Default_50}\caption{Default Cut-Off = 0.50}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Default_50}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_21}
		\caption{Min Misclassification Cut-Off= 0.21}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_17}
	\end{subfigure} 
	\medskip \newline
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Event_KS_0269}
		\caption{K-S Cut-Off = 0.026}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Event_KS_026}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.60\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_08}
		\caption{EPER Cut-Off = 0.08}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_09}
	\end{subfigure}
	\caption{Non Delinquency Model Cut-Off Analysis}
	\label{fig:NonDelinquencyModelCutOffAnalysis}
\end{adjustwidth}
\end{figure}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l|l|r|r|r|r|r|}
			\hline
			\textbf{Cut-off} & \textbf{Method} & \textbf{Accuracy} & \textbf{Recall} & \textbf{Specificity} & \textbf{MR*} & \textbf{BA*}  \\ \hline
			0.5             & Default (Valid)  & 0.973             & 0.000           & 1.000                & 0.027  & 0.5     \\
			0.5             & Default (Test)   & 0.973             & 0.000           & 1.000                & 0.027  & 0.5     \\ \hline
			
			0.21            & Min MR* (Valid)   & \cellcolor{yellow!25}0.972             & 0.000           & \cellcolor{yellow!25}0.999                & \cellcolor{yellow!25}0.028    & 0.495   \\
			0.21            & Min MR* (Test)    & 0.972             & 0.008           & 0.999                & 0.028  & 0.499     \\ \hline
			
			0.03            & K-S (Valid)      & 0.743             & \cellcolor{yellow!25}0.538           & 0.749                & 0.257  & \cellcolor{yellow!25}0.644     \\
			0.03            & K-S (Test)       & 0.739             & 0.515           & 0.745                & 0.261  & 0.63     \\ \hline
			
			0.08            & EPER* (Valid)     & 0.942             & 0.146           & 0.964                & 0.058    & 0.556   \\
			0.08            & EPER* (Test)      & 0.942             & 0.162           & 0.963                & 0.058 & 0.563     \\ \hline
		\end{tabular}
	}
	\caption{No Previous Delinquency Cut-off Results
			\\ K-S = Kologorov-Smirnov Statistic
			\\ MR = Misclassification Rate
			\\ BA = Balanced Accuracy
			\\ EPER = Event Precision Equals Recall
			}
	\label{table:NoDelinquencyModelCutoff}
\end{table}

As observed in the previous example of the optimal cut-off/threshold used for this dataset is based on the threshold of the KS statistic. Its the best threshold for identifying a high balanced accuracy and recall compared to the other performance measures. 

It can be concluded from the experiments in this Section that the Regression model worked well in comparison to the others on both datasets therefore we will continue use it in the next experiments using macro-economic features. It was also observed that the KS statistic was the optimal performance measure at selecting a cut-off value measure the performance of the model therefore it will also be used in future experiments.

\section{Methodology for Evaluating Macro-Economic Features}
Leading on from the results in Section \ref{sec:benchFeature} this section will layout the design of the experiment to evaluate macro-economic features created as part of this research. 
 
Fig. \ref{fig:Experiment_Workflow} below illustrates at a high level design of this experiment. 

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 11cm,center]{Experiment_Workflow}
	\caption{Overview of Experiment to Evaluate Macro-Economic Features \\
		for Previous Delinquency Dataset \\
		and No Previous Delinquency Dataset}
	\label{fig:Experiment_Workflow}
\end{figure}


Data for this experiment as discussed in Chapter \ref{Chapter3} will include features that were used in a historical credit scorecard model in \subjectname\ (See Section \ref{sec:existFeatures}) along with macro-economic features built from transaction spend, census metrics, default ratios and trends (See Section \ref{sec:dataForExper}). 

Stratified sampling will on target class to build a training/test dataset will be completed. The split will broken down into 70\% training and 30\% test recommended by research completed by \cite{siddiqi_credit_2012}. We no longer need a validation dataset since parameters for the model have been tuned using the benchmark evaluation. A baseline benchmark model will be built using this 70/30 split of data which will be evaluated using the AUC and threshold from the KS statistic discussed in the previous section. Regression will be used for as the algorithm of choice for both benchmark due to its success in the previous section.

Once the benchmark model has been built we can begin to start evaluating the macro economic features. There were 116 features created as part of this research. As discussed in Section \ref{sec2:featureSection} models can suffer from the curse of dimensionality where too many features are trained in the model. This causes over-fitting and the resulting model does not generalise well when making predictions on unseen data. It is also discussed in this section that research suggests a model should encompass between 8 and 20 features as a guideline.

Therefore as part of the experiment feature selection will be used to analyse what are features have the strongest relationship with the target feature. Two methods of features selection will be visited, correlation feature based selection and information gain. The features that demonstrate the strongest relationship to the target feature will be added for model training and evaluation. The correlation based feature selection will be run over the combined datasets (previous delinquency and no previous delinquency). The information gain feature selection will be run twice, once over each homogeneous dataset separately (previous delinquency and no previous delinquency). Variable importance using the random forests method will also be applied. The random forest importance features is used to generate an importance of each feature in the training dataset. This is done by created 1000 decision trees with subsets of the training dataset features. Collating these results will identify what features were of greatest importance at the decision node of each tree.

Coarse selection will next be applied to try and reduce the macro-economic features to a manageable level. As reviewed in Section \ref{sec:binning} this method will transform all the continuous macro-economic features into nominal/ordinal features. These nominal features will have a small number of values. As discussed earlier in the literature this allows the model to become more robust, mitigating the risk of over-fitting and also allowing it handle missing values for this feature. As discussed previously the information gain of each feature will then be calculated. As with the previous method the binned features that demonstrate the strongest relationship to the target class will be added for model training and evaluation. 

As discussed earlier the distribution  of number of good and bad customers is very imbalanced in the dataset for prediction. Random over sampling is a method of tackling this issue by creating new instances of the minority dataset that the model can be trained on. These method will be applied to the dataset including macro economic features to evaluate if it these models performed better than imbalanced tests.


It must be noted that all feature and coarse selection were carried out on the same training datasets as the benchmark model. Also it must be ensured that feature and coarse selection must only be carried out on the training dataset or the model could end up over fitting the the test data which cause misleading results. All models will be trained, tested and evaluated based on results discussed in Section \ref{sec:benchFeature}. All results will then be documented for comparison with the benchmark model. 


\section{Conclusions}

