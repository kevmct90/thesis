% Chapter Template

\chapter{Design \& Methodology} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Design \& Methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Introduction}

This chapter will present the design of the experiment to be undertaken as part of this research. The aim of this research is see if including macro-economic features while training a SME credit scoring model will it return better results than a model just trained using the benchmark features from a historic scorecard. 

There will be two main sections to this chapter. The first will be an exploratory evaluation building a predictive model and evaluating the benchmark features for \textit{previous delinquency} and \textit{no previous delinquency} datasets. The dataset will be split into training, validation and test as models and performance measures will be assessed as part of this experiment. This will involve training multiple models based on features selected for a historic credit scorecard. Each model will be assessed to accurately ascertain the strength of each model and make model selection for future experiments in this research. This section will also explore and evaluate the results from performance measures that will be used  to evaluate the accuracy of the model. Performance measures will be chosen to evaluate the the accuracy of the whole model and how well it generalises and also over a specific threshold. This will done using the validation dataset. 

The second part of the experiment will involve assessing the use if macro-economic features when building a predictive model. Another baseline model will be built using model and performance measure selection from part one of the this chapter using just a training and test dataset. Feature selection, coarse selection and a K-NN approach to modelling these features will be outlined. Following on from these experiments an approach to tackling the class imbalance issue will outlined using oversampling and synthetic sampling.


\section{Benchmark Features Evaluation}\label{sec:benchFeature}
It is useful to have a baseline or benchmark model from which to form model comparisons when building a predictive model. When building a predictive model, the first thing you are trying to do is build a model which results/predictions are better than the \textit{no information rate}. This means the accuracy of the model must be better than the no information rate, which is taken to be the biggest class percentage in the data to be modelled. 

For this research it would be redundant work/research to try and build a model that is better than the information rate as there are already industry models that exist in \subjectname\ for predicting arrears which can be leveraged. For this research the Risk team in \subjectname\ have provided two feature sets that have used historically for predicting arrears. They have segmented the data into two groups, \textit{Previous Delinquency} \& \textit{No Previous Delinquency}. As discussed in Section \ref{sec:existFeatures}, this was done because features that detailed if a customer had been in default previously would be dominant in when training a model on the full population.

So two models will be built as experiments benchmarks which will be compared to the results ;later experiments. One model will be based on a feature set for customers who have been delinquent in the past and the other feature set for customers who have not been in delinquent in the past. The customers will be modelled with these features first and results will be recorded. As part of the experiment in later Sections location based features will be added to be modelled, with the aim that these features will statistically significant for predicting SME arrears. 

There were many model validation methods discussed in Section \ref{sec:modelValid} such as LOOCV, k-fold validation and hold-out validation method. Due to practicability and reproducibility it was decided that hold-out method with training/validation/test dataset was the best assess model results and performance measures in this experiment. As mentioned before at the observation point, June 2014 SME customers were selected that were not in default. The number of customers for these experiments is 27,082 which is broken down into a previous delinquency and no previous delinquency dataset. Stratified sampling on the target class to build a training/validation/test dataset. The split will be broken down as 60\% training, 20\% validation and 20\% testing. The breakdown for the these partitions can be observed in Table \ref{table:benchmark_holdout} below. 

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			          & Training       & 436 & 1,318 & 1,754 & 75:25\\
			Previous Delinquency          & Validation       & 146 & 440 & 586 & 75:25\\
			         & Test & 146 & 440 & 586 & 75:25 \\ \hline
    \textbf{Previous Previous Delinquency}     & \textbf{Total} & \textbf{728} & \textbf{2,198} & \textbf{2,926} & \textbf{75:25} \\
			         			     \hline
			          & Training & 391 & 14,103 & 14,494 & 97:03 \\ 
			No Previous Delinquency          & Validation & 130 & 4,701 & 4,831 & 97:03 	\\
			          & Test & 130 & 4,701 & 4,831 & 97:03 \\\hline
			     \textbf{No Previous Delinquency}     & \textbf{Total} & \textbf{651} & \textbf{23,703} & \textbf{24,156} & \textbf{97:03} \\
			          	\hline
		\textbf{Total } 	&     	     & \textbf{1,379} & \textbf{25,703} & \textbf{27,082} & \textbf{95:05}\\ \hline
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Validation/Test Dataset \\for Benchmark Evaluation}
	\label{table:benchmark_holdout}
\end{table}

It can be observed it Table \ref{table:benchmark_holdout} that the \textit{Good:Bad} ratio is consistent for each partition due because stratified sampling based on the target feature ensuring each partition shares the same homogeneous properties as the full dataset. 

Five different predictive models will be trained as part of this experiment which have been discussed previously in Section \ref{sec:predictModels}. Typically in \subjectname\ and industry the predictive model of choice is logistic regression. This experiment will evaluate if there is any significant difference between the results from a logistic regression compared mode compared with a decision tree, neural network, gradient boosting, and SVM. As discussed in the research these are some of the most common algorithms used in predictive modelling.

Once the each of the models have been trained they will need to assesses for performance and accuracy. Based on the literature and industry practice the AUC is the performance measure of choice for evaluating how well the model performed across all test observations. When combined with the ROC chart it allows you identify if the model generalised well or if the model has over-fitted the training data and how accurate the model is overall. However AUC does not allow you to identify what threshold should be chosen to break your predictions into goods and bads. Statistics such as the minimum misclassification rate, K-S statistic, equal precision equal recall (EPER), lift and the default threshold will be evaluated to identify the optimal performance measure to split the test results into goods and bads. 

The evaluation process used for model and performance measure is illustrated below in Fig. \ref{fig:exploratoryevaluationmodel}. 

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 12cm,center]{exploratoryevaluationmodel}
	\caption{Overview of Benchmark Evaluation Process\\for model and performance measure selection}
	\label{fig:exploratoryevaluationmodel}
\end{figure}

\subsubsection{Previous Delinquency Dataset Benchmark Evaluation}

There are 11 features in the dataset to model customers who \textbf{have been} in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total number of customers to be be modelled in the Previous Delinquency dataset approximately 25\% of customers are in default at the end of outcome window which can be seen in Table \ref{table:benchmark_holdout}. This default rate is just based on customers in this analysis and is not reflective of the enterprise default rate. The results for the benchmarks models can be seen in Table \ref{table:prevdelinqbase}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Gradient Boosting & \cellcolor{green!25}0.655 & \cellcolor{green!25}0.331 & \cellcolor{green!25}0.681 & \cellcolor{green!25}0.362 & \cellcolor{green!25}0.62 & \cellcolor{green!25}0.239 \\
			Regression & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			AutoNeural Network & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			Regression Backstep & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Forward Step & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Both & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			SVM Polynomial & 0.654 & 0.308 & 0.62 & 0.241 & 0.593 & 0.186 \\
			SVM Radial Basis Fn & 0.812 & 0.624 & 0.6 & 0.2 & 0.619 & 0.238 \\
			Decision Tree & 0.626 & 0.252 & 0.588 & 0.176 & 0.55 & 0.1 \\
			SVM Sigmoid & 0.492 & -0.016 & 0.511 & 0.241 & 0.023 & -0.018 \\
			\hline
		\end{tabular}
	}
	\caption{Previous Delinquency Benchmark Results}
	\label{table:prevdelinqbase}
\end{table}

The model that outputted the best result as seen in Table \ref{table:prevdelinqbase} highlighted in green above was the Gradient Boosting Model(Ada), this is based on the highest validation AUC = 0.681. We can see above Table \ref{table:prevdelinqbase} and Fig. \ref{fig:Delinq_Model_ROC} below that the Gradient Boosting Model generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_Delinq_Model_ROC}
	\caption{Previous Delinquency Benchmark Model ROC Charts}
	\label{fig:Delinq_Model_ROC}
\end{figure}

It can be observed that the SVM Radial Basis Fn model does not generalise well where it appears to have completely over-fitted the training partition with an AUC training = 0.812 which drops to 0.60 and 0.619 for the validation and testing partitions datasets respectively. Overall most of the models appears to be predictive and have generalised quite well, however there may be case for investigating and removing the SVM Radial Basis Fn, Decision Tree and SVM Sigmoid as these appear to not be predictive, not generalised well or to have over-fitted the training data partition. There is no one stand-out model from the results in Table \ref{table:prevdelinqbase} and there is an argument that logistic regression should be continued to be used. 


Next we will evaluate the performance measure that will be used to identify the threshold that will be selected to evaluate the model. Results can observed in Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} and Table \ref{table:DelinquencyModelCutoff}. 	

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Default_50}\caption{Default Cut-Off = 0.50}\label{fig:Base_Delinq_Model_CutOff_Analysis_Default_50}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_48}
		\caption{Min Misclassification Rate Cut-Off = 0.48}\label{fig:Base_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_54}
	\end{subfigure} 
	\medskip \newline
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Event_KS_25}
		\caption{K-S Cut-Off = 0.25}\label{fig:Base_Delinq_Model_CutOff_Analysis_Event_KS_25}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_29}
		\caption{EPER Cut-Off = 0.29}\label{fig:Base_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_29}
	\end{subfigure}
	\caption{Delinquency Model Cut-off Analysis Confusion Matrix}
	\label{fig:Base_Delinq_Model_CutOff_Analysis}
\end{figure}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r}
			\hline
			\textbf{Cut-off} & \textbf{Method}       & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{Accuracy} & \textbf{Precision} & \textbf{NPV} & \textbf{Recall} & \textbf{Specificity} & \textbf{MR} & \textbf{BA} \\ \hline
			0.5              & Default (Train)       & 23          & 6           & 413         & 1312        & 0.761             & 0.793              & 0.761        & 0.053           & 0.995                & 0.239  & 0.524      \\
			0.5              & Default (Valid)       & 8           & 1           & 138         & 439         & 0.763             & 0.889              & 0.761        & 0.055           & 0.998                & 0.237   & 0.526    \\
			0.5              & Default (Test)        & 9           & 1           & 137         & 439         & 0.765             & 0.900              & 0.762        & 0.062           & 0.998                & 0.235      & 0.53  \\ \hline
			0.48             & Min MR (Train)        & 34          & 13          & 402         & 1305        & 0.763             & 0.723              & 0.764        & 0.078           & 0.990                & 0.237     & 0.534  \\
			0.48             & Min MR (Valid)        & 9           & 5           & 137         & 435         & \cellcolor{yellow!25}0.758             & \cellcolor{yellow!25}0.643              & 0.760        & 0.062           & \cellcolor{yellow!25}0.989                & \cellcolor{yellow!25}0.242       & 0.526 \\
			0.48             & Min MR (Test)         & 10          & 2           & 136         & 438         & 0.765             & 0.833              & 0.763        & 0.068           & 0.995                & 0.235    & 0.532   \\ \hline
			0.25             & K-S (Train) & 244         & 405         & 192         & 913         & 0.660             & 0.376              & 0.826        & 0.560           & 0.693                & 0.340  & 0.627     \\
			0.25             & K-S (Valid) & 80          & 130         & 66          & 310         & 0.666             & 0.381              & \cellcolor{yellow!25}0.824        & \cellcolor{yellow!25}0.548           & 0.705                & 0.334     & \cellcolor{yellow!25}0.627  \\
			0.25             & K-S (Test)  & 74          & 145         & 72          & 295         & 0.630             & 0.338              & 0.804        & 0.507           & 0.670                & 0.370  & 0.586     \\ \hline
			0.29             & EPER (Train)          & 178         & 246         & 258         & 1072        & 0.713             & 0.420              & 0.806        & 0.408           & 0.813                & 0.287  & 0.611     \\
			0.29             & EPER (Valid)          & 54          & 81          & 92          & 359         & 0.705             & 0.400              & 0.796        & 0.370           & 0.816                & 0.295    & 0.593   \\
			0.29             & EPER (Test)           & 50          & 87          & 96          & 353         & 0.688             & 0.365              & 0.786        & 0.342           & 0.802                & 0.312  & 0.572    \\ \hline
		\end{tabular}
	}
	\caption{Previous Delinquency Cut-off Results }
	\label{table:DelinquencyModelCutoff}
\end{table}

It can be observed in Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} and \ref{table:DelinquencyModelCutoff} that using the default, min misclassification and EPER methods for selecting the threshold are not very useful in this problem.

The default cut-off assumes that the dataset is balanced therefore it is not optimal for this dataset. The misclassification rate which inverse is accuracy, is made bias by the large number of true negative values it is getting correct. Basically overall it is making the making the most correct predictions but not on the true positive defaulting customers that we are really interested. 

The EPER cut-off derived where the threshold gives you precision rate and recall rate intersect. which are almost the same. With this method you are aiming to classify a large number of true positive will not classifying too many false positives. 

The KS statistic aim to set the threshold that maximises the separation between good and bads. As discussed in Section \ref{modelPerformMeasure} it widely used in credit scoring industry for selecting the threshold cut-off. Looking at the results from Table \ref{table:DelinquencyModelCutoff} it scores the best results in Recall which tells us how well its being able to classify all the customers in default. Balanced accuracy which is not biased to large imbalanced also performed best using the KS statistic. It is also worth noting that the KS statistic sets a threshold equal to the imbalance ratio in the dataset. For selecting the threshold of future experiments on the previous delinquency dataset the KS statistic will be used. 



\subsubsection{No Previous Delinquency}

There are 9 features in the dataset to model customers who \textbf{have not been} in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total customers to be be modelled in the No Previous Delinquency model approximately 2.7\% of customers are in default at the end of outcome window which can be seen in Table \ref{table:benchmark_holdout}. This default rate is just based on customers in this analysis and is not reflective of the enterprise default rate. The results for the baseline benchmarks models are as follows in Table \ref{table:NoPreviousDelinquencyBaseModelDetails}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Regression & \cellcolor{green!25}0.695 & \cellcolor{green!25}0.389 & \cellcolor{green!25}0.71 & \cellcolor{green!25}0.419 & \cellcolor{green!25}0.677 & \cellcolor{green!25}0.354 \\
			Regression Backstep & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Forward Step & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Both & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Gradient Boosting & 0.653 & 0.305 & 0.683 & 0.366 & 0.688 & 0.336 \\
			SVM Radial Fn & 0.591 & 0.182 & 0.558 & 0.116 & 0.512 & 0.025 \\
			SVM Sigmoid & 0.605 & 0.21 & 0.549 & 0.099 & 0.588 & 0.176 \\
			AutoNeural Network & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			Decision Tree & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			SVM Polynomial & 0.477 & -0.046 & 0.497 & -0.007 & 0.487 & -0.026 \\
			\hline
		\end{tabular}
	}
	\caption{No Previous Delinquency Bench Mark Results}
	\label{table:NoPreviousDelinquencyBaseModelDetails}
\end{table}

The model that outputted the best result as seen table \ref{table:NoPreviousDelinquencyBaseModelDetails} highlighted in green above  was Logistic Regression, this is based on the highest validation AUC = 0.71. We can see above table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} below that the Logistic Regression Model generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_NonDelinq_Model_ROC}
	\caption{No Previous Delinquency Model ROC Chart}
	\label{fig:NonDelinq_Model_ROC}
\end{figure}

It can be observed that most the majority of the models generalise quite well and are predictive. However it can observed in Table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} the AutoNeural Network, Decision Tree and SVM Polynomial Models are not predictive at all. These models as a result will be removed from any further analysis. Out of the remaining model logistic regression performed the best like in the previous dataset in this experiment. 



The class imbalance problem in the No Previous Delinquency dataset becomes very evident in Fig.  \ref{fig:NonDelinquencyModelCutOffAnalysis} when compared to Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis}. This issue of imbalance will be investigated further in this Chapter. The algorithm really struggles to distinguish the good from the bad customers in this dataset. Results can observed in Fig. \ref{fig:NonDelinquencyModelCutOffAnalysis} and Table \ref{table:NoDelinquencyModelCutoff}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{ 0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Default_50}\caption{Default Cut-Off = 0.50}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Default_50}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_21}
		\caption{Min Misclassification Cut-Off= 0.21}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_17}
	\end{subfigure} 
	\medskip \newline
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Event_KS_0269}
		\caption{K-S Cut-Off = 0.026}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Event_KS_026}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_08}
		\caption{EPER Cut-Off = 0.08}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_09}
	\end{subfigure}
	\caption{Non Delinquency Model Cut-Off Analysis}
	\label{fig:NonDelinquencyModelCutOffAnalysis}
\end{figure}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r}
			\hline
			\textbf{Cut-off} & \textbf{Method} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{Accuracy} & \textbf{Precision} & \textbf{NPV} & \textbf{Recall} & \textbf{Specificity} & \textbf{MR} & \textbf{BA}  \\ \hline
			0.5             & Default (Train) & 0           & 0           & 391         & 14103       & 0.973             & NA                 & 0.973        & 0.000           & 1.000                & 0.027   & 0.5    \\
			0.5             & Default (Valid) & 0           & 0           & 130         & 4701        & 0.973             & NA                 & 0.973        & 0.000           & 1.000                & 0.027  & 0.5     \\
			0.5             & Default (Test)  & 0           & 0           & 130         & 4701        & 0.973             & NA          & 0.973        & 0.000           & 1.000                & 0.027  & 0.5     \\ \hline
			
			0.21            & Min MR (Train)  & 1           & 17          & 390         & 14086       & 0.972             & 0.056              & 0.973        & 0.003           & 0.999                & 0.028    & 0.497   \\
			0.21            & Min MR (Valid)  & 0           & 6           & 130         & 4695        & \cellcolor{yellow!25}0.972             & 0.000              & 0.973        & 0.000           & \cellcolor{yellow!25}0.999                & \cellcolor{yellow!25}0.028    & 0.495   \\
			0.21            & Min MR (Test)   & 1           & 4           & 129         & 4697        & 0.972             & 0.200              & 0.973        & 0.008           & 0.999                & 0.028  & 0.499     \\ \hline
			
			0.03            & K-S (Train)     & 208         & 3583        & 183         & 10520       & 0.740             & 0.055              & 0.983        & 0.532           & 0.746                & 0.260    &  0.639  \\
			0.03            & K-S (Valid)      & 70          & 1182        & 60          & 3519        & 0.743             & 0.056              & \cellcolor{yellow!25}0.983        & \cellcolor{yellow!25}0.538           & 0.749                & 0.257  & \cellcolor{yellow!25}0.644     \\
			0.03            & K-S (Test)     & 67          & 1199        & 63          & 3502        & 0.739             & 0.053              & 0.982        & 0.515           & 0.745                & 0.261  & 0.63     \\ \hline
			
			0.08            & EPER (Train)    & 65          & 464         & 326         & 13639       & 0.945             & 0.123              & 0.977        & 0.166           & 0.967                & 0.055  & 0.567     \\
			0.08            & EPER (Valid)    & 19          & 167         & 111         & 4534        & 0.942             & \cellcolor{yellow!25}0.102              & 0.976        & 0.146           & 0.964                & 0.058    & 0.556   \\
			0.08            & EPER (Test)     & 21          & 173         & 109         & 4528        & 0.942             & 0.108              & 0.976        & 0.162           & 0.963                & 0.058 & 0.563     \\ \hline
		\end{tabular}
	}
	\caption{No Previous Delinquency Cut-off Results}
	\label{table:NoDelinquencyModelCutoff}
\end{table}

As observed in the previous example of the performance measures used in this dataset the KS statistic is the most useful from separating correct good and bad predictions, it also the best metric for identifying a high average accuracy and recall compared to the other performance measures. 


It can be concluded from the experiments in this Section that there Regression model worked well in comparison to the others on both datasets therefore we will continue use it in the next experiments using macro-economic features. It was also observed that the KS statistic was the optimal performance measure at selecting a cut-off value measure the performance of the model therefore it will also be used in future experiments.

\section{Macro-Economic Experiment Setup }

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 11cm,center]{Experiment_Workflow}
	\caption{Default Ratios for Homeloan and Personal Loans by Local Authority}
	\label{fig:Experiment_Workflow}
\end{figure}


\pagebreak


\section{Old Sections}
This chapter will detail the design of the experiments that are to be implemented as part of the research project. It will detail how customers addresses are mapped to locations and how features are generated for these locations. It will also discuss any of the pre-processing, and feature selection and statistical techniques used to create select the data for the ABT.

The SME default problem would be considered imbalanced dataset for classification problem where algorithms make the assumption there is a equal number of good and bad classes/group \citep{japkowicz_class_2000}. The dataset used in this experiment has a small number of SME customer in default, because of this issue a number of approaches will taken to considered to address.

Statistical analysis and evaluation methods used to assess the strength of models will be detailed and discussed, also it will outline any further transformations necessary for specific algorithms. Finally criteria for deciding which model will be chosen as the best and most fit for purpose will be detailed.


\subsection{Generating the ABTs}
We now have a reference list that allows us to apply a rank to all achievable loan application events prior to completion. A method is required to join this reference list with our csv (comma seperated value) log files resulting from the MapReduce job. 

%Fine from here


\section{Building Models}
In order to avoid over-fitting models, considerations need to be made as to what data to use in training the model. Each of the models built will be trained using a training set which is a subset of the over all available samples. We discussed possible options for training set selection in subsection


\section{Evaluation Methods}
Several evaluation techniques will be applied to quantify the usefulness of all models produced. As discussed in section \ref{modelEval}, there are various ways to evaluate the performance of a classifier. A common metric used is overall accuracy (\% of records correctly classified). 

