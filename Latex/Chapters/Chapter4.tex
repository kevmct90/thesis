% Chapter Template

\chapter{Design and Methodology} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Design \& Methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Introduction}

This chapter will present the design and methodology of the experiments to be undertaken as part of this research. The aim of this research is to see if including macro-economic features while training a SME credit scoring model will return better results than a model trained using the benchmark features from a historic scorecard. 

There will be two main sections to this chapter. The first will be an exploratory evaluation building a predictive model and evaluating the benchmark features for \textit{previous delinquency} and \textit{no previous delinquency} datasets. The dataset will be split into training, validation and test as models and performance measures will be assessed as part of this experiment. This will involve training multiple models based on features selected for a historic credit scorecard to evaluate which is the best. Each model will be assessed to accurately ascertain the strength of each model and make model selection for future experiments in this research. This section will also explore and evaluate the results from performance measures that will be used  to evaluate the accuracy of the model. Performance measures will be chosen to evaluate the the accuracy of the whole model, how well it generalises and also accuracy over a specific threshold. 

The second part of the experiment will involve assessing the use of macro-economic features when building a predictive model. Another baseline model will be built using the model selected in model evaluation stage to evaluate the performance measure to use also. Feature selection, coarse selection  modelling these features will be outlined. Following on from these experiments an approach to tackling the class imbalance issue will outlined using oversampling of the minority class.


\section{Benchmark Features Evaluation}\label{sec:benchFeature}
It is useful to have a baseline or benchmark model from which to form model comparisons when building a predictive model. When building a predictive model, the first thing you are trying to do is build a model where the results/predictions are better than the \textit{no information rate}. This means the accuracy of the model must be better than the no information rate, which is taken to be the biggest class percentage in the data to be modelled. 

For this research it would be redundant work/research to try and build a model that is better than the no information rate as there are already industry models that exist in \subjectname\ for predicting arrears which can be leveraged. For this research the Risk team in \subjectname\ have provided two feature sets that have used historically for predicting arrears. They have segmented the data into two groups, \textit{Previous Delinquency} \& \textit{No Previous Delinquency}. As discussed in Section \ref{sec:existFeatures}, this was done because features that detailed if a customer had been in default previously would be dominant in a training model on the full population.

So two models will be built as benchmarks which will be compared to the results in later experiments. One model will be based on a feature set for customers who have been delinquent in the past and the other feature set for customers who have not been delinquent in the past. The customers will be modelled with these features first and results will be recorded. As part of the experiment in later Sections location based features will be added to be modelled, with the aim that these features will statistically significant for predicting SME arrears. 

There were many model validation methods discussed in Section \ref{sec:modelValid} such as LOOCV, k-fold validation and hold-out validation method. Due to practicability and reproducibility it was decided that hold-out method with training/validation/test dataset was the best assess model results using the performance measures. As mentioned before at the observation point, June 2014 SME customers were selected that were not in default. There were 27,082 customers in these experiments which is broken down into a previous delinquency and no previous delinquency dataset. Stratified sampling was used on the target class to build the training, validation and test datasets. The split will be broken down as 60\% training, 20\% validation and 20\% testing. The validation will be used for setting the threshold parameter for assessing the performance of the model. The breakdown for the these partitions can be observed in Table \ref{table:benchmark_holdout} below. 

Note: The data partitions will be consistent across all tests to mitigate the risk of misleading results.

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			          & Training       & 436 & 1,318 & 1,754 & 75:25\\
			Previous Delinquency          & Validation       & 146 & 440 & 586 & 75:25\\
			         & Test & 146 & 440 & 586 & 75:25 \\ \hline
    \textbf{Previous Previous Delinquency}     & \textbf{Total} & \textbf{728} & \textbf{2,198} & \textbf{2,926} & \textbf{75:25} \\
			         			     \hline
			          & Training & 391 & 14,103 & 14,494 & 97:03 \\ 
			No Previous Delinquency          & Validation & 130 & 4,701 & 4,831 & 97:03 	\\
			          & Test & 130 & 4,701 & 4,831 & 97:03 \\\hline
			     \textbf{No Previous Delinquency}     & \textbf{Total} & \textbf{651} & \textbf{23,703} & \textbf{24,156} & \textbf{97:03} \\
			          	\hline
		\textbf{Total } 	&     	     & \textbf{1,379} & \textbf{25,703} & \textbf{27,082} & \textbf{95:05}\\ \hline
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Validation/Test Dataset \\for Benchmark Evaluation}
	\label{table:benchmark_holdout}
\end{table}

It can be observed in Table \ref{table:benchmark_holdout} that the \textit{Good:Bad} ratio is consistent for each partition because stratified sampling based on the target feature ensuring each partition shares the same homogeneous properties as the full dataset. 

Five different predictive models will be trained as part of this experiment which have been discussed previously in Section \ref{sec:predictModels}. Typically in \subjectname\ and industry the predictive model of choice is logistic regression. This experiment will evaluate if there is any significant difference between the results from a logistic regression compared with a decision tree, neural network, gradient boosting, and SVM model. As discussed in the research these are some of the most common algorithms used in predictive modelling.

Once each of the models have been trained they will need to be assessed for their performance and accuracy. Based on the literature and industry practice the AUC is the performance measure of choice for evaluating how well the model performed across all test observations. When combined with the ROC chart it allows you identify if the model generalised well or if the model has over-fitted the training data and how accurate the model is overall. However AUC does not allow you to identify what threshold should be chosen to break your predictions into goods and bads. Statistics such as the minimum misclassification rate, K-S statistic, equal precision equal recall (EPER), lift and the default threshold will be evaluated to identify the optimal threshold to split the test results into goods and bads. 

The evaluation process used for model and performance measure is illustrated below in Fig. \ref{fig:exploratoryevaluationmodel}. 

\begin{figure}[H]
	\includegraphics[width=.9\textwidth,height = 10cm,center]{exploratoryevaluationmodel}
	\caption{Overview of Benchmark Evaluation Process\\for Model and Threshold Selection}
	\label{fig:exploratoryevaluationmodel}
\end{figure}

\subsubsection{Previous Delinquency Dataset Benchmark Evaluation}

There are 11 features in the dataset to model customers who \textbf{have been} in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total number of customers to be be modelled in the Previous Delinquency dataset approximately 25\% of customers are in default at the end of outcome window which can be seen in Table \ref{table:benchmark_holdout}. This default rate is just based on customers in this analysis and is not reflective of the enterprise default rate. The results for the benchmarks models can be seen in Table \ref{table:prevdelinqbase}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Gradient Boosting & \cellcolor{green!25}0.655 & \cellcolor{green!25}0.331 & \cellcolor{green!25}0.681 & \cellcolor{green!25}0.362 & \cellcolor{green!25}0.62 & \cellcolor{green!25}0.239 \\
			Regression & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			AutoNeural Network & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			Regression Backstep & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Forward Step & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Both & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			SVM Polynomial & 0.654 & 0.308 & 0.62 & 0.241 & 0.593 & 0.186 \\
			SVM Radial Basis Fn & 0.812 & 0.624 & 0.6 & 0.2 & 0.619 & 0.238 \\
			Decision Tree & 0.626 & 0.252 & 0.588 & 0.176 & 0.55 & 0.1 \\
			SVM Sigmoid & 0.492 & -0.016 & 0.511 & 0.241 & 0.023 & -0.018 \\
			\hline
		\end{tabular}
	}
	\caption{Previous Delinquency Benchmark Results}
	\label{table:prevdelinqbase}
\end{table}

The model that outputted the best result as seen in Table \ref{table:prevdelinqbase} highlighted in green above was the Gradient Boosting Model(Ada), this is based on the highest validation AUC = 0.681. We can see above Table \ref{table:prevdelinqbase} and Fig. \ref{fig:Delinq_Model_ROC} below that the Gradient Boosting Model generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_Delinq_Model_ROC}
	\caption{Previous Delinquency Benchmark Model ROC Charts}
	\label{fig:Delinq_Model_ROC}
\end{figure}

It can be observed that the SVM Radial Basis Fn model does not generalise well where it appears to have completely over-fitted the training partition with an AUC training = 0.812 which drops to 0.60 and 0.619 for the validation and testing partitions datasets respectively. Overall most of the models appears to be predictive and have generalised quite well, however there may be case for investigating and removing the SVM Radial Basis Fn, Decision Tree and SVM Sigmoid as these appear to not be predictive, not generalised well or to have over-fitted the training data partition. There is no one stand-out model from the results in Table \ref{table:prevdelinqbase} and there is an argument that logistic regression should be continued to be used. 


Although the AUC is very useful for detailing how well the prediction performed over all possible thresholds it is not very useful if trying to split the predictions into goods and bads. In an ideal scenario the target class distribution would 50:50 this would allow the baseline/default threshold of 0.50. When there is an imbalance in the dataset many thresholds need to be evaluated to test which one meets the business objective.

Recall measures the percentage of defaulters (positive class) that were identified correctly. Accuracy measures what percentage of the predictions made were correct. Specificity measures the percentage of the non-defaulter (negative class) that were identified correctly. Precision measures of the positive predictions made what percentage were correctly identified. Balanced accuracy is the average of the sum of recall and specificity. All of these performance measures are based on a single threshold in the predictions made, a cut-off point.

Selecting this cut-off point is important for your business objective. In predicting defaults you want to maximise recall where possible but you cannot maximise this performance measure because all the good customers will get predicted bad too. Balanced accuracy is a useful performance measure as it uses how many of the negative class and positive class are being classified correctly but because it is an average the results could be skewed by one performance measure performing well so recall and specificity will have to be checked to ensure there is no bias. So this experiment will try and maximise balanced accuracy, recall and specificity.

An experiment will be generated for the previous delinquency and no previous delinquency dataset where threshold will selected based on the validation dataset and will then evaluated on the test dataset. The default cut-off is 0.50 so it will be included in the experiment as a base. The minimum misclassification rate, Kologorov-Smirnov (K-S) statistic, and event precision event recall (EPER) methods of evaluating cut-off values for this problem will be use.
K-S statistic measures at what cut-off the maximum difference between the cumulative positive and negative distributions of the predicted positive and negative class are. EPER measures at what value the cut-off precision and recall measures are the same. The min misclassification rate is the cut-off where the model makes the most correct prediction. 

Results for the previous delinquency data can observed below in Table \ref{table:DelinquencyModelCutoff}. 	

\begin{table}[H]
	\centering
\small
		\begin{tabular}{|l|l|r|r|r|r|}
			\hline
			\textbf{Cut-off} & \textbf{Method}       & \textbf{Accuracy} & \textbf{Recall} & \textbf{Specificity}  & \textbf{BA*} \\ \hline
			0.5              & Default (Valid)       & 0.763             & 0.055           & 0.998                  & 0.526    \\
			0.5              & Default (Test)        & 0.765             & 0.062           & 0.998                    & 0.53  \\ \hline
			0.48             & Min MR (Valid)        & 0.758             & 0.062           & 0.989                       & 0.526 \\
			0.48             & Min MR (Test)         & \cellcolor{yellow!25}0.765         & 0.068                & \cellcolor{yellow!25}0.995    & 0.532   \\ \hline
			0.25             & K-S* (Valid) 		     & 0.666             & 0.548           & 0.705                    & 0.627  \\
			0.25             & K-S* (Test)  		     & 0.630             & \cellcolor{yellow!25}0.507                          & 0.670  & \cellcolor{yellow!25}0.586     \\ \hline
			0.29             & EPER* (Valid)          & 0.705             & 0.370           & 0.816                    & 0.593   \\
			0.29             & EPER* (Test)           & 0.688             & 0.342           & 0.802                 & 0.572    \\ \hline
		\end{tabular}
	\caption{Previous Delinquency Cut-off Results 
			\\ K-S = Threshold that's optimal for Kologorov-Smirnov Statistic
			\\ Min MR = Threshold that's optimal for Min Misclassification Rate
			\\ EPER = Threshold that's optimal for Event Precision Equals Recall
			\\ BA = Balanced Accuracy = $\frac{Recall + Specificity }{2}$
		}
	\label{table:DelinquencyModelCutoff}
\end{table}

The validation data is used to choose the cut-off for the measures and the test data is then evaluated to see how they perform on unseen data. Optimal values are coloured in yellow in Table \ref{table:DelinquencyModelCutoff}.

The min misclassification rate calculates the accuracy of model, but because the model is so heavily imbalanced the majority negative class (good customers) is skewing the results. It can be seen that the cut-off that gives the min misclassification rate scores accuracy very well, but terribly on the recall so it will be discounted.

The EPER cut-off derives the threshold where the precision rate and recall rate intersect. This method performs extremely well on the specificity, while also performing at a decent level on the recall.

The KS statistic gives threshold that maximises the separation between good and bads. It scores the optimal results as it is scores recall the highest and balanced accuracy while specificity also performs well. For these reason the threshold that will be used when building the benchmark model for previous delinquency will be where the cut-off value is equal to 0.25.


\subsubsection{No Previous Delinquency}

There are 9 features in the dataset to model customers who \textbf{have not been} in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total customers to be be modelled in the No Previous Delinquency model approximately 2.7\% of customers are in default at the end of outcome window which can be seen in Table \ref{table:benchmark_holdout}. This default rate is just based on customers in this analysis and is not reflective of the enterprise default rate. The results for the baseline benchmarks models are as follows in Table \ref{table:NoPreviousDelinquencyBaseModelDetails}.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Regression & \cellcolor{green!25}0.695 & \cellcolor{green!25}0.389 & \cellcolor{green!25}0.71 & \cellcolor{green!25}0.419 & \cellcolor{green!25}0.677 & \cellcolor{green!25}0.354 \\
			Regression Backstep & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Forward Step & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Both & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Gradient Boosting & 0.653 & 0.305 & 0.683 & 0.366 & 0.688 & 0.336 \\
			SVM Radial Fn & 0.591 & 0.182 & 0.558 & 0.116 & 0.512 & 0.025 \\
			SVM Sigmoid & 0.605 & 0.21 & 0.549 & 0.099 & 0.588 & 0.176 \\
			AutoNeural Network & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			Decision Tree & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			SVM Polynomial & 0.477 & -0.046 & 0.497 & -0.007 & 0.487 & -0.026 \\
			\hline
		\end{tabular}
	}
	\caption{No Previous Delinquency Bench Mark Results}
	\label{table:NoPreviousDelinquencyBaseModelDetails}
\end{table}

The model that outputted the best result highlighted in green in Table \ref{table:NoPreviousDelinquencyBaseModelDetails} was the Logistic Regression, this is based on the highest validation AUC = 0.71. We can see above in Table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} below that the Logistic Regression Model generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_NonDelinq_Model_ROC}
	\caption{No Previous Delinquency Model ROC Chart}
	\label{fig:NonDelinq_Model_ROC}
\end{figure}

It can be observed that the majority of the models generalise quite well and are predictive. However it can observed in Table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} the AutoNeural Network, Decision Tree and SVM Polynomial Models are not predictive at all. These models as a result will be removed from any further analysis. Out of the remaining model logistic regression performed the best like in the previous delinquency dataset in this experiment. 

An experiment for the no previous delinquency dataset to evaluate what threshold will now be carried out like in the previous delinquency dataset. Results for the no previous delinquency data can observed below in Table. \ref{table:NoDelinquencyModelCutoff}.

\begin{table}[H]
	\centering
	\small
		\begin{tabular}{|l|l|r|r|r|r|}
			\hline
			\textbf{Cut-off} & \textbf{Method} & \textbf{Accuracy} & \textbf{Recall} & \textbf{Specificity}  & \textbf{BA*}  \\ \hline
			0.5             & Default (Valid)  & 0.972             & 0.000           & 1.000                  & 0.5     \\
			0.5             & Default (Test)   & 0.972             & 0.000           & 1.000                  & 0.5     \\ \hline
			
			0.21            & Min MR* (Valid)   & 0.972             & 0.000           & 0.999                    & 0.495   \\
			0.21            & Min MR* (Test)    & \cellcolor{yellow!25}0.972             & 0.008                         & \cellcolor{yellow!25}0.99  & 0.499     \\ \hline
			
			0.03            & K-S (Valid)      & 0.743             & 0.538           & 0.749                 & 0.644     \\
			0.03            & K-S (Test)       & 0.739             & \cellcolor{yellow!25}0.515                          & 0.745  & \cellcolor{yellow!25}0.63     \\ \hline
			
			0.08            & EPER* (Valid)     & 0.942             & 0.146           & 0.964                   & 0.556   \\
			0.08            & EPER* (Test)      & 0.942             & 0.162           & 0.963                 & 0.563     \\ \hline
		\end{tabular}
	\caption{No Previous Delinquency Cut-off Results
			\\ K-S = Threshold that's optimal for Kologorov-Smirnov Statistic
			\\ MR = Threshold that's optimal for Misclassification Rate
			\\ EPER = Threshold that's optimal for Event Precision Equals Recall
			\\ BA = Balanced Accuracy = $\frac{Recall + Specificity }{2}$
			}
	\label{table:NoDelinquencyModelCutoff}
\end{table}

The validation data is used to choose the cut-off value and the test data is then evaluated to see how they perform on unseen data. Optimal values are coloured in yellow in Table \ref{table:NoDelinquencyModelCutoff}.

As with the previous experiment evaluating on the previous delinquency dataset the threshold for the K-S statistic performs significantly better when analysing the recall, specificity and balanced accuracy. For this reason  the threshold that will be used when building the benchmark model for the no previous delinquency dataset will be where the cut-off value is equal to 0.03.

\section{Methodology for Evaluating Macro-Economic Features}
Leading on from the results in Section \ref{sec:benchFeature} this section will layout the design of the experiment to evaluate macro-economic features created as part of this research. 
 
Fig. \ref{fig:Experiment_Workflow} below illustrates at a high level design of this experiment. 

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 11cm,center]{Experiment_Workflow}
	\caption{Overview of Experiment to Evaluate Macro-Economic Features \\
		for Previous Delinquency Dataset \\
		and No Previous Delinquency Dataset}
	\label{fig:Experiment_Workflow}
\end{figure}


Data for this experiment as discussed in Chapter \ref{Chapter3} will include features that were used in a historical credit scorecard model in \subjectname\ (See Section \ref{sec:existFeatures}) along with macro-economic features built from transaction spend, census metrics, default ratios and trends (See Section \ref{sec:dataForExper}). 

Stratified sampling will applied on target class to build the training and test datasets. The split will broken down into 70\% training and 30\% test recommended by research completed by \cite{siddiqi_credit_2012}. There is no longer a requirement for a validation dataset since parameters for the model have been tuned using the benchmark evaluation. A baseline benchmark model will be built using this 70/30 split of data which will be evaluated using the AUC and balanced accuracy using the threshold from the KS statistic discussed in the previous section. Regression will be used as the algorithm of choice for both datasets due to the results in previous sections.

Once the benchmark model has been built the next phase will be to evaluate the macro economic features. There were 116 features created as part of this research. As discussed in Section \ref{sec2:featureSection} models can suffer from the curse of dimensionality where too many features are trained in the model. This can cause over-fitting and the resulting model may not generalise well when making predictions on unseen data. It is also discussed in this section that research suggests a model should encompass between 8 and 20 features as a guideline \citep{thomas_consumer_2009, mays_credit_2004}.

Therefore as part of the experiment feature selection will be used to analyse what features have the strongest relationship with the target feature. Two methods of features selection will be visited, correlation feature based selection and information gain. The features that demonstrate the strongest relationship to the target feature will be added for model training and evaluation. The correlation based feature selection will be run over the combined datasets (previous delinquency and no previous delinquency). The information gain feature selection will be run twice, once over each homogeneous dataset separately (previous delinquency and no previous delinquency). Variable importance using the random forests method will also be applied. The random forest feature importance is used to generate an importance of each feature in the training dataset. This is done by generating 1000 decision trees with subsets of the training dataset features. Collating these results will identify what features were of greatest importance at the decision node of each tree.

Coarse selection will next be applied to try and reduce the macro-economic features to a manageable level. As reviewed in Section \ref{sec:binning} this method will transform all the continuous macro-economic features into nominal/ordinal features. These nominal features will have a small number of values. As discussed earlier in the literature this allows the model to become more robust, mitigating the risk of over-fitting and also allowing it handle missing values for features. As discussed previously the information gain of each feature will then be calculated. As with the previous method the binned features that demonstrate the strongest relationship to the target class will be added for model training and evaluation. 

As discussed earlier the distribution of the number of good and bad customers is very imbalanced in the dataset for prediction. Random over sampling is a method of tackling this issue by creating new instances of the minority dataset that the model can be trained on. This method will be applied to the datasets including macro economic features to evaluate if it these models performed better than imbalanced tests.


It must be noted that all feature and coarse selection were carried out on the same training datasets as the benchmark model. Also it must be ensured that feature and coarse selection must only be carried out on the training dataset or the model could end up over fitting the the test data which may cause misleading results. All models will be trained, tested and evaluated based on results discussed in Section \ref{sec:benchFeature}. All results will then be documented for comparison with the benchmark model. 


\section{Conclusions}
This chapter focused on two main topics. The first topic of this chapter focused on designing and building an experiment to evaluate the best algorithm to build the benchmark prediction model. Decision trees, logistic regression, auto neural network, Support Vector Machine (SVM) Polynomial, SVM Radial Basis Function, SVM Sigmoid, and Gradient Boosting were evaluated. AUC was used to evaluate how accurate each model was over all possible thresholds. It was found that SVM Sigmoid model performed poorly for both the previous delinquency and no previous delinquency datasets so it should not be used in the future for this problem. The algorithms that performed best were the logistic regression and gradient boosting, they performed the best and second best in each of the datasets. It was decided because of the wide use and support for logistic regression in credit scoring that it would be chosen as the algorithm for modelling in the remaining experiments. 

AUC is the performance measure of choice for assessing how a model performed over all possible thresholds but it does not detail the threshold that should be chosen to break the predictions into goods and bads. An experiment was set-up to evaluate what threshold should be selected for maximising performance measure such as recall, specificity and balanced accuracy. It was found that the threshold using the K-S statistic gave the best overall results for recall, specificity and balanced accuracy over the previous delinquency and no previous delinquency dataset.

The second topic of this chapter details the experiment design for evaluating the predictive capability of macro-economic features in modelling credit risk for small medium enterprises. This experiment will incorporate two benchmark models one for the previous delinquency dataset and one for the no previous delinquency dataset. Macro-economic features will be put through various feature selection processes to try and and identify features that could be predictive in the model. There will also be an experiment to address the target feature imbalance in these dataset. This will be done through a sampling method \textit{random oversampling of the minority class}.