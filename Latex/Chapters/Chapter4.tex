% Chapter Template

\chapter{Design \& Methodology} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{Design \& Methodology}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Introduction}

This chapter will present the design of the experiment to be undertaken as part of this research. The aim of this research is see if including macro-economic features while training a SME credit scoring model will it return better results than a model just trained using the benchmark features from a historic scorecard. 

There will be two main sections to this chapter. The first will be an exploratory evaluation building a predictive model and evaluating the benchmark features for \textit{previous delinquency} and \textit{no previous delinquency} datasets. The dataset will be split into training, validation and test as models and performance measures will be assessed as part of this experiment. This will involve training multiple models based on features selected for a historic credit scorecard. Each model will be assessed to accurately ascertain the strength of each model and make model selection for future experiments in this research. This section will also explore and evaluate the results from performance measures that will be used  to evaluate the accuracy of the model. Performance measures will be chosen to evaluate the the accuracy of the whole model and how well it generalises and also over a specific threshold. This will done using the validation dataset. 

The second part of the experiment will involve assessing the use if macro-economic features when building a predictive model. Another baseline model will be built using model and performance measure selection from part one of the this chapter using just a training and test dataset. Feature selection, coarse selection and a K-NN approach to modelling these features will be outlined. Following on from these experiments an approach to tackling the class imbalance issue will outlined using oversampling and synthetic sampling.


\section{Benchmark Features Evaluation}
It is useful to have a baseline or benchmark model from which to form model comparisons when building a predictive model. When building a predictive model, the first thing you are trying to do is build a model which results/predictions are better than the \textit{no information rate}. This means the accuracy of the model must be better than the no information rate, which is taken to be the biggest class percentage in the data to be modelled. 

For this research it would be redundant work/research to try and build a model that is better than the information rate as there are already industry models that exist in \subjectname\ for predicting arrears which can be leveraged. For this research the Risk team in \subjectname\ have provided two feature sets that have used historically for predicting arrears. They have segmented the data into two groups, \textit{Previous Delinquency} \& \textit{No Previous Delinquency}. As discussed in Section \ref{sec:existFeatures}, this was done because features that detailed if a customer had been in default previously would be dominant in when training a model on the full population.

So two models will be built as experiments benchmarks which will be compared to the results ;later experiments. One model will be based on a feature set for customers who have been delinquent in the past and the other feature set for customers who have not been in delinquent in the past. The customers will be modelled with these features first and results will be recorded. As part of the experiment in later Sections location based features will be added to be modelled, with the aim that these features will statistically significant for predicting SME arrears. 



There were many model validation methods discussed in Section \ref{sec:modelValid} such as LOOCV, k-fold validation and hold-out validation method. Due to practicability and reproducibility it was decided that hold-out method with training/validation/test dataset was the best assess model results and performance measures in this experiment. As mentioned before at the observation point, June 2014 SME customers were selected that were not in default. The number of customers for these experiments is 27,082 which is broken down into a previous delinquency and no previous delinquency dataset. Stratified sampling on the target class to build a training/validation/test dataset. The split will be broken down as 60\% training, 20\% validation and 20\% testing. The breakdown for the these partitions can be observed in Table \ref{table:benchmark_holdout} below. 

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l l r r r r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{\# Bad} & \textbf{\# Good} & \textbf{\# Observations} & \textbf{Good:Bad} \\
			\hline
			          & Training       & 436 & 1,318 & 1,754 & 75:25\\
			Previous Delinquency          & Validation       & 146 & 440 & 586 & 75:25\\
			         & Test & 146 & 440 & 586 & 75:25 \\ \hline
    \textbf{Previous Previous Delinquency}     & \textbf{Total} & \textbf{728} & \textbf{2,198} & \textbf{2,926} & \textbf{75:25} \\
			         			     \hline
			          & Training & 391 & 14,103 & 14,494 & 97:03 \\ 
			No Previous Delinquency          & Validation & 130 & 4,701 & 4,831 & 97:03 	\\
			          & Test & 130 & 4,701 & 4,831 & 97:03 \\\hline
			     \textbf{No Previous Delinquency}     & \textbf{Total} & \textbf{651} & \textbf{23,703} & \textbf{24,156} & \textbf{97:03} \\
			          	\hline
		\textbf{Total } 	&     	     & \textbf{1,379} & \textbf{25,703} & \textbf{27,082} & \textbf{95:05}\\ \hline
		\end{tabular}
	}
	\caption{Breakdown Holdout Training/Validation/Test Dataset \\for Benchmark Evaluation}
	\label{table:benchmark_holdout}
\end{table}

It can be observed it Table \ref{table:benchmark_holdout} that the \textit{Good:Bad} ratio is consistent for each partition due because stratified sampling based on the target feature ensuring each partition shares the same homogeneous properties as the full dataset. 

Five different predictive models will be trained as part of this experiment which have been discussed previously in Section \ref{sec:predictModels}. Typically in \subjectname\ and industry the predictive model of choice is logistic regression. This experiment will evaluate if there is any significant difference between the results from a logistic regression compared mode compared with a decision tree, neural network, adaboost, and SVM. As discussed in the research these are some of the most common algorithms used in predictive modelling.

Once the each of the models have been trained they will need to assesses for performance and accuracy. Based on the literature and industry practice the AUC is the performance measure of choice for evaluating how well the model performed across all test observations. When combined with the ROC chart it allows you identify if the model generalised well or if the model has over-fitted the training data and how accurate the model is overall. However AUC does not allow you to identify what threshold should be chosen to break your predictions into goods and bads. Statistics such as the minimum misclassification rate, K-S statistic, equal precision equal recall (EPER), lift and the default threshold will be evaluated to identify the optimal performance measure to split the test results into goods and bads. 

The evaluation process used for model and performance measure is illustrated below in Fig. \ref{fig:exploratoryevaluationmodel}. 

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 12cm,center]{exploratoryevaluationmodel}
	\caption{Overview of Benchmark Evaluation Process\\for model and performance measure selection}
	\label{fig:exploratoryevaluationmodel}
\end{figure}

\subsubsection{Previous Delinquency Dataset Benchmark Evaluation}

There are 11 features in the dataset to model customers who have been in default in the past. For privacy reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total number of customers to be be modelled in the Previous Delinquency dataset approximately 25\% of customers are in default at the end of outcome window. This default rate is just based on SME customers for this analysis and is not reflective of the enterprise default rate. The results for the benchmarks models are can be seen in Table \ref{table:prevdelinqbase}.
\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Gradient Boosting & \cellcolor{green!25}0.655 & \cellcolor{green!25}0.331 & \cellcolor{green!25}0.681 & \cellcolor{green!25}0.362 & \cellcolor{green!25}0.62 & \cellcolor{green!25}0.239 \\
			Dmine Regression & 0.678 & 0.356 & 0.674 & 0.349 & 0.61 & 0.22 \\
			Regression & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			AutoNeural Network & 0.65 & 0.301 & 0.672 & 0.344 & 0.597 & 0.195 \\
			Regression Backstep & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Forward Step & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			Regression Both & 0.643 & 0.287 & 0.661 & 0.323 & 0.6 & 0.2 \\
			SVM Polynomial & 0.654 & 0.308 & 0.62 & 0.241 & 0.593 & 0.186 \\
			SVM Radial Basis Fn & 0.812 & 0.624 & 0.6 & 0.2 & 0.619 & 0.238 \\
			Decision Tree & 0.626 & 0.252 & 0.588 & 0.176 & 0.55 & 0.1 \\
			SVM Sigmoid & 0.492 & -0.016 & 0.511 & 0.241 & 0.023 & -0.018 \\
			\hline
		\end{tabular}
	}
	\caption{Previous Delinquency Base Benchmark Results}
	\label{table:prevdelinqbase}
\end{table}

The model that outputted the best result as seen table \ref{table:prevdelinqbase} highlighted in green above  was the Gradient Boosting Model, this is based on the highest validation AUC = 0.681. We can see above table \ref{table:prevdelinqbase} and Fig. \ref{fig:Delinq_Model_ROC} below that the \textit{Gradient Boosting Model} generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_Delinq_Model_ROC}
	\caption{Previous Delinquency Base Benchmark Model ROC Charts}
	\label{fig:Delinq_Model_ROC}
\end{figure}

It can be observed that this is not the case for the \textit{SVM Radial Basis Fn} model where it appears to have completely \textit{over-fitted} the training partition with an AUC training = 0.812 which drops to 0.60 and 0.619 for the validation and testing partitions datasets respectively. Overall most of the models appears to be predictive and have generalised quite well, however there may be case for investigating and removing the \textit{SVM Radial Basis Fn, Decision Tree and SVM Sigmoid} as these appear to not be predictive, not generalised well or to have over-fitted the training data partition. 

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_Delinq_Model_Lift}
	\caption{Previous Delinquency Base Benchmark Model Lift Charts}
	\label{fig:Delinq_Model_Lift}
\end{figure}

We can see above in Fig. \ref{fig:Delinq_Model_Lift} that the majority of the models are returning positive results in the unseen test chart. This is very encouraging, for example if we were to contact 10\% of customers that these models decided were going to go into default, you would contact over twice as many customers that would go into default than if you used no model at all. This is very important when deciding on a business strategy and is where BE becomes extremely important. 

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Default_50}\caption{Default Cut-Off = 0.50}\label{fig:Base_Delinq_Model_CutOff_Analysis_Default_50}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_48}
		\caption{Min Misclassification Rate Cut-Off = 0.48}\label{fig:Base_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_54}
	\end{subfigure} 
	\medskip \newline
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Event_KS_25}
		\caption{K-S Cut-Off = 0.25}\label{fig:Base_Delinq_Model_CutOff_Analysis_Event_KS_25}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_29}
		\caption{EPER Cut-Off = 0.29}\label{fig:Base_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_29}
	\end{subfigure}
	\caption{Delinquency Model Cut-off Analysis Confusion Matrix}
	\label{fig:Base_Delinq_Model_CutOff_Analysis}
\end{figure}

As mentioned before knowing the business goal and aim can be very useful when trying to decide how you want to evaluate you model. The over usefulness and predictiveness of model can be got by combing the AUC with the ROC chart but sometimes this will not suffice. 

In Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} and Table \ref{table:DelinquencyModelCutoff} you can see the cut-off analysis for the best benchmark model \textit{Gradient Boosting}. As discussed in Chapter \ref{Chapter2} this analysis demonstrates that your business goal and objectives will dictate which performance metric you may want to maximise or minimise over another. 

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r}
			\hline
			\textbf{Cut-off} & \textbf{Method}       & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{Accuracy} & \textbf{Precision} & \textbf{NPV} & \textbf{Recall} & \textbf{Specificity} & \textbf{MR} & \textbf{BA} \\ \hline
			0.5              & Default (Train)       & 23          & 6           & 413         & 1312        & 0.761             & 0.793              & 0.761        & 0.053           & 0.995                & 0.239  & 0.524      \\
			0.5              & Default (Valid)       & 8           & 1           & 138         & 439         & 0.763             & 0.889              & 0.761        & 0.055           & 0.998                & 0.237   & 0.526    \\
			0.5              & Default (Test)        & 9           & 1           & 137         & 439         & 0.765             & 0.900              & 0.762        & 0.062           & 0.998                & 0.235      & 0.53  \\ \hline
			0.48             & Min MR (Train)        & 34          & 13          & 402         & 1305        & 0.763             & 0.723              & 0.764        & 0.078           & 0.990                & 0.237     & 0.534  \\
			0.48             & Min MR (Valid)        & 9           & 5           & 137         & 435         & \cellcolor{yellow!25}0.758             & \cellcolor{yellow!25}0.643              & 0.760        & 0.062           & \cellcolor{yellow!25}0.989                & \cellcolor{yellow!25}0.242       & 0.526 \\
			0.48             & Min MR (Test)         & 10          & 2           & 136         & 438         & 0.765             & 0.833              & 0.763        & 0.068           & 0.995                & 0.235    & 0.532   \\ \hline
			0.25             & K-S (Train) & 244         & 405         & 192         & 913         & 0.660             & 0.376              & 0.826        & 0.560           & 0.693                & 0.340  & 0.627     \\
			0.25             & K-S (Valid) & 80          & 130         & 66          & 310         & 0.666             & 0.381              & \cellcolor{yellow!25}0.824        & \cellcolor{yellow!25}0.548           & 0.705                & 0.334     & \cellcolor{yellow!25}0.627  \\
			0.25             & K-S (Test)  & 74          & 145         & 72          & 295         & 0.630             & 0.338              & 0.804        & 0.507           & 0.670                & 0.370  & 0.586     \\ \hline
			0.29             & EPER (Train)          & 178         & 246         & 258         & 1072        & 0.713             & 0.420              & 0.806        & 0.408           & 0.813                & 0.287  & 0.611     \\
			0.29             & EPER (Valid)          & 54          & 81          & 92          & 359         & 0.705             & 0.400              & 0.796        & 0.370           & 0.816                & 0.295    & 0.593   \\
			0.29             & EPER (Test)           & 50          & 87          & 96          & 353         & 0.688             & 0.365              & 0.786        & 0.342           & 0.802                & 0.312  & 0.572    \\ \hline
		\end{tabular}
	}
	\caption{Delinquency Model Cut-off Results }
	\label{table:DelinquencyModelCutoff}
\end{table}

Table \ref{table:DelinquencyModelCutoff} above illustrates which cut-off threshold are best for results you are trying to maximise for the previous delinquency model.

\textit{
	\url{https://cran.r-project.org/doc/contrib/Sharma-CreditScoring.pdf}
	To the dismay of optimal credit scoring cut off decision literature the KS statistic is
	heavily in use of the industry. Hand has shown that KS can be misleading and the only
	metric which matters should be the conditional bad rate given the loan is approved
	(Hand, 2005).
	That said due to prevalence of KS we show how to compute it in R as it might be needed
	in work settings. The efficient frontier trade off approach although optimal seems to not appeal to executives as making explicit and forced trade offs seems to cause cognitive
	dissonance. For some reason people in the industry are entrenched on showing 1 number
	to communicate models whether it is KS or FICO etc.
	KS is the maximum difference between the cumulative true positive and cumulative false
	positive rate. The code above calculates this using the ROC curve.
	If you do not use this cut off point the KS in essence does not mean much for actual
	separation of the cut off chosen for the credit granting decision.
}

\subsubsection{No Previous Delinquency}

There are 9 features in the dataset to model customers who have been in default in the past. For security reasons the names and descriptions of these features could not be disclosed. There is an imbalance in the dataset, of the total customers to be be modelled in the No Previous Delinquency model approximately 2.7\% of customers are in default at the end of outcome window. This default rate is just based on SME customers for this analysis and is not reflective of the enterprise default rate. The results for the baseline benchmarks models are as follows.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r |r| r|r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Train GINI} & \textbf{Valid AUC} & \textbf{Valid GINI}& \textbf{Test AUC} & \textbf{Test GINI}\\
			\hline
			\cellcolor{green!25}Regression & \cellcolor{green!25}0.695 & \cellcolor{green!25}0.389 & \cellcolor{green!25}0.71 & \cellcolor{green!25}0.419 & \cellcolor{green!25}0.677 & \cellcolor{green!25}0.354 \\
			Regression Backstep & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Forward Step & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Regression Both & 0.691 & 0.381 & 0.706 & 0.413 & 0.678 & 0.357 \\
			Gradient Boosting & 0.653 & 0.305 & 0.683 & 0.366 & 0.688 & 0.336 \\
			Dmine Regression & 0.649 & 0.297 & 0.67 & 0.34 & 0.628 & 0.255 \\
			SVM Radial Fn & 0.591 & 0.182 & 0.558 & 0.116 & 0.512 & 0.025 \\
			SVM Sigmoid & 0.605 & 0.21 & 0.549 & 0.099 & 0.588 & 0.176 \\
			AutoNeural Network & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			Decision Tree & 0.5 & 0 & 0.5 & 0 & 0.5 & 0 \\
			SVM Polynomial & 0.477 & -0.046 & 0.497 & -0.007 & 0.487 & -0.026 \\
			\hline
		\end{tabular}
	}
	\caption{No Previous Delinquency Base Model Details}
	\label{table:NoPreviousDelinquencyBaseModelDetails}
\end{table}

The model that outputted the best result as seen table \ref{table:NoPreviousDelinquencyBaseModelDetails} highlighted in green above  was the \textit{Logistic Regression Model}, this is based on the highest validation AUC = 0.71. We can see above table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} below that the \textit{Logistic Regression Model} generalises quite well across the training, validation and testing partitions.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_NonDelinq_Model_ROC}
	\caption{No Previous Delinquency Model ROC Chart}
	\label{fig:NonDelinq_Model_ROC}
\end{figure}

It can be observed that most the majority of the models generalise quite well and are predictive. However as can seen in Table \ref{table:NoPreviousDelinquencyBaseModelDetails} and Fig. \ref{fig:NonDelinq_Model_ROC} the \textit{AutoNeural Network, Decision Tree and SVM Polynomial Models} are not predictive. There be a case in investigate this further and the huge class imbalance may be causing an issue or to remove the models completely for further investigation.

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_NonDelinq_Model_Lift}
	\caption{No Previous Delinquency Model Lift Chart}
	\label{fig:NonDelinq_Model_Lift}
\end{figure}

As with the lift chart in the Previous Delinquency model we can see in Fig. \ref{fig:NonDelinq_Model_Lift} the majority of the models are returning positive results most notably in the the regression validation and test result as you would expect. We are seeing a larger lift than in the Previous Delinquency model also. We can see for the regression model if we were to contact 10\% of the customers that this model decided were to going to go into default we would contact three times as many customers than you if you didn't use a model at all and choose them at random. 

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{ 0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Default_50}\caption{Default Cut-Off = 0.50}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Default_50}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_21}
		\caption{Min Misclassification Cut-Off= 0.21}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Min_Misclassification_Rate_17}
	\end{subfigure} 
	\medskip \newline
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Event_KS_0269}
		\caption{K-S Cut-Off = 0.026}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Event_KS_026}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,keepaspectratio]{Base_Non_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_08}
		\caption{EPER Cut-Off = 0.08}\label{fig:Base_Non_Delinq_Model_CutOff_Analysis_Event_Precision_Equal_Recall_09}
	\end{subfigure}
	\caption{Non Delinquency Model Cut-Off Analysis}
	\label{fig:NonDelinquencyModelCutOffAnalysis}
\end{figure}

The imbalance problem in the No Previous Delinquency model becomes very evident in Fig.  \ref{fig:NonDelinquencyModelCutOffAnalysis} when compared to Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis}. This issue of imbalance will be investigated further in this Chapter.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r}
			\hline
			\textbf{Cut-off} & \textbf{Method} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{Accuracy} & \textbf{Precision} & \textbf{NPV} & \textbf{Recall} & \textbf{Specificity} & \textbf{MR} & \textbf{BA}  \\ \hline
			0.5             & Default (Train) & 0           & 0           & 391         & 14103       & 0.973             & NA                 & 0.973        & 0.000           & 1.000                & 0.027   & 0.5    \\
			0.5             & Default (Valid) & 0           & 0           & 130         & 4701        & 0.973             & NA                 & 0.973        & 0.000           & 1.000                & 0.027  & 0.5     \\
			0.5             & Default (Test)  & 0           & 0           & 130         & 4701        & 0.973             & NA          & 0.973        & 0.000           & 1.000                & 0.027  & 0.5     \\ \hline
			
			0.21            & Min MR (Train)  & 1           & 17          & 390         & 14086       & 0.972             & 0.056              & 0.973        & 0.003           & 0.999                & 0.028    & 0.497   \\
			0.21            & Min MR (Valid)  & 0           & 6           & 130         & 4695        & \cellcolor{yellow!25}0.972             & 0.000              & 0.973        & 0.000           & \cellcolor{yellow!25}0.999                & \cellcolor{yellow!25}0.028    & 0.495   \\
			0.21            & Min MR (Test)   & 1           & 4           & 129         & 4697        & 0.972             & 0.200              & 0.973        & 0.008           & 0.999                & 0.028  & 0.499     \\ \hline
			
			0.03            & K-S (Train)     & 208         & 3583        & 183         & 10520       & 0.740             & 0.055              & 0.983        & 0.532           & 0.746                & 0.260    &  0.639  \\
			0.03            & K-S (Valid)      & 70          & 1182        & 60          & 3519        & 0.743             & 0.056              & \cellcolor{yellow!25}0.983        & \cellcolor{yellow!25}0.538           & 0.749                & 0.257  & \cellcolor{yellow!25}0.644     \\
			0.03            & K-S (Test)     & 67          & 1199        & 63          & 3502        & 0.739             & 0.053              & 0.982        & 0.515           & 0.745                & 0.261  & 0.63     \\ \hline
			
			0.08            & EPER (Train)    & 65          & 464         & 326         & 13639       & 0.945             & 0.123              & 0.977        & 0.166           & 0.967                & 0.055  & 0.567     \\
			0.08            & EPER (Valid)    & 19          & 167         & 111         & 4534        & 0.942             & \cellcolor{yellow!25}0.102              & 0.976        & 0.146           & 0.964                & 0.058    & 0.556   \\
			0.08            & EPER (Test)     & 21          & 173         & 109         & 4528        & 0.942             & 0.108              & 0.976        & 0.162           & 0.963                & 0.058 & 0.563     \\ \hline
		\end{tabular}
	}
	\caption{My caption}
	\label{my-label}
\end{table}




\section{Macro-Economic Experiment Setup }

\begin{figure}[H]
	\includegraphics[width=1\textwidth,height = 11cm,center]{Experiment_Workflow}
	\caption{Default Ratios for Homeloan and Personal Loans by Local Authority}
	\label{fig:Experiment_Workflow}
\end{figure}


\pagebreak


\section{Old Sections}
This chapter will detail the design of the experiments that are to be implemented as part of the research project. It will detail how customers addresses are mapped to locations and how features are generated for these locations. It will also discuss any of the pre-processing, and feature selection and statistical techniques used to create select the data for the ABT.

The SME default problem would be considered imbalanced dataset for classification problem where algorithms make the assumption there is a equal number of good and bad classes/group \citep{japkowicz_class_2000}. The dataset used in this experiment has a small number of SME customer in default, because of this issue a number of approaches will taken to considered to address.

Statistical analysis and evaluation methods used to assess the strength of models will be detailed and discussed, also it will outline any further transformations necessary for specific algorithms. Finally criteria for deciding which model will be chosen as the best and most fit for purpose will be detailed.


\subsection{Generating the ABTs}
We now have a reference list that allows us to apply a rank to all achievable loan application events prior to completion. A method is required to join this reference list with our csv (comma seperated value) log files resulting from the MapReduce job. 

%Fine from here
\subsection{Software Used}\label{softwareUsed}

\subsubsection{Address Matching}
A combination of Apache Solr\footnote{\url{http://lucene.apache.org/solr/}} and Python\footnote{\url{https://www.python.org/}} were used to match \subjectname's addresses to an ED/LA in GDD. Solr ia an open source web application enterprise search engine. It is an open source application written in Java, which is a wrapper around the Apache Lucence \footnote{{\url{https://lucene.apache.org/core/}}}. Combined they provide a reliable, fast, scalable platform capable of providing distributed indexing which can then be used for searching or navigation. Solr was used to index GD which then allowed it to be searched. Python is a high-level, general purpose programming language which can be used to build both large and small scale programs. Python like Solr is open source and freely available. One its most attractive and best characteristics is it is easy to read and use. A program was created to standardise and cleanse the addresses in \subjectname's database. The program would then take the cleansed addresses and query them against GDD indexed through the Solr web API, using string comparison algorithms discussed in Chapter 2 to return the most likely GDD address.

\subsubsection{Data Wrangling}
Anecdotally speaking, data scientists and analysts spend majority of their time data wrangling. Data wrangling is time consuming mundane process used to collect and prepare data prior to being explored for useful information. In the experiment of this thesis a variety of data types and data sources were used. Data from the \subjectname\ EDW, GDD in Solr served in JSON, CSO data in flat file to name a few. R\footnote{{\url{https://www.r-project.org/}}}, another open source programming language but has much more emphasis on statistical computing. It also has many libraries available for processing and data manipulation which are available in the CRAN\footnote{{\url{https://cran.r-project.org/}}} repository. The most useful package used during this process was \textit{reshape}\footnote{{\url{https://cran.r-project.org/web/packages/reshape/index.html}}}. Reshape allows to easily restructure, transpose and aggregate your data. 

\subsubsection{Visualisations}
R is also a very strong programming language at creating beautiful visualisations so it will be used throughout this paper. One package used in this paper was \textit{ggplot}
\footnote{{\url{https://cran.r-project.org/web/packages/ggplot2/index.html}}}. 

Some other custom geospatial visualisations may also be required. This could be done using Arcgis, Qgis, D3 or R still to be decided.

\subsubsection{Modelling}
SQL was used to identify customers to be used for predictions and generate the target class.
The models and experiments performed in this used in this are built in R and SAS. SAS is a proprietary software. SAS is the tool of choice by the modelling teams in \subjectname. Anecdotally SAS is a legacy in financial institutions, it is what people are used to using but also there is a for-profit corporation vetting the code for its customers and customer service and support corporations are used to. SAS offers a graphical interfaces which means users do not have to enter code, but this can be complemented using the SAS programming language. Anecdotally speaking SAS is excellent for building predictive modes resulting in good time to value. SAS Enterprise Miner includes the following components Time Series, Variable Clustering, Cluster, Interactive Binning, Principal Components, AutoNeural, DMNeural, DMine Regression, Gradient Boosting, Ensemble, and Text Mining.
\\
R is a very strong competitor to SAS in this space. Because of its open source nature there are many libraries available for building predictive models. For example one popular package \textit{caret}\footnote{{\url{https://cran.r-project.org/web/packages/caret/index.html}}} contains many models and continues to grow

\section{Building Models}
In order to avoid over-fitting models, considerations need to be made as to what data to use in training the model. Each of the models built will be trained using a training set which is a subset of the over all available samples. We discussed possible options for training set selection in subsection


\section{Evaluation Methods}
Several evaluation techniques will be applied to quantify the usefulness of all models produced. As discussed in section \ref{modelEval}, there are various ways to evaluate the performance of a classifier. A common metric used is overall accuracy (\% of records correctly classified). 

