% Chapter Template

\chapter{State-of-the-art} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 2. \emph{State-of-the-art}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Introduction}
This chapter will review research literature in the field of machine learning and credit scoring. We 


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Location/Geospatial Data in Predictive Modelling}
The geography of the data  Provinces  Ireland is divided into four provinces: Leinster, Ulster, Munster and Connacht. Although at present they do not have any  administrative functions, they are relevant for a number of historical, cultural and sporting reasons. The borders of the  provinces coincide exactly with the boundaries of the administrative counties. Three of the nine counties in Ulster are  within the jurisdiction of the State.  NUTS boundaries  The Nomenclature of Territorial Units for Statistics (NUTS) were drawn up by Eurostat in order to define territorial  units for the production of regional statistics across the European Union. The NUTS classification has been used in EU  legislation since 1988, but it was only in 2003 that the EU Member States, the European Parliament and the  Commission established the NUTS regions within a legal framework.  The Irish NUTS 3 regions comprise the eight Regional Authorities established under the Local Government Act, 1991  (Regional Authorities) (Establishment) Order, 1993 which came into operation on January 1st 1994. The NUTS 2  regions, which were proposed by Government and agreed to by Eurostat in 1999, are groupings of the Regional  Authorities.  Administrative counties  In census reports the country is divided into 29 counties/administrative counties and the five Cities which represent the  local authority areas. Outside Dublin there are 26 administrative counties (North Tipperary and South Tipperary each  ranks as a separate county for administrative purposes) and four Cities, i.e. Cork, Limerick, Waterford and Galway. In  Dublin the four local authority areas are identified separately, i.e. Dublin City and the three administrative counties of  Dún Laoghaire-Rathdown, Fingal and South Dublin.  Electoral Divisions  There are 3,440 Electoral Divisions (EDs) which are the smallest legally defined administrative areas in the State. One  ED, St. Mary's, straddles the Louth-Meath county border, and is presented in two parts in the SAPS1 tables, with one  part in Louth and the other in Meath. There are 32 EDs with low population, which for reasons of confidentiality have  been amalgamated into neighbouring EDs giving a total of 3,409 EDs which appear in the SAPS tables.  Small Areas  Small Areas are areas of population comprising between 50 and 200 dwellings created by The National Institute of  Regional and Spatial Analysis(NIRSA) on behalf of the Ordnance Survey Ireland(OSi) in consultation with CSO. Small  Areas were designed as the lowest level of geography for the compilation of statistics in line with data protection and  generally comprise either complete or part of townlands or neighbourhoods. There is a constraint on Small Areas that  they must nest within Electoral Division boundaries.  Small areas were used as the basis for the Enumeration in Census 2011. Enumerators were assigned a number of  adjacent Small Areas constituting around 400 dwelling in which they had to visit every dwelling and deliver and collect  a completed census form and record the dwelling status of unoccupied dwellings.  The small area boundaries have been amended in line with population data from Census 2011  2007 Constituency boundaries  For the purpose of elections to Dáil Éireann the country is divided into Constituencies which, under Article 16.4 of the  Constitution of Ireland, have to be revised at least once every twelve years with due regard to changes in the distribution  of the population. The Constituencies were revised in 2007.  Gaeltacht Areas  The Gaeltacht Areas Orders, 1956, 1967, 1974 and 1982 defined the Gaeltacht as comprising 155 Electoral Divisions or  parts of Electoral Divisions in the counties of Cork, Donegal, Galway, Kerry, Mayo, Meath and Waterford.  2008 Local Electoral Areas  For the purposes of County Council and Corporation elections each county and city is divided into Local Electoral  Areas (LEAs) which are constituted on the basis of Orders made under the Local Government Act, 1941. In general,  LEAs are formed by aggregating Electoral Divisions. However, in a number of cases Electoral Divisions are divided  between LEAs to facilitate electors.  Legal Towns and Cities  Urban areas with legally defined boundaries consist of the five Cities (Cork, Dublin, Galway, Limerick and Waterford),  five Boroughs (Clonmel, Drogheda, Kilkenny, Sligo and Wexford) and 75 Towns as established under the Local  Government Act, 2001 (S.I. 591 of 2001). Extensions to the boundaries can also occur, subject to legislation passed  under the instruction of the Department of Environment, Community and Local Government.  Settlements (Census towns, legal towns and environs, cities and suburbs)  In order to distinguish between the urban and rural population for census analysis, the boundaries of distinct settlements  need to be defined. This requires the creation of suburbs and extensions to existing cities and legal towns as well as  delineating boundaries for settlements which are not legally defined (called Census towns).  From 1971 to 2006, Census towns were defined as a cluster of fifty or more occupied dwellings where, within a radius  of 800 metres there was a nucleus of thirty occupied dwellings (on both sides of a road, or twenty on one side of a  road), along with a clearly defined urban centre e.g. a shop, a school, a place of worship or a community centre. Census  town boundaries where extended over time where there was an occupied dwelling within 200 metres of the existing  boundary.  To avoid the agglomeration of adjacent towns caused by the inclusion of low density one off dwellings on the approach  routes to towns, the 2011 criteria were tightened, in line with UN criteria.  In Census 2011 a new Census town was defined as being a cluster with a minimum of 50 occupied dwellings, with a  maximum distance between any dwelling and the building closest to it of 100 metres, and where there was evidence of  an urban centre (shop, school etc). The proximity criteria for extending existing 2006 Census town boundaries was also  amended to include all occupied dwellings within 100 metres of an existing building. Other information based on OSi  mapping and orthogonal photography was taken into account when extending boundaries. Boundary extensions were  generally made to include the land parcel on which a dwelling was built or using other physical features such as roads,  paths etc.  Extensions to the environs and suburbs of legal towns and cities were also constructed using the 100 metre proximity  rule applied to Census towns.  1 SAPS – Small Area Population Statistics  For census reports, urban settlements are towns with a population of 1,500 or more, while settlements with a population  of less than 1,500 are classified as rural.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\section{SME Lending Arrears}

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
\section{Sampling Period}
As already stated in the thesis, predictive models are built using historical data. It must be stated that past performance can be useful predictor of default it does not guarantee that future predictions of the model will be accurate or reliable. A training dataset is built to build a predictive model, customers are observed at two different points in time \citep{martens_credit_2010}, these are called the \textit{observation point and prediction point/"default observation point"} (cite). The time period between these two points is referred to the \textit{outcome window}. This can vary based on business objectives and requirements, industry standard in \subjectname\ dictates that this usually 12 months. \\\\

Reason/Arguments for shorter/longer periods may need to be added here \\

%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------
\section{Class Label Definition ABT} \label{classLabelDef}
For customer to be defined as defaulted is dependant on what the objective of that predictive model is and the requirements of the financial institution \citep{mcnab_principles_2000}. The Basel II definition (paragraph 452) which is widely used by financial institutions and \subjectname\ considers a default to have taken place when either or both of the following criteria are met:
\vspace{-3mm} 
\begin{itemize}
	\item The bank/financial institution considers that the obligor is unlikely to pay its credit obligations to the banking group in full, without recourse by the bank to actions such as realising security (if held).
	\item The obligor is past due more than 90 days on any material credit obligation to the banking group. Overdrafts will be considered as being past due once the customer has breached an advised limit or been advised of a limit smaller than current outstandings.
\end{itemize} 

There are two well known approaches to class label definition that financial institutions can choose according to \cite{anderson_credit_2007}: (\textit{i}) a \textit{current status} label definition which classifies a customer to have defaulted or not at the end of the outcome window; or option (\textit{ii}) a \textit{worst status} label definition which classifies whether the customer has defaulted or not throughout the outcome window. It is \subjectname's industry standard to use the \textit{worst status} option. This agrees with Basel II \citep{basel_international_2006}, that customers 90 days worst status covering a one-year period is considered the standard definition for customers that have defaulted. 


%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------
\section{Feature Selection}
\subsection{Correlation-based Feature Selection}
\subsection{Stepwise Procedures}


%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------


\section{Coarse Classification/ Binning}
%----------------------------------------------------------------------------------------
%	SECTION 7
%----------------------------------------------------------------------------------------
\section{Supervised Classification Algorithms}
The research question can be qualified as a sequential data analysis problem but in other respects, it is akin to standard two-class classification; will the user complete the process or not?

\subsection{Nearest Neighbours \& Distance Measures} \label{kNN}
When the \textit{k} nearest classifications are considered, 


\subsection{Decision Trees} \label{decTrees}
Decision Tree algorithms seek to split records into classes based on the inherent characteristics of the variables in the dataset and their relationship with the target variable(class).\\Prior to splitting the data, if we were to randomly pick a record from the dataset, the degree of uncertainty as to what class we would expect it to be would be relative to the distribution of the classes in the original dataset.


\subsection{Ensemble models \& Boosting} \label{boosting}
In 1907, statistician Sir Francis Galton attended a fair in which there was a competition to judge the weight of an Ox. Upon reviewing the 787 predictions made by the competing public, he observed that while there was a 


\subsection{Regression} \label{logicReg}
Regression models involve the modelling of linear relationships between features in a feature space. In it's simplest form we can think of a basic linear regression model where there is one independent variable and one 


\subsection{Neural Networks} \label{neuralNets}
Neural Networks in broad terms are a class of model that attempt to learn patterns in data by simulating the activity in a human brain.


\subsection{Support Vector Machines} \label{SVM}
Support Vector Machines (SVM) were developed from Statistical Learning Theory 

\iffalse
\subsection{Sequential Association Rules, N-grams \& Markov Chains} \label{assosRules}
In their paper, introduce the concept of association rules for discovering statistically significant relationships between frequently occurring items in large datasets. The algorithm.
\subsection{Applications of Techniques in 2-Class Classification \& Path Prediction}\label{applic}
Compared using k-NN versus association rules as an approach to personalising recommendations on websites based on presenting users with pages that they are likely to visit next based on a history of page requests. The results showed that using association rules provided better coverage and precision than k-NN when they used a window of 4 previous page requests.
\fi


%----------------------------------------------------------------------------------------
%	SECTION 8
%----------------------------------------------------------------------------------------
\section{Class Imbalance Problem/Low Default Problem}



%----------------------------------------------------------------------------------------
%	SECTION 8
%----------------------------------------------------------------------------------------
\section{Holdout set or cross validation}
When calculating a measure of model accuracy there are some considerations to be made to increase the likelihood of the model making accurate predictions on unseen data. If we build a model using the full historical dataset available and subsequently test the accuracy of the model against the same dataset then we are in danger of generating a model that is over-fitted to the training set and the accuracy observed from the subsequent test may be biased (i.e. optimistic of the accuracy of the model).

%----------------------------------------------------------------------------------------
%	SECTION 9
%----------------------------------------------------------------------------------------
\section{Model Evaluation Measures}\label{modelEvalMeasure}

This section details some of the metrics that can be used to assess the accuracy of a classifier. The result of the classification algorithm maps the modelled data into a category, in this thesis it a binary classifier that is output: 1 is output for identifying customers who will default (bad) or 0 is output for customers who will be not-default (good). The majority of classification algorithms will produce a ranked numeric value which can be converted to a binary representation by some threshold or cut-off decision depending on the business decision that is trying to be optimised. This section will begin with a confusion matrix, details how this is leveraged to build other performance measures and details how charts and metrics can be leveraged together to decide on the performance measure to maximise the intended objective.

\url{http://www.saedsayad.com/model_evaluation_c.htm}

\subsubsection{Confusion Matrix}

The results that made by the classification algorithm can be represented by a contingency table known as a confusion matrix. In this thesis the classification algorithm will output a binary classification, so the confusion matrix will be made from a $2 \times 2$ matrix that has two classes, known as the \textit{positive} and \textit{negative} class. For this thesis, the positive class will be the customers that default and negative class will be the customers that do not default. The confusion matrix can be broken down into the following categories for this thesis:

\begin{itemize}
	\item \textit{true positive} (TP), cases that are predicted to default, and are {\color{green}{correctly}} classified as \textit{positive}
	\item \textit{false positive} (FP), cases that are predicted to default, and are {\color{red}{incorrectly}} classified as \textit{positive}, also known as \textit{Type I error}.
	\item \textit{false negative} (FN), cases that are predicted to not-default, and are {\color{red}{incorrectly}} classified as \textit{negative}, also known as \textit{Type II error}.
	\item  \textit{true negative} (TN), cases that are predicted to not default, and are {\color{green}{correctly}} classified \textit{negative}
	
\end{itemize}

Fig \ref{fig:ConfusionMatrix} illustrates how information from a confusion matrix can be presented and read.

\begin{figure}[H]
	\includegraphics[width=0.8\textwidth,center]{Confusion_Matrix}
	\caption[Confusion Matrix]
	{Confusion Matrix}
	\label{fig:ConfusionMatrix}
\end{figure}

Using the numbers outputted from the confusion matrix, model evaluation measures can calculated and evaluated for the required objective.

\begin{equation} \label{eq:Sensitivity}
\text{Sensitivity} = \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\begin{equation} \label{eq:Specificity}
\text{Specificity} = \frac{TN}{FP + TN}
\end{equation}

\begin{equation} \label{eq:precision}
\text{True Positive Value} = \text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\begin{equation} \label{eq:Accuracy}
\text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

\begin{equation} \label{eq:Balanaced Accuracy}
\text{Average Accuracy} = \text{Balanaced Accuracy} = \frac{Sensitivity + Specificity}{2}
\end{equation}

\begin{equation} \label{eq:Misclassification Rate}
\text{Misclassification Rate} = 1 - \text{Accuracy}
\end{equation}




Confusion Matrix is used to evaluating a model where you have divided the output into distinct or discrete categories. A confusion matrix can be reconstructed for any point on the ROC curve. The confusion matrix illustrate how many correct and incorrect predictions are made by the predictive model compared to the target value. 

"ROBBED" \textit{The matrix can be a $N \times N$, where $N$ is the number of target classes/groups. Performance of such models is commonly evaluated using the data in the matrix.}

\textit{In binary classification, often a quality of a model is calculated by the term Accuracy. Before the accuracy can be measured, one need to calculate the true positive (TP) and true negative rate(TN). True positive calculate the number of positive prediction provided that the actual value is positive. True negative calculate the number of negative prediction provided that the actual value is negative.}



\textit{The term of positive and negative in above paragraph is a common term for binary classification (2 class prediction). The value 1 and 0 also can be used. For example, in email, some email can be cateorized as not a spam and some others can be categorized as a spam. Not a spam is clasified as True or 1, while a spam is classified as 0 or negative.}

\begin{itemize}
	\item \textbf{Accuracy:} the proportion  of the total number of predictions that were correct.
	\item \textbf{Precision} or \textbf{Positive Predictive Value:} the proportion of positive cases that were correctly identified.
	\item \textbf{Negative Predictive Value:} the proportion of negative cases that were correctly identified.
	\item \textbf{Sensitivity} or \textbf{Recall:} the proportion of actual positive cases which are correctly identified.
	\item \textbf{Specificity:} the proportion of actual negative cases which are correctly identified.
\end{itemize}


\textit{Balanced accuracy is generally a more reliable measure as imbalances in the prediction class don't have such an e ect (Brodersen et al., 2010). tommy}

\begin{figure}[H]
	\includegraphics[width=0.8\textwidth,center]{Confusion_Matrix_Example}
	\caption[Confusion Matrix Example]
	{Confusion Matrix Example}
	\label{fig:ConfusionMatrixExample}
\end{figure}

\textit{if u r talking about cut point for probability value, u can decide it by 2 ways .
	1. Calculate the misclassification cost for different probability values, and choose the one which will have least misclassification cost. .
	2 . Draw lift chart for probability values, number of accurately classified events per decile (In precise Results of KS test). Point where u get highest distance is the cut of point for your probability	
}

\begin{equation}
	Misclassification\ Rate = \frac{FP + FN}{TN + FP + FN + TP} = 1 - Accuracy
\end{equation}


\textit{You will have to decide on the cutoff based on the basis of your business objective, level of impact and the tradeoff between sensitivity, specificity and false positivity values. · You should select cutoff value such that you can improve sensitivity of the model by restricting the false positive rate to the lowest minimum value · You need to be very careful in selecting cut-off value because selecting a very low cut-off may give you a high True positive rate but that will increase the false positive rate as well, badly impacting your model performance. · You may even use table instead of graph to get better insight on the effect of the various probability cutoff. Click view àtable in the result window}

\textit{SAS: A cut-off value of 0.5 is not always acceptable because the observed proportion of a primary outcome in a given population may not be always 50\%. SAS® Enterprise Miner™ provides Cutoff node to adjust probability cut-off point based on model’s ability to predict true positive, false positive \& true negative}

\textit{
A popular automatic cutoff selection method is to use the Event Precision Equal Recall. This method selects a cutoff where the event precision rate and the true positive rate intersect.
}

\textit{ You will have to decide on the cutoff based on the basis of your business objective, level of impact and the tradeoff between sensitivity, specificity and false positivity values. · You should select cutoff value such that you can improve sensitivity of the model by restricting the false positive rate to the lowest minimum value · You need to be very careful in selecting cut-off value because selecting a very low cut-off may give you a high True positive rate but that will increase the false positive rate as well, badly impacting your model performance. · You may even use table instead of graph to get better insight on the effect of the various probability cutoff.}


\textit{Accuracy and No Information Rate comment on this here model evaluation}

\subsubsection{Gain \& Lift}
"Robbed" The
\textit{Gain or lift is a measure of the effectiveness of a classification model calculated as the ratio between the results obtained with and without the model. Gain and lift charts are visual aids for evaluating performance of classification models. However, in contrast to the confusion matrix that evaluates models on the whole population gain or lift chart evaluates model performance in a portion of the population. }

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.90\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=4cm]{gains_ratio_process}\caption{Process}\label{fig:gains_ratio_process}
	\end{subfigure} 
	\medskip
	\newline
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth]{gains}
		\caption{Gain Chart}\label{fig:gains}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth]{lift}
		\caption{Lift Chart}\label{fig:lift}
	\end{subfigure}
	\caption{Gain \& Lift Work-flow}
	\label{fig:unbal_corr_analysis}
\end{figure}

\textit{The lift chart shows how much more likely we are to receive positive responses than if we contact a random sample of customers. For example, by contacting only 10\% of customers based on the predictive model we will reach 3 times as many respondents, as if we use no model.}

\subsubsection{K-S Chart}
\url{http://www.saedsayad.com/model_evaluation_c.htm}
\textit{K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.}

\begin{figure}[H]
	\includegraphics[width=0.8\textwidth,center]{ks}
	\caption[Kolmogorov-Smirnov chart ]
	{Kolmogorov-Smirnov chart }
	\label{fig:ks}
\end{figure}



\subsubsection{ROC Chart, AUC and Gini Coefficient}
\url{https://staesthetic.wordpress.com/2014/04/14/gini-roc-auc-and-accuracy/}

(1) AUC doesn't give total probability 
(2) AUC is used to compare the performance of your model on the training and test datasets. if there is significant drop from the AUC on training to that on test, it shows it doesn't generalise well.

\textit{ROC curves are used to evaluate and compare the performance of diagnostic tests; they can also be used to evaluate model fit. An ROC curve is just a plot of the proportion of true positives (events predicted to be events) versus the proportion of false positives (nonevents predicted to be events).}

\textit{ROC is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied (from wikipedia), while AUC is the Area Under ROC Curve. The last term, gini, is calculated by 1-2*AUC, in another source, it was calculated by 2*AUC-1.}

\textit{Another method to measure the performance in binary classification beside accurary is the ROC method. Different from accuracy, ROC analysis uses the true positive rate (TPR) and false positive rate (FPR). TPR is the proportion positive correctly classified (TP/P) and  False positive rate (FPR) is calculated by 1-TNR (TN/N). The ROC region plot TPR against FPR.}

\textit{Above is the example of ROC plot for classification model. Please remember that the ROC is used to evaluate the performance of a classifier model. The first thing to do before creating the ROC plot is of course creating the model. The iterative method of ROC development can be summarized into three step. Step 1 is developing the model that can produce a score for each individual data with based on all its variable. The score usually is the probability that this data will be postitive. Step 2 involves the data and its subsequent score is then ordered, usually in descending order (score). The last step is plot the ROC.}

\textit{When observing the plot, if the plot follow the straight line from lower left to upper right, then the classifier cannot differentiate between negative and postive data. If the curve tend to bend to the upper left, then the model can differentiate the actual positive and negative data. On the contrary, if the curve tend to bend to lower right, then it just completely wrong prediction model.
To performance of a classifier model is then calculated by calculating the Area Under Curve (AUC). The AUC score will be between 0 and 1. The higher the value of AUC, usually the better the model is.}

\begin{figure}[H]
	\includegraphics[width=0.8\textwidth,center]{ROC}
	\caption[ROC]
	{ROC}
	\label{fig:ROC}
\end{figure}

\textit{Area under ROC curve is often used as a measure of quality of the classification models. A random classifier has an area under the curve of 0.5, while AUC for a perfect classifier is equal to 1. In practice, most of the classification models have an AUC between 0.5 and 1.}

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth]{Confusion_Matrix_threshold_065}
		\caption{Threshold $=.65$}\label{fig:Threshold65}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height=2.025cm]{Confusion_Matrix_threshold_050}
		\caption{Threshold $=.50$}\label{fig:Threshold50}
	\end{subfigure}
	\caption{Confusion Matrix Threshold comparison}
	\label{fig:matric_compare}
\end{figure}

\textit{An area under the ROC curve of 0.8, for example, means that a randomly selected case from the group with the target equals 1 has a score larger than that for a randomly chosen case from the group with the target equals 0 in 80\% of the time. When a classifier cannot distinguish between the two groups, the area will be equal to 0.5 (the ROC curve will coincide with the diagonal). When there is a perfect separation of the two groups, i.e., no overlapping of the distributions, the area under the ROC curve reaches to 1 (the ROC curve will reach the upper left corner of the plot).}

\textit{AUC
AUC stands for “area under curve”, and as it's name implies, it refers to the amount of area under the ROC curve, which theoretically is a value between 0 and 1. As I explained, the worst possible curve in practice is a diagonal line, hence the AUC should never be lower than 0.5 (for large data sets) .
Using the AUC metric you can quickly compare multiple learning models. Remember that the ROC curves of two models usually don’t cross each other, hence when comparing two models, the one with a higher AUC will be the better one regardless of the threshold setting. Compared to the statistical measures of accuracy, precision, recall and F1 score, AUC’s independence of threshold makes it uniquely qualified for model selection.
On the other hand, unlike accuracy, precision, recall and F1 score, AUC does not tell us what performance to expect from the model for a given threshold setting, nor can it be used to determine the optimal value for threshold. In that regard it doesn't take away the need for the other statistical measures.
Putting it together
The ROC plot and the AUC are very useful for comparing and selecting the best machine learning model for a given data set. A model with an AUC score near 1, and where the ROC curve comes close to the upper left corner, has a very good performance. A model with a score near 0.5 will have a curve near to the diagonal and its performance is hardly better than a random predictor.
After selecting the best model, the next step will be to configure the threshold and the other model configuration settings. The Precision/Recall plot can be helpful for understanding the trade-off between false positives and false negatives. In the third article we’ll look closer at the threshold value and learn now to calculate the optimal value for it.
}

Gini Coefficient is also known as Gini index or gini ratio. It is very closely related to the calculation of AUC and can be computed by\\
$Gini = 2 \times AUC - 1$


%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------
\section{Conclusion}\label{sotaConc}
This chapter has summarised the relevant literature available for two-class classification and web data analysis, with discussion of predicting future behaviour based on sequential click patterns. Various models have been discussed including K-Nearest Neighbours, Association Rules, Decision Trees, SVM, Neural Networks, Regression Models and ensemble models

%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------
