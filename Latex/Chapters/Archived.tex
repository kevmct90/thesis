\chapter{Archived} % Main chapter title

\label{Archived} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Archived}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\section{Chapter4}
\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_Delinq_Model_Lift}
	\caption{Previous Delinquency Base Benchmark Model Lift Charts}
	\label{fig:Delinq_Model_Lift}
\end{figure}

We can see above in Fig. \ref{fig:Delinq_Model_Lift} that the majority of the models are returning positive results in the unseen test chart. This is very encouraging, for example if we were to contact 10\% of customers that these models decided were going to go into default, you would contact over twice as many customers that would go into default than if you used no model at all. This is very important when deciding on a business strategy and is where BE becomes extremely important. 

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{Base_NonDelinq_Model_Lift}
	\caption{No Previous Delinquency Model Lift Chart}
	\label{fig:NonDelinq_Model_Lift}
\end{figure}

As with the lift chart in the Previous Delinquency model we can see in Fig. \ref{fig:NonDelinq_Model_Lift} the majority of the models are returning positive results most notably in the the regression validation and test result as you would expect. We are seeing a larger lift than in the Previous Delinquency model also. We can see for the regression model if we were to contact 10\% of the customers that this model decided were to going to go into default we would contact three times as many customers than you if you didn't use a model at all and choose them at random. 

Because of the large imbalance in the dataset the default, min misclassification rate will return very misleading results. These performance measures are made easily bias and skewed due to the 

Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} allows you analyse multiple methods deriving what the cut-off threshold should be and Table \ref{table:DelinquencyModelCutoff} allows you to evaluate these results.

Ideally in this situation for credit scoring maximise the number of customer you can identify that will go into defaul 

As mentioned before knowing the business goal and aim can be very useful when trying to decide how you want to evaluate you model. The over usefulness and predictiveness of model can be got by combing the AUC with the ROC chart but sometimes this will not suffice. 

In Fig. \ref{fig:Base_Delinq_Model_CutOff_Analysis} and Table \ref{table:DelinquencyModelCutoff} you can see the cut-off analysis for the best benchmark model \textit{Gradient Boosting}. As discussed in Chapter \ref{Chapter2} this analysis demonstrates that your business goal and objectives will dictate which performance metric you may want to maximise or minimise over another. 

Based 

Based on the literature it was decided that \textit{balanced accuracy} (BA) would be be the most measure to evaluate the performance of the model at a specific threshold. As discussed in Section \ref{modelPerformMeasure} BA negates the impact of bias and skewness of mo

\section{Chapter 4: Old Sections}
This chapter will detail the design of the experiments that are to be implemented as part of the research project. It will detail how customers addresses are mapped to locations and how features are generated for these locations. It will also discuss any of the pre-processing, and feature selection and statistical techniques used to create select the data for the ABT.

The SME default problem would be considered imbalanced dataset for classification problem where algorithms make the assumption there is a equal number of good and bad classes/group \citep{japkowicz_class_2000}. The dataset used in this experiment has a small number of SME customer in default, because of this issue a number of approaches will taken to considered to address.

Statistical analysis and evaluation methods used to assess the strength of models will be detailed and discussed, also it will outline any further transformations necessary for specific algorithms. Finally criteria for deciding which model will be chosen as the best and most fit for purpose will be detailed.

\section{Chapter 4: Evaluation Methods}
Several evaluation techniques will be applied to quantify the usefulness of all models produced. As discussed in section \ref{modelEval}, there are various ways to evaluate the performance of a classifier. A common metric used is overall accuracy (\% of records correctly classified). 

\section{Chapter 5:Correlation}
\textit{Estimation results are presented in Table 4 a
	nd show no consistent 
	and robust correlations 
	between the levels of SME finan
	ce and definition criteria. We do not find statistically significant 
	correlations between the value of SME financi
	ng and the maximum number of employees used 
	as a criterion to define SME.
	10
	In a smaller sample the maximum sales volume criteria is 
	positively correlated with the ratio of SME loans to GDP but not with the share of SME loans in 
	total commercial bank loans.  Moreover, the correlation between the ratio of SME loans to GDP 
	and sales volume is not statistically signifi
	cant once we control for income per capita.   }
	
	\textit{A number of macroeconomic and institutional factors are associated with greater levels of SME financing.  Table 5 presents pairwise correlations of the ratio of SME loans to GDP and a number of macroeconomic and institutional factors. Consistent with earlier research on SME and enterprise financing, we find a positive correlation between the overall level of economic development measured by income per capita and financial development measured by the ratio of private credit to GDP with the level of SME financing.  Legal frameworks and the overall business environment are also important factors affecting the level of SME financing.  For example, the ability to open and close a business is found to be an important factor associated with growth.  Using information from the Doing Businessdatabase, we find a negative correlation between the number of days it takes to start and close a business and the value of SME financing}
	
	\textit{In addition, we consider a number of parameters describing financial institutions operating in a 
		country.  We do not find a statistically significant
		correlation between the share of foreign- or 
		state-owned banks and levels of 
		SME financing, which is consiste
		nt with bank level evidence in 
		Beck 
		et al.
		(2008b).  Unlike firm level analysis in Beck 
		et al.
		(2005b), we do not find a 
		statistically significant level of 
		correlation between the level of 
		bank concentration and the ratio 
		of SME financing to GDP. However, broader re
		tail outlet networks meas
		ured by the number of 
		bank branches per 100,000 adults from the 
		Financial Access
		database are associated with more 
		SME financing. Countries where banks have less 
		efficient structures measured by a higher ratio 
		of overhead costs to total assets, 
		higher interest rate margin and a 
		greater cost to income ratio,  
		tend to have lower levels of SME financing. }

\section{Chapter 4: Methodology for Evaluating Macro-Economic Features}
\subsection{Building the Models}
\subsubsection{Baseline Model in R}
\subsubsection{Correlation Analysis}
\subsubsection{Information Gain}
\subsubsection{Coarse Selection}
\subsubsection{KNN Approach}
\subsubsection{Random Oversampling}
\subsubsection{Synthetic Sampling }
\subsubsection{Evaluation Methods}


\section{Chapter 5}

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l| l|r|r|r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{Default} & \textbf{Not-Default} & \textbf{Total} \\
			\hline
			Previous Delinquency          & Training       & 423 & 1,332 & 1,755 \\
			Previous Delinquency          & Validation       & 156 & 429 & 585 \\
			Previous Delinquency          & Test & 149 & 437 & 586 \\ \hline
			No Previous Delinquency          & Training & 406 & 14,087 & 14,493 \\ 
			No Previous Delinquency          & Validation & 121 & 4,710 & 4,831 \\
			No Previous Delinquency          & Test & 124 & 4,708 & 4,832 \\
			\hline
			&      	\textbf{Total }     & \textbf{1,379} & \textbf{25,703} & \textbf{27,082} \\ \hline
		\end{tabular}
	}
	\caption{Breakdown of Training/Validation/Test Partitions for Experiment}
\end{table}


\section{Exploration of  Data}
\begin{figure}[H]
	\includegraphics[width=0.85\textwidth,height=6cm, center]{percentageArrears}
	\caption{Percentage of Customers in arrears for each month 
		\\ June 2012 - June 2015}
	\label{fig:percentageArrears}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=0.85\textwidth,height=6cm, center]{SME_Status}
	\caption{SME Customers Status\\ June 2012 - June 2015}
	\label{fig:SME_Status}
\end{figure}

\begin{figure}[H]
	\includegraphics[width=0.85\textwidth,height=6cm, center]{ChangesInArrears}
	\caption{Monthly Changes Customers Leaving and going into arrears\\June 2012 - June 2015}
	\label{fig:ChangesInArrears}
\end{figure}


\begin{figure}[H]
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height = 10cm]{June2014ArrearsByLA}
		\caption{Arrears by County June 2014}\label{fig:June2014ArrearsByLA}
	\end{subfigure} ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth,height = 10cm]{NewArrearsByLA}
		\caption{New Arrears June 2015 by County}\label{fig:NewArrearsByLA}
	\end{subfigure}
	\medskip \newline
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth, height = 10cm]{June2014ArrearsByED}
		\caption{Arrears by Electoral Division June 2014}
		\label{fig:June2014ArrearsByED}
	\end{subfigure}  ~\quad
	\begin{subfigure}[b]{0.45\textwidth}
		\captionsetup{font=scriptsize}
		\includegraphics[width=\textwidth, height = 10cm]{NewArrearsByED}
		\caption{New Arrears June 2015 by Electoral Division}
		\label{fig:NewArrearsByED}
	\end{subfigure}
	\caption{SME Arrears by County and Electoral Division}
	\label{fig:SMEArrearsLAED}
\end{figure}



\section{Feature Selection}
There has been 116 features created as part of this experiment along with the features that already exist in the base models, it is necessary where possible to reduce this as reducing the dimensionality of a model is very important. Feature selection will be carried out to reduce this dimensionality of the model and reducing the complexity of the dataset. The feature selection process that will be used will encompass Correlation Analysis, entropy based calculating the information gain, gain ratio and symmetrical uncertainty, and random forest method for feature selection. 


\subsection{Base Model}
A base model has been built using SAS, however for the feature selection process there is of manipulation and over head in the process. Another model will be built in R to to base the selection of the experimental features. The logistic regression model performed strongly in both the previous delinquency and no previous delinquency model so it will be used again here as base for the feature selection process. The result of the feature selection baseline benchmark models are as follows 

\begin{table}[H]
	\centering
	\begin{tabular}{l | r | r| r}
		\hline
		\textbf{Model} & \textbf{Train AUC} & \textbf{Valid AUC} &  \textbf{Test AUC} \\
		\hline
		Regression (Previous Delinq) & 0.653 & 0.615 & 0.683  \\
		Regression (No Previous Delinq) & 0.704 & 0.656 & 0.691  \\
		\hline
	\end{tabular}
	\caption{Feature Selection Baseline Benchmark Results}
	\label{table:featureselection_base_model}
\end{table}

\subsection{Correlation Analysis}



\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{CorrelationChart}
	\caption{Correlation Analysis}
	\label{fig:Correlation Analysis}
\end{figure}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Valid AUC} &  \textbf{Test AUC} \\
			\hline
			Regression (No Previous Delinq) Top 5 Features & 0.707 & 0.661 & 0.693  \\
			Regression (No Previous Delinq) Top 10 Features & 0.715 & 0.660 & 0.701  \\
			\cellcolor{green!25}Regression (No Previous Delinq) Top 15 Features &  \cellcolor{green!25}0.719 &  \cellcolor{green!25}0.662 &  \cellcolor{green!25}0.699  \\
			Regression (No Previous Delinq) Top 20 Features & 0.729 & 0.652 & 0.695  \\
			\hline
			Regression (Previous Delinq) Top 5 Features & 0.658 & 0.613 & 0.681  \\
			Regression (Previous Delinq) Top 10 Features & 0.659 & 0.609 & 0.686  \\
			\cellcolor{green!25}Regression (Previous Delinq) Top 15 Features &  \cellcolor{green!25}0.665 &  \cellcolor{green!25}0.62 &  \cellcolor{green!25}0.690  \\
			Regression (Previous Delinq) Top 20 Features & 0.679 & 0.611 & 0.688  \\	
			\hline
		\end{tabular}
	}
	\caption{Results based on features output from correlation analysis}
	\label{table:featureselection_base_model}
\end{table}

\subsection{Information Gain, Gain Ratio \& Symmetrical Uncertainty}

\subsubsection{Experiment Features}

\begin{figure}[H]
	\includegraphics[width=0.9\textwidth,center]{InformationGain_Experiment_Features}
	\caption{Information Gain Experiment Features}
	\label{fig:Information Gain Experiment Features}
\end{figure}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l | r | r| r}
			\hline
			\textbf{Model} & \textbf{Train AUC} & \textbf{Valid AUC} &  \textbf{Test AUC} \\
			\hline
			Regression (No Previous Delinq) Top 5 Features & 0.706 & 0.657 & 0.694  \\
			Regression (No Previous Delinq) Top 10 Features & 0.709 & 0.657 & 0.693  \\
			Regression (No Previous Delinq) Top 15 Features & 0.72 & 0.646 & 0.694  \\
			Regression (No Previous Delinq) Top 20 Features & 0.725 & 0.655 & 0.689  \\
			\hline
			
			Regression (Previous Delinq) Top 5 Features &  0.664 &  0.597 &  0.678  \\
			Regression (Previous Delinq) Top 10 Features &  0.664 &  0.596 &  0.675  \\
			Regression (Previous Delinq) Top 15 Features &  0.670 &  0.597 &  0.692  \\
			Regression (Previous Delinq) Top 20 Features &  0.675 &  0.593 &  0.681  \\		
			\hline
		\end{tabular}
	}
	\caption{Results based on features output from Information Gain Analysis}
	\label{table:featureselection_base_model}
\end{table}

\section{K-NN Approach}

\subsubsection{No Previous Delinquency Analysis}




\begin{table}[H]
	\centering
	\begin{tabular}{l | r | r| r}
		\hline
		\textbf{Model} & \textbf{Train AUC} & \textbf{Valid AUC} &  \textbf{Test AUC} \\
		\hline
		Regression (No Previous Delinq) & 0.771 & 0.662 & 0.628  \\
		\hline
	\end{tabular}
	\caption{Model Results from optimised KNN results for No Previous Delinquency Model where K=75}
	\label{table:knnNoPrevDelinqModel}
\end{table}



\subsubsection{Previous Delinquency Analysis}




\lstinputlisting[float=H,frame=tb,caption=Delinquency Model Analysis K-NN,label=zebra]{DelinqknnFit.txt}



\begin{table}[H]
	\centering
	\begin{tabular}{l | r | r| r}
		\hline
		\textbf{Model} & \textbf{Train AUC} & \textbf{Valid AUC} &  \textbf{Test AUC} \\
		\hline
		KNN51 No Previous Delinq) & 0.771 & 0.662 & 0.628  \\
		\hline
	\end{tabular}
	\caption{Model Results from optimised KNN results for No Previous Delinquency Model where K=51}
	\label{table:knnNoPrevDelinqModel}
\end{table}


\section{Addressing Imbalance}


As discussed is Chapter 2, undersampling is not a good fit for absolute rarity so will perform oversampling of the minority class. This will be done by oversampling in the training data only, the validation and test will remain unchanged. 
\subsection{Oversampling}
\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l| l|r|r|r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{Default} & \textbf{Not-Default} & \textbf{Total} \\
			\hline
			Previous Delinquency          & Training       & 435 & 1,320 & 1,755 \\
			Previous Delinquency          & Training(OverSample) & 1,304 & 1,320 & 2,624 \\
			Previous Delinquency          & Validation       & 151 & 436 & 585 \\
			Previous Delinquency          & Test & 142 & 444 & 586 \\ \hline
			No Previous Delinquency          & Training & 379 & 14,114 & 14,493 \\ 
			No Previous Delinquency          & Training(OverSample) & 13,959 & 14,113 & 28,072 \\ 
			No Previous Delinquency          & Validation & 120 & 4,711 & 4,831 \\
			No Previous Delinquency          & Test & 152 & 4,680 & 4,832 \\
			\hline
		\end{tabular}
	}
	\caption{Breakdown of Training/Validation/Test Partitions for Experiment}
\end{table}


\footnote{{\url{https://cran.r-project.org/web/packages/ROSE/index.html}}}. 


\begin{table}[H]
	\centering
	\caption{My caption}
	\label{my-label}
	\begin{tabular}{llll}
		\textbf{Model}                 & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\
		DelinquencyBase                & 0.663             & 0.659               & 0.606            \\
		DelinquencyBaseOverSample      & 0.664             & 0.649               & 0.609            \\
		DelinquencyExperBase           & 0.667             & 0.655               & 0.655            \\
		DelinquencyExperBaseOverSample & 0.672             & 0.647               & 0.606           
	\end{tabular}
\end{table}


\begin{table}[H]
	\centering
	\caption{My caption}
	\label{my-label}
	\begin{tabular}{llll}
		\textbf{Model}                    & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\
		NonDelinquencyBase                & 0.676             & 0.704               & 0.716            \\
		NonDelinquencyBaseOverSample      & 0.677             & 0.708               & 0.719            \\
		NonDelinquencyExperBase           & 0.678             & 0.709               & 0.72             \\
		NonDelinquencyExperBaseOverSample & 0.68              & 0.713               & 0.722           
	\end{tabular}
\end{table}



\subsection{Synthetic Sampling}

\begin{table}[H]
	\centering\
	\resizebox{\textwidth}{!}
	{
		\begin{tabular}{l| l|r|r|r}
			\hline
			\textbf{Model} &  \textbf{Dataset} & \textbf{Default} & \textbf{Not-Default} & \textbf{Total} \\
			\hline
			Previous Delinquency          & Training       & 435 & 1,320 & 1,755 \\
			Previous Delinquency          & Training(Synthetic KNN51) & 1,305 & 1,740 & 3,045 \\
			Previous Delinquency          & Validation       & 151 & 436 & 585 \\
			Previous Delinquency          & Test & 142 & 444 & 586 \\ \hline
			No Previous Delinquency          & Training & 379 & 14,114 & 14,493 \\ 
			No Previous Delinquency          & Training(Synthetic KNN75) & 7,580 & 14,402 & 21,982 \\ 
			No Previous Delinquency          & Validation & 120 & 4,711 & 4,831 \\
			No Previous Delinquency          & Test & 152 & 4,680 & 4,832 \\
			\hline
		\end{tabular}
	}
	\caption{Breakdown of Training/Validation/Test Partitions for Experiment}
\end{table}

\footnote{{\url{https://cran.r-project.org/web/packages/DMwR/index.html}}}. 



\begin{table}[H]
	\centering
	\caption{My caption}
	\label{my-label}
	\begin{tabular}{llll}
		\textbf{Model}                             & \textbf{Training} & \textbf{Validation} & \textbf{Testing} \\
		DelinquencyBase                            & 0.663             & 0.659               & 0.606            \\
		DelinquencyBaseSyntheticSample\_KNN51      & 0.659             & 0.659               & 0.6              \\
		DelinquencyExperBase                       & 0.657             & 0.655               & 0.606            \\
		DelinquencyExperBaseSyntheticSample\_KNN51 & 0.673             & 0.655               & 0.6             
	\end{tabular}
\end{table}


\begin{table}[H]
	\centering
	\caption{My caption}
	\label{my-label}
	\begin{tabular}{llll}
		Model                                       & Training & Validation & Testing \\
		NonDelinquencyBase                          & 0.676    & 0.704      & 0.716   \\
		NonDelinquencyBaseSyntheticSampleKNN75      & 0.669    & 0.699      & 0.718   \\
		NonDelinquencyExperBase                     & 0.68     & 0.709      & 0.72    \\
		NonDelinquencyExperBaseSyntheticSampleKNN75 & 0.673    & 0.689      & 0.706  
	\end{tabular}
\end{table}

\section{Comparing Results of Various Experiments}

\section{Model Selection/Validation}

\section{Interpretation of Results}

\section{Evaluation of Experiments}

\section{Background OlD}

We begin with a high-level overview of SME credit scoring and location data in machine learning.


Credit scoring, in layman's language is the name given to describe the process of assessing how risky customers and applicants are to default on their financial obligation \citep{hand_statistical_1997}. The goal is to split customers into two classes, 'good' and 'bad'. Customers in the good class are considered likely to repay on their financial obligation while customers in the bad class are likely to default on theirs. In order to make this decision about its customers, financial institutions use information known about the customer and other relevant metrics to identify how risky a customer is. 


Since the financial crisis of 2007-2008 there has been a  much greater emphasis on credit scoring in the consumer lending process in banking. One of the most common methods of building scorecards is by using data mining \citep{baesens_50_2009}. 
\\\\
There are three main stages of a typical credit scorecard development \cite{van_gestel_credit_2009}

\begin{itemize}
	\item Dataset Construction
	\item Modelling
	\item Documentation 
\end{itemize}

This can displayed using this architectural diagram. Insert find diagram.
\\\\
Each stage of this process cannot be left out when completing a industry credit scorecard, however for this research will be focusing particularly on the data construction and modelling stages. The documentation stage is critical for deployment of an industry scorecard but will not be investigated further in this thesis. 

%----------------------------------------------------------------------------------------
%	
%----------------------------------------------------------------------------------------

Since the financial of 2007-2008 there is an even greater emphasis on quality credit scoring. In Ireland it was the property bubble that collapsed causing the banking crisis. 

"Many believe that focusing on lending on the SME sector could play an important role in restoring faith in the banking system and supporting economic growth, according to leading experts"


The primary aim of this research project is to build a quantitative credit scorecard that will identify Small Medium Enterprise (SME) customers who are likely to go into arrears/defaults.


The upheaval in the financial markets that accompanied the 2007-2008 sub-prime
mortgage crisis has emphasised the large proportion of the banking industry based
on consumer lending (Thomas, 2009b). Credit scoring is an important part of the
consumer lending process. It is an endeavour regarded as one of the most popular
application fields for both data mining and operational research techniques (Baesens
et al., 2009). Improving the scoring accuracy of the credit decision by as much as
a fraction of a percent can result in significant future savings (West, 2000). Furthermore,
global (Bank for International Settlements) and national (central banks)
regulators insist that financial institutions keep better track of their credit scoring
systems. The costs of incorrectly classifying a customer can be high, both financially
and in terms of reputation.


\textit{It is an endeavour regarded as one of the most popular
	application fields for both data mining and operational research techniques (Baesens
	et al., 2009). Improving the scoring accuracy of the credit decision by as much as
	a fraction of a percent can result in significant future savings}



\textit{footnotes found in \citep{rocha_status_2011} also very good introduction for references
	According to Ayyagari et al., (2007) SMEs account for more than 60\% of manufacturing employment across 76
	developed and developing economies. 
	Schiffer and Weder (2001), IADB (2004) and Beck et al. (2005, 2006 and 2008) show SMEs perceive access to
	finance and cost of credit to be greater obstacles than large firms and these factors affect their growth.
}

Two thirds of the labour workforce are employed by SMEs with less than 250 employees in countries in \textit{The Organisation for Economic Co-operation and Development} (OECD)\footnote{The OECD's origins date back to 1960, when 18 European countries plus the United States and Canada joined forces to create an organisation dedicated to economic development. Today, our 34 Member countries span the globe, from North and South America to Europe and Asia-Pacific. They include many of the world’s most advanced countries but also emerging countries like Mexico, Chile and Turkey. We also work closely with emerging economies like the People's Republic of China, India and Brazil and developing economies in Africa, Asia, Latin America and the Caribbean. Together, our goal continues to be to build a stronger, cleaner and fairer world. \url{http://www.oecd.org/about/membersandpartners/}} (\cite{beck_bank_2008}; \cite{dietrich_explaining_2012}). In its simplest 

\textit{There  are  numerous  studies  on  credit 
	risk based on financial data of listed companies, which are mostly 
	large  corporations,  but  very  few  studies  utilized  the  data  of  small 
	and  medium  enterprises  (SMEs).  This  could  be  mainly  due  to  the 
	concern of the reliability of the data since large firms listed at the 
	stock  exchanges  are  closely  monitored  by  the  authority  to  ensure 
	that the financial data provide accurate and useful information for 
	investors and shareholders. \citep{sirirattanaphonkun_default_2012}}

\textit{The
	implementation of reliable statistical methods to measure and forecast these probabilities implies the
	consideration of an observation period. In other words, the identification of default events implies mon
	-
	itoring each debtor over time and the identification of a transition from non-default to a default state.
	The obtained statistics are useful for credit institutions in many ways, spanning from the credit ap-
	proval process (for instance to implement cut-off points to screen credit applications), to the establish-
	ment of risk-sensitive pricing, the decision on whether collateral is required or not, the estimation of
	provisioning requirements or impairment losses, and the assessment of capital adequacy.
	\citep{antunes_estimating_2005}}

Studies demonstrate that performing SMEs are vital as they as seen as the spine of countries economies all around the world, because they are hubs for providing jobs, innovation and growth to the economy \citep{craig_sba-guaranteed_2004}.


\textit{For example, we know that during recessions, consumers
	are likely to cut back on luxuries, and thus firms in the consumer durable goods sector
	should see their credit risk increase. \citep{hackbarth_capital_2006}}

This chapter introduces the research topic, the predictiveness of location-based features in \textit{Small Medium Enterprises} SME credit scoring, and how it can potentially can be used to improve credit scoring in financial institutions. The motivation for the research question are discussed with details of its significance. The aims of the thesis along with the research objectives and analytical approach are then stated. The scope and limitation of this research are then described. The chapter concludes with a summary and outlines of this thesis.

\textit{
	http://www.cso.ie/en/surveysandmethodology/multisectoral/businessdemography/
	The CSO Business Demography results for 2012 found that 68% of Irish private sector employees work for an SME. They account for 52% of total employment.
}


\textit{footnotes found in \citep{rocha_status_2011} also very good introduction for references
	According to Ayyagari et al., (2007) SMEs account for more than 60\% of manufacturing employment across 76
	developed and developing economies. 
	Schiffer and Weder (2001), IADB (2004) and Beck et al. (2005, 2006 and 2008) show SMEs perceive access to
	finance and cost of credit to be greater obstacles than large firms and these factors affect their growth.
}


\cite{beck_bank_2008} research indicates that most commercial financial institutions consider the SME sector to be very profitable. 


\cite{anderson_credit_2007} defines \textit{credit risk} as a risk of a customers defaulting on their financial obligation due to the a negative change in their credit worthiness.

Since the financial crisis of 2007-2008 there has been a  much greater emphasis on credit scoring in the consumer lending process in banking. One of the most common methods of building scorecards is by using data mining \citep{baesens_50_2009}. 

Since the financial crisis of 2007-2008 there has been a  much greater emphasis on credit scoring in the consumer lending process in banking.

The most common definition for an SME is a registered business that with less than 250 employees \citep{ifc_sme_2009}, however this definition is not always consistent and can vary from country to country, financial institution to financial institution. 

Studies demonstrate that performing SMEs are vital as they as seen as the spine of countries economies all around the world, because they are hubs for providing jobs, innovation and growth to the economy \citep{craig_sba-guaranteed_2004}.

Worldwide the most common method by regulators for defining businesses as SME are based the number of people they have employed, sales/turnover or/and loan size \cite{ardic_small_2011}.


\cite{beck_bank_2008} research indicates that most commercial financial institutions consider the SME sector to be very profitable.




